{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sopralapanca/Pattern-Recognition/blob/main/Assignment4__Christian_Peluso.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5W-Wjzojb4P"
      },
      "source": [
        "# Setting the Stage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M1tfGKPcbbdS",
        "outputId": "abcee4f2-0fb4-4b20-aaf1-6d3f8d774fb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: PYTHONPATH=# /env/python\n"
          ]
        }
      ],
      "source": [
        "%env PYTHONPATH = # /env/python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLJAY-VzbdUZ"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!wget https://repo.anaconda.com/miniconda/Miniconda3-py38_4.12.0-Linux-x86_64.sh\n",
        "!chmod +x Miniconda3-py38_4.12.0-Linux-x86_64.sh\n",
        "!./Miniconda3-py38_4.12.0-Linux-x86_64.sh -b -f -p /usr/local\n",
        "!conda update -y conda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-ShfFvtbffP"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/usr/local/lib/python3.8/site-packages')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t95HYqFeboNQ"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!conda create -y -n myenv python=3.6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l26uD7ixsTJE",
        "outputId": "2dd81645-0889-4631-c90d-e48a38036d54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "driver_version\n",
            "525.85.12\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi --query-gpu=driver_version --format=csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5CjIp6uBbtdm",
        "outputId": "ba8f9bd7-a08b-4253-bdfe-219ba7b0f427"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "Solving environment: \\ \b\b| \b\bdone\n",
            "\n",
            "\n",
            "==> WARNING: A newer version of conda exists. <==\n",
            "  current version: 4.12.0\n",
            "  latest version: 23.3.1\n",
            "\n",
            "Please update conda by running\n",
            "\n",
            "    $ conda update -n base -c defaults conda\n",
            "\n",
            "\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local/envs/myenv\n",
            "\n",
            "  added / updated specs:\n",
            "    - cudatoolkit=10.0\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    cudatoolkit-10.0.130       |                0       261.2 MB\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:       261.2 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  cudatoolkit        pkgs/main/linux-64::cudatoolkit-10.0.130-0\n",
            "\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "cudatoolkit-10.0.130 | 261.2 MB  | : 100% 1.0/1 [00:06<00:00,  6.46s/it]               \n",
            "Preparing transaction: - \b\bdone\n",
            "Verifying transaction: | \b\bdone\n",
            "Executing transaction: - \b\bdone\n",
            "Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "Solving environment: | \b\bfailed with initial frozen solve. Retrying with flexible solve.\n",
            "Collecting package metadata (repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "Solving environment: / \b\b- \b\b\\ \b\b| \b\bdone\n",
            "\n",
            "\n",
            "==> WARNING: A newer version of conda exists. <==\n",
            "  current version: 4.12.0\n",
            "  latest version: 23.3.1\n",
            "\n",
            "Please update conda by running\n",
            "\n",
            "    $ conda update -n base -c defaults conda\n",
            "\n",
            "\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local/envs/myenv\n",
            "\n",
            "  added / updated specs:\n",
            "    - cudnn=7.3.1\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    cudnn-7.3.1                |       cuda10.0_0       286.1 MB\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:       286.1 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  cudnn              pkgs/main/linux-64::cudnn-7.3.1-cuda10.0_0\n",
            "\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "cudnn-7.3.1          | 286.1 MB  | : 100% 1.0/1 [00:06<00:00,  6.36s/it]               \n",
            "Preparing transaction: - \b\bdone\n",
            "Verifying transaction: | \b\bdone\n",
            "Executing transaction: - \b\bdone\n",
            "\u001b[K     |████████████████████████████████| 411.5 MB 20 kB/s \n",
            "\u001b[K     |████████████████████████████████| 75 kB 3.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.6 MB 48.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 75.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 71.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 57 kB 5.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 50 kB 7.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 14.8 MB 66.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 126 kB 77.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 65 kB 4.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 503 kB 78.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 71.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 97 kB 8.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 289 kB 80.3 MB/s \n",
            "\u001b[?25h  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for termcolor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "%%shell\n",
        "eval \"$(conda shell.bash hook)\"\n",
        "conda activate myenv\n",
        "conda install -y cudatoolkit=10.0\n",
        "conda install -y cudnn=7.3.1\n",
        "pip3 install -q tensorflow-gpu==1.15"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11fCCYNQe130"
      },
      "outputs": [],
      "source": [
        "multiline_str = (\"import tensorflow as tf\\n\"\n",
        "\"print(tf. __version__)\\n\"\n",
        "\"print(tf.__path__ )\\n\"\n",
        "\"if tf.test.gpu_device_name():\\n\"\n",
        "\"  print('Default GPU Device:{}'.format(tf.test.gpu_device_name()))\\n\"\n",
        "\"else:\\n\"\n",
        "\"  print('Please install GPU version of TF')\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-tUE-PFb-1Y"
      },
      "outputs": [],
      "source": [
        "with open('test.py', 'w') as f:\n",
        "    f.write(multiline_str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SuX0_U0af0A6",
        "outputId": "7cb13578-3c63-44b8-aac8-6de6244406a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.15.0\n",
            "['/usr/local/envs/myenv/lib/python3.6/site-packages/tensorflow_core/python/keras/api/_v1', '/usr/local/envs/myenv/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/api/_v1', '/usr/local/envs/myenv/lib/python3.6/site-packages/tensorflow_core', '/usr/local/envs/myenv/lib/python3.6/site-packages/tensorflow_core/_api/v1']\n",
            "2023-05-12 17:15:45.677143: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
            "2023-05-12 17:15:45.683360: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2299995000 Hz\n",
            "2023-05-12 17:15:45.683557: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x558cc36b5500 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2023-05-12 17:15:45.683586: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2023-05-12 17:15:45.687054: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2023-05-12 17:15:45.913001: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-05-12 17:15:45.913298: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x558cc28aa4d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2023-05-12 17:15:45.913325: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2023-05-12 17:15:45.913523: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-05-12 17:15:45.913665: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2023-05-12 17:15:45.913921: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2023-05-12 17:15:45.915157: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "2023-05-12 17:15:45.916246: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
            "2023-05-12 17:15:45.916549: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
            "2023-05-12 17:15:45.917956: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2023-05-12 17:15:45.919085: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2023-05-12 17:15:45.922097: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2023-05-12 17:15:45.922214: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-05-12 17:15:45.922389: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-05-12 17:15:45.922513: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2023-05-12 17:15:45.922560: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2023-05-12 17:15:45.923662: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2023-05-12 17:15:45.923688: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2023-05-12 17:15:45.923697: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2023-05-12 17:15:45.923808: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-05-12 17:15:45.923975: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-05-12 17:15:45.924109: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2023-05-12 17:15:45.924155: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/device:GPU:0 with 14248 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "2023-05-12 17:15:45.924992: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-05-12 17:15:45.925158: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2023-05-12 17:15:45.925191: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2023-05-12 17:15:45.925208: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "2023-05-12 17:15:45.925223: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
            "2023-05-12 17:15:45.925238: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
            "2023-05-12 17:15:45.925254: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2023-05-12 17:15:45.925268: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2023-05-12 17:15:45.925284: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2023-05-12 17:15:45.925341: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-05-12 17:15:45.925502: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-05-12 17:15:45.925625: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2023-05-12 17:15:45.925648: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2023-05-12 17:15:45.925661: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2023-05-12 17:15:45.925672: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2023-05-12 17:15:45.925753: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-05-12 17:15:45.925909: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-05-12 17:15:45.926042: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/device:GPU:0 with 14248 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "Default GPU Device:/device:GPU:0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "%%shell\n",
        "eval \"$(conda shell.bash hook)\"\n",
        "conda activate myenv\n",
        "python /content/test.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJc8Xfn5FZPT"
      },
      "source": [
        "The stage is set, now we can start to import the repository and the dataset..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4Yc_y77GpiI"
      },
      "source": [
        "## Data extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4FJ_8eDF9xu",
        "outputId": "4c048a68-0761-4f30-ffd9-8a10cd584d9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'char-rnn-tensorflow'...\n",
            "remote: Enumerating objects: 404, done.\u001b[K\n",
            "remote: Total 404 (delta 0), reused 0 (delta 0), pack-reused 404\u001b[K\n",
            "Receiving objects: 100% (404/404), 508.45 KiB | 4.20 MiB/s, done.\n",
            "Resolving deltas: 100% (238/238), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/sherjilozair/char-rnn-tensorflow.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWaA8dfl8zBB",
        "outputId": "c25c1425-f2eb-43ea-871b-0094187c0047"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1AvpkoouQhcmoYzqcbkQd-XAawOC_0-G6\n",
            "To: /content/lercio_headlines.csv\n",
            "100% 487k/487k [00:00<00:00, 108MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1RZnghfnuVRtUxybFiyglLDnWZp4srg6M\n",
            "To: /content/it.parquet\n",
            "100% 4.87M/4.87M [00:00<00:00, 125MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown \"1AvpkoouQhcmoYzqcbkQd-XAawOC_0-G6\"\n",
        "!gdown \"1RZnghfnuVRtUxybFiyglLDnWZp4srg6M\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ck53kB9y9yPc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "PATH_TO_DATA=\"/content/char-rnn-tensorflow/data/\"\n",
        "\n",
        "lercio = pd.read_csv(\"/content/lercio_headlines.csv\", header=None)\n",
        "corpus = pd.read_parquet(\"/content/it.parquet\")\n",
        "corpus['x']=corpus['x'].str.replace(r\"[-!$%^&*()_+|~=`{}\\[\\]:;'<>?\\/]\",'', regex=True)\n",
        "\n",
        "try :\n",
        "  os.mkdir(PATH_TO_DATA+\"lercioheadlines/\")\n",
        "except:\n",
        "  pass\n",
        "try :\n",
        "  os.mkdir(PATH_TO_DATA+\"corpus/\")\n",
        "except:\n",
        "    pass\n",
        "\n",
        "np.savetxt(PATH_TO_DATA+'lercioheadlines/input.txt', lercio.values, fmt='%s')\n",
        "np.savetxt(PATH_TO_DATA+'corpus/input.txt', corpus.values, fmt='%s', \n",
        "           delimiter=' ', newline='\\n', header='')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IBraJgdx_kTt"
      },
      "outputs": [],
      "source": [
        "os.chdir('char-rnn-tensorflow')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gS552jOyFaig"
      },
      "source": [
        "## Random Search\n",
        "\n",
        "Here we are searching through the space of hyperparameters, reducing the time spent in the model selection. For simplicity here are listen only two models, but originally the RS were performed in a stream of 10 models, then the checkpoint of the best models were loaded and compared in generator skills"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9WGPhXLn14k8"
      },
      "outputs": [],
      "source": [
        "parameters = {'model':['gru', 'lstm','nas'],\n",
        "              'rnn_size':[96,128,256],\n",
        "              'seq_length':range(130,140,2), # The average number of chars in a title is 72 so we span the memory of the RNN at max this number\n",
        "              'batch_size':range(200,250,10),\n",
        "              'num_layers':range(2,4,1),\n",
        "              'output_keep_prob':np.arange(0.7,1,0.1),\n",
        "              'input_keep_prob':np.arange(0.8,1,0.1),\n",
        "              'grad_clip':np.arange(3,8,1),\n",
        "              'learning_rate':np.arange(0.007,0.02,0.003),\n",
        "              'decay_rate':np.arange(0.9,1,0.02)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0lStGoskDNPj",
        "outputId": "3550128d-90cb-4dae-aee6-5ad434a9c890"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "python train.py --data_dir=./data/corpus/ --save_dir=/content/drive/Shareddrives/Materials\\ UNI/UNIPI/ISPR/Midterm3\\ Assignment_4/save/save0 --model=nas --num_epochs=50 --rnn_size=256 --seq_length=130 --batch_size=200 --num_layers=2 --output_keep_prob=0.9999999999999999 --input_keep_prob=0.8 --grad_clip=5 --learning_rate=0.019 --decay_rate=0.92\n",
            "python train.py --data_dir=./data/corpus/ --save_dir=/content/drive/Shareddrives/Materials\\ UNI/UNIPI/ISPR/Midterm3\\ Assignment_4/save/save1 --model=nas --num_epochs=50 --rnn_size=256 --seq_length=136 --batch_size=210 --num_layers=2 --output_keep_prob=0.8999999999999999 --input_keep_prob=0.8 --grad_clip=6 --learning_rate=0.007 --decay_rate=0.9800000000000001\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "random_search = ''\n",
        "pars = {}\n",
        "for run in range(0,2):\n",
        "  try :\n",
        "    os.mkdir(f\"./save/save{run}\")\n",
        "  except:\n",
        "    pass\n",
        "  for i, k in enumerate(parameters.keys()):\n",
        "    r = random.randint(0,len(parameters[k]))\n",
        "    pars[i]=(parameters[k][r-1])\n",
        "  random_search = random_search + f'python train.py --data_dir=./data/corpus/ --save_dir=./save/save{run} --model={pars[0]} --num_epochs=50 --rnn_size={pars[1]} --seq_length={pars[2]} --batch_size={pars[3]} --num_layers={pars[4]} --output_keep_prob={pars[5]} --input_keep_prob={pars[6]} --grad_clip={pars[7]} --learning_rate={pars[8]} --decay_rate={pars[9]}\\n'\n",
        "print(random_search)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XEAjdp4ow0BK"
      },
      "outputs": [],
      "source": [
        "with open('file', 'w') as f:\n",
        "    f.write(random_search)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "eval \"$(conda shell.bash hook)\"\n",
        "conda activate myenv\n",
        "bash file"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51a22495-ca42-467e-ec1c-5f15e7c13ba8",
        "id": "wZf7go1_PhpY"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mOutput streaming troncato alle ultime 5000 righe.\u001b[0m\n",
            "19135/20850 (epoch 45), train_loss = 1.925, time/batch = 0.745\n",
            "19136/20850 (epoch 45), train_loss = 1.928, time/batch = 0.689\n",
            "19137/20850 (epoch 45), train_loss = 1.954, time/batch = 0.684\n",
            "19138/20850 (epoch 45), train_loss = 1.900, time/batch = 0.728\n",
            "19139/20850 (epoch 45), train_loss = 1.907, time/batch = 0.683\n",
            "19140/20850 (epoch 45), train_loss = 1.920, time/batch = 0.719\n",
            "19141/20850 (epoch 45), train_loss = 1.907, time/batch = 0.729\n",
            "19142/20850 (epoch 45), train_loss = 1.906, time/batch = 0.725\n",
            "19143/20850 (epoch 45), train_loss = 1.912, time/batch = 0.710\n",
            "19144/20850 (epoch 45), train_loss = 1.940, time/batch = 0.680\n",
            "19145/20850 (epoch 45), train_loss = 1.929, time/batch = 0.718\n",
            "19146/20850 (epoch 45), train_loss = 1.920, time/batch = 0.708\n",
            "19147/20850 (epoch 45), train_loss = 1.920, time/batch = 0.682\n",
            "19148/20850 (epoch 45), train_loss = 1.951, time/batch = 0.670\n",
            "19149/20850 (epoch 45), train_loss = 1.927, time/batch = 0.651\n",
            "19150/20850 (epoch 45), train_loss = 1.951, time/batch = 0.599\n",
            "19151/20850 (epoch 45), train_loss = 1.926, time/batch = 0.505\n",
            "19152/20850 (epoch 45), train_loss = 1.930, time/batch = 0.518\n",
            "19153/20850 (epoch 45), train_loss = 1.945, time/batch = 0.514\n",
            "19154/20850 (epoch 45), train_loss = 1.924, time/batch = 0.520\n",
            "19155/20850 (epoch 45), train_loss = 1.936, time/batch = 0.525\n",
            "19156/20850 (epoch 45), train_loss = 1.935, time/batch = 0.524\n",
            "19157/20850 (epoch 45), train_loss = 1.928, time/batch = 0.517\n",
            "19158/20850 (epoch 45), train_loss = 1.929, time/batch = 0.529\n",
            "19159/20850 (epoch 45), train_loss = 1.897, time/batch = 0.504\n",
            "19160/20850 (epoch 45), train_loss = 1.936, time/batch = 0.519\n",
            "19161/20850 (epoch 45), train_loss = 1.921, time/batch = 0.502\n",
            "19162/20850 (epoch 45), train_loss = 1.934, time/batch = 0.525\n",
            "19163/20850 (epoch 45), train_loss = 1.927, time/batch = 0.511\n",
            "19164/20850 (epoch 45), train_loss = 1.930, time/batch = 0.526\n",
            "19165/20850 (epoch 45), train_loss = 1.911, time/batch = 0.529\n",
            "19166/20850 (epoch 45), train_loss = 1.939, time/batch = 0.530\n",
            "19167/20850 (epoch 45), train_loss = 1.939, time/batch = 0.506\n",
            "19168/20850 (epoch 45), train_loss = 1.940, time/batch = 0.531\n",
            "19169/20850 (epoch 45), train_loss = 1.926, time/batch = 0.546\n",
            "19170/20850 (epoch 45), train_loss = 1.916, time/batch = 0.661\n",
            "19171/20850 (epoch 45), train_loss = 1.909, time/batch = 0.673\n",
            "19172/20850 (epoch 45), train_loss = 1.929, time/batch = 0.657\n",
            "19173/20850 (epoch 45), train_loss = 1.932, time/batch = 0.656\n",
            "19174/20850 (epoch 45), train_loss = 1.915, time/batch = 0.680\n",
            "19175/20850 (epoch 45), train_loss = 1.944, time/batch = 0.653\n",
            "19176/20850 (epoch 45), train_loss = 1.932, time/batch = 0.670\n",
            "19177/20850 (epoch 45), train_loss = 1.938, time/batch = 0.694\n",
            "19178/20850 (epoch 45), train_loss = 1.962, time/batch = 0.663\n",
            "19179/20850 (epoch 45), train_loss = 1.944, time/batch = 0.664\n",
            "19180/20850 (epoch 45), train_loss = 1.936, time/batch = 0.683\n",
            "19181/20850 (epoch 45), train_loss = 1.936, time/batch = 0.667\n",
            "19182/20850 (epoch 46), train_loss = 1.934, time/batch = 0.737\n",
            "19183/20850 (epoch 46), train_loss = 1.915, time/batch = 0.628\n",
            "19184/20850 (epoch 46), train_loss = 1.943, time/batch = 0.665\n",
            "19185/20850 (epoch 46), train_loss = 1.927, time/batch = 0.673\n",
            "19186/20850 (epoch 46), train_loss = 1.906, time/batch = 0.558\n",
            "19187/20850 (epoch 46), train_loss = 1.926, time/batch = 0.504\n",
            "19188/20850 (epoch 46), train_loss = 1.933, time/batch = 0.522\n",
            "19189/20850 (epoch 46), train_loss = 1.929, time/batch = 0.521\n",
            "19190/20850 (epoch 46), train_loss = 1.915, time/batch = 0.542\n",
            "19191/20850 (epoch 46), train_loss = 1.924, time/batch = 0.517\n",
            "19192/20850 (epoch 46), train_loss = 1.907, time/batch = 0.530\n",
            "19193/20850 (epoch 46), train_loss = 1.904, time/batch = 0.520\n",
            "19194/20850 (epoch 46), train_loss = 1.933, time/batch = 0.508\n",
            "19195/20850 (epoch 46), train_loss = 1.954, time/batch = 0.511\n",
            "19196/20850 (epoch 46), train_loss = 1.934, time/batch = 0.525\n",
            "19197/20850 (epoch 46), train_loss = 1.961, time/batch = 0.511\n",
            "19198/20850 (epoch 46), train_loss = 1.934, time/batch = 0.507\n",
            "19199/20850 (epoch 46), train_loss = 1.923, time/batch = 0.506\n",
            "19200/20850 (epoch 46), train_loss = 1.902, time/batch = 0.511\n",
            "19201/20850 (epoch 46), train_loss = 1.909, time/batch = 0.503\n",
            "19202/20850 (epoch 46), train_loss = 1.926, time/batch = 0.505\n",
            "19203/20850 (epoch 46), train_loss = 1.915, time/batch = 0.516\n",
            "19204/20850 (epoch 46), train_loss = 1.941, time/batch = 0.517\n",
            "19205/20850 (epoch 46), train_loss = 1.938, time/batch = 0.571\n",
            "19206/20850 (epoch 46), train_loss = 1.931, time/batch = 0.668\n",
            "19207/20850 (epoch 46), train_loss = 1.921, time/batch = 0.671\n",
            "19208/20850 (epoch 46), train_loss = 1.910, time/batch = 0.668\n",
            "19209/20850 (epoch 46), train_loss = 1.932, time/batch = 0.685\n",
            "19210/20850 (epoch 46), train_loss = 1.925, time/batch = 0.685\n",
            "19211/20850 (epoch 46), train_loss = 1.910, time/batch = 0.661\n",
            "19212/20850 (epoch 46), train_loss = 1.929, time/batch = 0.644\n",
            "19213/20850 (epoch 46), train_loss = 1.930, time/batch = 0.637\n",
            "19214/20850 (epoch 46), train_loss = 1.920, time/batch = 0.669\n",
            "19215/20850 (epoch 46), train_loss = 1.910, time/batch = 0.678\n",
            "19216/20850 (epoch 46), train_loss = 1.904, time/batch = 0.671\n",
            "19217/20850 (epoch 46), train_loss = 1.923, time/batch = 0.661\n",
            "19218/20850 (epoch 46), train_loss = 1.920, time/batch = 0.683\n",
            "19219/20850 (epoch 46), train_loss = 1.910, time/batch = 0.693\n",
            "19220/20850 (epoch 46), train_loss = 1.911, time/batch = 0.688\n",
            "19221/20850 (epoch 46), train_loss = 1.928, time/batch = 0.664\n",
            "19222/20850 (epoch 46), train_loss = 1.930, time/batch = 0.666\n",
            "19223/20850 (epoch 46), train_loss = 1.911, time/batch = 0.664\n",
            "19224/20850 (epoch 46), train_loss = 1.902, time/batch = 0.718\n",
            "19225/20850 (epoch 46), train_loss = 1.948, time/batch = 0.502\n",
            "19226/20850 (epoch 46), train_loss = 1.927, time/batch = 0.518\n",
            "19227/20850 (epoch 46), train_loss = 1.924, time/batch = 0.515\n",
            "19228/20850 (epoch 46), train_loss = 1.924, time/batch = 0.540\n",
            "19229/20850 (epoch 46), train_loss = 1.922, time/batch = 0.528\n",
            "19230/20850 (epoch 46), train_loss = 1.935, time/batch = 0.533\n",
            "19231/20850 (epoch 46), train_loss = 1.911, time/batch = 0.519\n",
            "19232/20850 (epoch 46), train_loss = 1.919, time/batch = 0.526\n",
            "19233/20850 (epoch 46), train_loss = 1.921, time/batch = 0.512\n",
            "19234/20850 (epoch 46), train_loss = 1.913, time/batch = 0.516\n",
            "19235/20850 (epoch 46), train_loss = 1.939, time/batch = 0.501\n",
            "19236/20850 (epoch 46), train_loss = 1.931, time/batch = 0.516\n",
            "19237/20850 (epoch 46), train_loss = 1.923, time/batch = 0.507\n",
            "19238/20850 (epoch 46), train_loss = 1.933, time/batch = 0.530\n",
            "19239/20850 (epoch 46), train_loss = 1.915, time/batch = 0.530\n",
            "19240/20850 (epoch 46), train_loss = 1.933, time/batch = 0.515\n",
            "19241/20850 (epoch 46), train_loss = 1.926, time/batch = 0.511\n",
            "19242/20850 (epoch 46), train_loss = 1.932, time/batch = 0.506\n",
            "19243/20850 (epoch 46), train_loss = 1.933, time/batch = 0.504\n",
            "19244/20850 (epoch 46), train_loss = 1.931, time/batch = 0.605\n",
            "19245/20850 (epoch 46), train_loss = 1.938, time/batch = 0.651\n",
            "19246/20850 (epoch 46), train_loss = 1.919, time/batch = 0.660\n",
            "19247/20850 (epoch 46), train_loss = 1.913, time/batch = 0.663\n",
            "19248/20850 (epoch 46), train_loss = 1.924, time/batch = 0.660\n",
            "19249/20850 (epoch 46), train_loss = 1.911, time/batch = 0.696\n",
            "19250/20850 (epoch 46), train_loss = 1.917, time/batch = 0.672\n",
            "19251/20850 (epoch 46), train_loss = 1.901, time/batch = 0.662\n",
            "19252/20850 (epoch 46), train_loss = 1.902, time/batch = 0.685\n",
            "19253/20850 (epoch 46), train_loss = 1.913, time/batch = 0.667\n",
            "19254/20850 (epoch 46), train_loss = 1.918, time/batch = 0.659\n",
            "19255/20850 (epoch 46), train_loss = 1.915, time/batch = 0.680\n",
            "19256/20850 (epoch 46), train_loss = 1.928, time/batch = 0.668\n",
            "19257/20850 (epoch 46), train_loss = 1.926, time/batch = 0.670\n",
            "19258/20850 (epoch 46), train_loss = 1.896, time/batch = 0.650\n",
            "19259/20850 (epoch 46), train_loss = 1.928, time/batch = 0.678\n",
            "19260/20850 (epoch 46), train_loss = 1.898, time/batch = 0.657\n",
            "19261/20850 (epoch 46), train_loss = 1.908, time/batch = 0.674\n",
            "19262/20850 (epoch 46), train_loss = 1.920, time/batch = 0.671\n",
            "19263/20850 (epoch 46), train_loss = 1.945, time/batch = 0.580\n",
            "19264/20850 (epoch 46), train_loss = 1.928, time/batch = 0.527\n",
            "19265/20850 (epoch 46), train_loss = 1.919, time/batch = 0.499\n",
            "19266/20850 (epoch 46), train_loss = 1.916, time/batch = 0.510\n",
            "19267/20850 (epoch 46), train_loss = 1.923, time/batch = 0.506\n",
            "19268/20850 (epoch 46), train_loss = 1.892, time/batch = 0.512\n",
            "19269/20850 (epoch 46), train_loss = 1.944, time/batch = 0.502\n",
            "19270/20850 (epoch 46), train_loss = 1.922, time/batch = 0.511\n",
            "19271/20850 (epoch 46), train_loss = 1.906, time/batch = 0.511\n",
            "19272/20850 (epoch 46), train_loss = 1.909, time/batch = 0.519\n",
            "19273/20850 (epoch 46), train_loss = 1.903, time/batch = 0.499\n",
            "19274/20850 (epoch 46), train_loss = 1.930, time/batch = 0.511\n",
            "19275/20850 (epoch 46), train_loss = 1.898, time/batch = 0.498\n",
            "19276/20850 (epoch 46), train_loss = 1.900, time/batch = 0.516\n",
            "19277/20850 (epoch 46), train_loss = 1.919, time/batch = 0.520\n",
            "19278/20850 (epoch 46), train_loss = 1.948, time/batch = 0.531\n",
            "19279/20850 (epoch 46), train_loss = 1.903, time/batch = 0.506\n",
            "19280/20850 (epoch 46), train_loss = 1.910, time/batch = 0.522\n",
            "19281/20850 (epoch 46), train_loss = 1.907, time/batch = 0.534\n",
            "19282/20850 (epoch 46), train_loss = 1.934, time/batch = 0.524\n",
            "19283/20850 (epoch 46), train_loss = 1.904, time/batch = 0.664\n",
            "19284/20850 (epoch 46), train_loss = 1.923, time/batch = 0.659\n",
            "19285/20850 (epoch 46), train_loss = 1.935, time/batch = 0.662\n",
            "19286/20850 (epoch 46), train_loss = 1.920, time/batch = 0.670\n",
            "19287/20850 (epoch 46), train_loss = 1.917, time/batch = 0.712\n",
            "19288/20850 (epoch 46), train_loss = 1.903, time/batch = 0.682\n",
            "19289/20850 (epoch 46), train_loss = 1.929, time/batch = 0.675\n",
            "19290/20850 (epoch 46), train_loss = 1.940, time/batch = 0.668\n",
            "19291/20850 (epoch 46), train_loss = 1.905, time/batch = 0.677\n",
            "19292/20850 (epoch 46), train_loss = 1.910, time/batch = 0.667\n",
            "19293/20850 (epoch 46), train_loss = 1.893, time/batch = 0.664\n",
            "19294/20850 (epoch 46), train_loss = 1.898, time/batch = 0.676\n",
            "19295/20850 (epoch 46), train_loss = 1.913, time/batch = 0.640\n",
            "19296/20850 (epoch 46), train_loss = 1.931, time/batch = 0.698\n",
            "19297/20850 (epoch 46), train_loss = 1.942, time/batch = 0.679\n",
            "19298/20850 (epoch 46), train_loss = 1.896, time/batch = 0.669\n",
            "19299/20850 (epoch 46), train_loss = 1.898, time/batch = 0.676\n",
            "19300/20850 (epoch 46), train_loss = 1.912, time/batch = 0.638\n",
            "19301/20850 (epoch 46), train_loss = 1.934, time/batch = 0.680\n",
            "19302/20850 (epoch 46), train_loss = 1.912, time/batch = 0.520\n",
            "19303/20850 (epoch 46), train_loss = 1.922, time/batch = 0.508\n",
            "19304/20850 (epoch 46), train_loss = 1.939, time/batch = 0.515\n",
            "19305/20850 (epoch 46), train_loss = 1.936, time/batch = 0.515\n",
            "19306/20850 (epoch 46), train_loss = 1.918, time/batch = 0.529\n",
            "19307/20850 (epoch 46), train_loss = 1.912, time/batch = 0.516\n",
            "19308/20850 (epoch 46), train_loss = 1.927, time/batch = 0.508\n",
            "19309/20850 (epoch 46), train_loss = 1.911, time/batch = 0.522\n",
            "19310/20850 (epoch 46), train_loss = 1.945, time/batch = 0.515\n",
            "19311/20850 (epoch 46), train_loss = 1.929, time/batch = 0.525\n",
            "19312/20850 (epoch 46), train_loss = 1.934, time/batch = 0.534\n",
            "19313/20850 (epoch 46), train_loss = 1.945, time/batch = 0.505\n",
            "19314/20850 (epoch 46), train_loss = 1.938, time/batch = 0.511\n",
            "19315/20850 (epoch 46), train_loss = 1.920, time/batch = 0.515\n",
            "19316/20850 (epoch 46), train_loss = 1.892, time/batch = 0.507\n",
            "19317/20850 (epoch 46), train_loss = 1.908, time/batch = 0.512\n",
            "19318/20850 (epoch 46), train_loss = 1.919, time/batch = 0.514\n",
            "19319/20850 (epoch 46), train_loss = 1.949, time/batch = 0.506\n",
            "19320/20850 (epoch 46), train_loss = 1.951, time/batch = 0.513\n",
            "19321/20850 (epoch 46), train_loss = 1.939, time/batch = 0.598\n",
            "19322/20850 (epoch 46), train_loss = 1.947, time/batch = 0.674\n",
            "19323/20850 (epoch 46), train_loss = 1.930, time/batch = 0.656\n",
            "19324/20850 (epoch 46), train_loss = 1.930, time/batch = 0.652\n",
            "19325/20850 (epoch 46), train_loss = 1.923, time/batch = 0.651\n",
            "19326/20850 (epoch 46), train_loss = 1.922, time/batch = 0.653\n",
            "19327/20850 (epoch 46), train_loss = 1.933, time/batch = 0.682\n",
            "19328/20850 (epoch 46), train_loss = 1.919, time/batch = 0.680\n",
            "19329/20850 (epoch 46), train_loss = 1.906, time/batch = 0.691\n",
            "19330/20850 (epoch 46), train_loss = 1.921, time/batch = 0.679\n",
            "19331/20850 (epoch 46), train_loss = 1.925, time/batch = 0.662\n",
            "19332/20850 (epoch 46), train_loss = 1.906, time/batch = 0.633\n",
            "19333/20850 (epoch 46), train_loss = 1.949, time/batch = 0.687\n",
            "19334/20850 (epoch 46), train_loss = 1.922, time/batch = 0.668\n",
            "19335/20850 (epoch 46), train_loss = 1.928, time/batch = 0.647\n",
            "19336/20850 (epoch 46), train_loss = 1.945, time/batch = 0.675\n",
            "19337/20850 (epoch 46), train_loss = 1.920, time/batch = 0.674\n",
            "19338/20850 (epoch 46), train_loss = 1.906, time/batch = 0.696\n",
            "19339/20850 (epoch 46), train_loss = 1.920, time/batch = 0.660\n",
            "19340/20850 (epoch 46), train_loss = 1.919, time/batch = 0.606\n",
            "19341/20850 (epoch 46), train_loss = 1.912, time/batch = 0.519\n",
            "19342/20850 (epoch 46), train_loss = 1.905, time/batch = 0.516\n",
            "19343/20850 (epoch 46), train_loss = 1.918, time/batch = 0.507\n",
            "19344/20850 (epoch 46), train_loss = 1.915, time/batch = 0.508\n",
            "19345/20850 (epoch 46), train_loss = 1.920, time/batch = 0.515\n",
            "19346/20850 (epoch 46), train_loss = 1.918, time/batch = 0.533\n",
            "19347/20850 (epoch 46), train_loss = 1.908, time/batch = 0.512\n",
            "19348/20850 (epoch 46), train_loss = 1.916, time/batch = 0.505\n",
            "19349/20850 (epoch 46), train_loss = 1.923, time/batch = 0.514\n",
            "19350/20850 (epoch 46), train_loss = 1.922, time/batch = 0.520\n",
            "19351/20850 (epoch 46), train_loss = 1.909, time/batch = 0.527\n",
            "19352/20850 (epoch 46), train_loss = 1.903, time/batch = 0.513\n",
            "19353/20850 (epoch 46), train_loss = 1.915, time/batch = 0.517\n",
            "19354/20850 (epoch 46), train_loss = 1.908, time/batch = 0.509\n",
            "19355/20850 (epoch 46), train_loss = 1.902, time/batch = 0.521\n",
            "19356/20850 (epoch 46), train_loss = 1.940, time/batch = 0.535\n",
            "19357/20850 (epoch 46), train_loss = 1.922, time/batch = 0.525\n",
            "19358/20850 (epoch 46), train_loss = 1.933, time/batch = 0.516\n",
            "19359/20850 (epoch 46), train_loss = 1.918, time/batch = 0.518\n",
            "19360/20850 (epoch 46), train_loss = 1.939, time/batch = 0.667\n",
            "19361/20850 (epoch 46), train_loss = 1.930, time/batch = 0.699\n",
            "19362/20850 (epoch 46), train_loss = 1.904, time/batch = 0.670\n",
            "19363/20850 (epoch 46), train_loss = 1.924, time/batch = 0.682\n",
            "19364/20850 (epoch 46), train_loss = 1.912, time/batch = 0.699\n",
            "19365/20850 (epoch 46), train_loss = 1.921, time/batch = 0.676\n",
            "19366/20850 (epoch 46), train_loss = 1.899, time/batch = 0.683\n",
            "19367/20850 (epoch 46), train_loss = 1.920, time/batch = 0.695\n",
            "19368/20850 (epoch 46), train_loss = 1.942, time/batch = 0.671\n",
            "19369/20850 (epoch 46), train_loss = 1.936, time/batch = 0.690\n",
            "19370/20850 (epoch 46), train_loss = 1.931, time/batch = 0.698\n",
            "19371/20850 (epoch 46), train_loss = 1.920, time/batch = 0.658\n",
            "19372/20850 (epoch 46), train_loss = 1.919, time/batch = 0.676\n",
            "19373/20850 (epoch 46), train_loss = 1.938, time/batch = 0.655\n",
            "19374/20850 (epoch 46), train_loss = 1.918, time/batch = 0.659\n",
            "19375/20850 (epoch 46), train_loss = 1.917, time/batch = 0.688\n",
            "19376/20850 (epoch 46), train_loss = 1.933, time/batch = 0.664\n",
            "19377/20850 (epoch 46), train_loss = 1.926, time/batch = 0.671\n",
            "19378/20850 (epoch 46), train_loss = 1.930, time/batch = 0.698\n",
            "19379/20850 (epoch 46), train_loss = 1.934, time/batch = 0.531\n",
            "19380/20850 (epoch 46), train_loss = 1.924, time/batch = 0.520\n",
            "19381/20850 (epoch 46), train_loss = 1.914, time/batch = 0.517\n",
            "19382/20850 (epoch 46), train_loss = 1.933, time/batch = 0.525\n",
            "19383/20850 (epoch 46), train_loss = 1.934, time/batch = 0.526\n",
            "19384/20850 (epoch 46), train_loss = 1.904, time/batch = 0.511\n",
            "19385/20850 (epoch 46), train_loss = 1.919, time/batch = 0.519\n",
            "19386/20850 (epoch 46), train_loss = 1.943, time/batch = 0.519\n",
            "19387/20850 (epoch 46), train_loss = 1.959, time/batch = 0.527\n",
            "19388/20850 (epoch 46), train_loss = 1.940, time/batch = 0.518\n",
            "19389/20850 (epoch 46), train_loss = 1.922, time/batch = 0.510\n",
            "19390/20850 (epoch 46), train_loss = 1.902, time/batch = 0.504\n",
            "19391/20850 (epoch 46), train_loss = 1.910, time/batch = 0.521\n",
            "19392/20850 (epoch 46), train_loss = 1.918, time/batch = 0.512\n",
            "19393/20850 (epoch 46), train_loss = 1.943, time/batch = 0.508\n",
            "19394/20850 (epoch 46), train_loss = 1.930, time/batch = 0.566\n",
            "19395/20850 (epoch 46), train_loss = 1.915, time/batch = 0.521\n",
            "19396/20850 (epoch 46), train_loss = 1.913, time/batch = 0.503\n",
            "19397/20850 (epoch 46), train_loss = 1.931, time/batch = 0.499\n",
            "19398/20850 (epoch 46), train_loss = 1.947, time/batch = 0.537\n",
            "19399/20850 (epoch 46), train_loss = 1.920, time/batch = 0.647\n",
            "19400/20850 (epoch 46), train_loss = 1.906, time/batch = 0.664\n",
            "19401/20850 (epoch 46), train_loss = 1.914, time/batch = 0.656\n",
            "19402/20850 (epoch 46), train_loss = 1.922, time/batch = 0.658\n",
            "19403/20850 (epoch 46), train_loss = 1.885, time/batch = 0.679\n",
            "19404/20850 (epoch 46), train_loss = 1.925, time/batch = 0.647\n",
            "19405/20850 (epoch 46), train_loss = 1.903, time/batch = 0.667\n",
            "19406/20850 (epoch 46), train_loss = 1.917, time/batch = 0.653\n",
            "19407/20850 (epoch 46), train_loss = 1.926, time/batch = 0.683\n",
            "19408/20850 (epoch 46), train_loss = 1.937, time/batch = 0.672\n",
            "19409/20850 (epoch 46), train_loss = 1.949, time/batch = 0.651\n",
            "19410/20850 (epoch 46), train_loss = 1.925, time/batch = 0.675\n",
            "19411/20850 (epoch 46), train_loss = 1.902, time/batch = 0.677\n",
            "19412/20850 (epoch 46), train_loss = 1.920, time/batch = 0.653\n",
            "19413/20850 (epoch 46), train_loss = 1.909, time/batch = 0.627\n",
            "19414/20850 (epoch 46), train_loss = 1.909, time/batch = 0.690\n",
            "19415/20850 (epoch 46), train_loss = 1.907, time/batch = 0.663\n",
            "19416/20850 (epoch 46), train_loss = 1.896, time/batch = 0.690\n",
            "19417/20850 (epoch 46), train_loss = 1.925, time/batch = 0.659\n",
            "19418/20850 (epoch 46), train_loss = 1.927, time/batch = 0.525\n",
            "19419/20850 (epoch 46), train_loss = 1.926, time/batch = 0.513\n",
            "19420/20850 (epoch 46), train_loss = 1.930, time/batch = 0.510\n",
            "19421/20850 (epoch 46), train_loss = 1.923, time/batch = 0.521\n",
            "19422/20850 (epoch 46), train_loss = 1.931, time/batch = 0.530\n",
            "19423/20850 (epoch 46), train_loss = 1.920, time/batch = 0.561\n",
            "19424/20850 (epoch 46), train_loss = 1.928, time/batch = 0.520\n",
            "19425/20850 (epoch 46), train_loss = 1.919, time/batch = 0.513\n",
            "19426/20850 (epoch 46), train_loss = 1.927, time/batch = 0.522\n",
            "19427/20850 (epoch 46), train_loss = 1.916, time/batch = 0.520\n",
            "19428/20850 (epoch 46), train_loss = 1.919, time/batch = 0.524\n",
            "19429/20850 (epoch 46), train_loss = 1.930, time/batch = 0.512\n",
            "19430/20850 (epoch 46), train_loss = 1.934, time/batch = 0.502\n",
            "19431/20850 (epoch 46), train_loss = 1.926, time/batch = 0.542\n",
            "19432/20850 (epoch 46), train_loss = 1.949, time/batch = 0.504\n",
            "19433/20850 (epoch 46), train_loss = 1.942, time/batch = 0.522\n",
            "19434/20850 (epoch 46), train_loss = 1.902, time/batch = 0.512\n",
            "19435/20850 (epoch 46), train_loss = 1.908, time/batch = 0.521\n",
            "19436/20850 (epoch 46), train_loss = 1.917, time/batch = 0.500\n",
            "19437/20850 (epoch 46), train_loss = 1.923, time/batch = 0.633\n",
            "19438/20850 (epoch 46), train_loss = 1.923, time/batch = 0.662\n",
            "19439/20850 (epoch 46), train_loss = 1.900, time/batch = 0.652\n",
            "19440/20850 (epoch 46), train_loss = 1.924, time/batch = 0.658\n",
            "19441/20850 (epoch 46), train_loss = 1.938, time/batch = 0.668\n",
            "19442/20850 (epoch 46), train_loss = 1.920, time/batch = 0.637\n",
            "19443/20850 (epoch 46), train_loss = 1.912, time/batch = 0.631\n",
            "19444/20850 (epoch 46), train_loss = 1.931, time/batch = 0.648\n",
            "19445/20850 (epoch 46), train_loss = 1.938, time/batch = 0.663\n",
            "19446/20850 (epoch 46), train_loss = 1.922, time/batch = 0.659\n",
            "19447/20850 (epoch 46), train_loss = 1.905, time/batch = 0.644\n",
            "19448/20850 (epoch 46), train_loss = 1.910, time/batch = 0.668\n",
            "19449/20850 (epoch 46), train_loss = 1.933, time/batch = 0.655\n",
            "19450/20850 (epoch 46), train_loss = 1.921, time/batch = 0.668\n",
            "19451/20850 (epoch 46), train_loss = 1.890, time/batch = 0.659\n",
            "19452/20850 (epoch 46), train_loss = 1.917, time/batch = 0.674\n",
            "19453/20850 (epoch 46), train_loss = 1.933, time/batch = 0.639\n",
            "19454/20850 (epoch 46), train_loss = 1.941, time/batch = 0.668\n",
            "19455/20850 (epoch 46), train_loss = 1.908, time/batch = 0.662\n",
            "19456/20850 (epoch 46), train_loss = 1.908, time/batch = 0.665\n",
            "19457/20850 (epoch 46), train_loss = 1.911, time/batch = 0.520\n",
            "19458/20850 (epoch 46), train_loss = 1.925, time/batch = 0.517\n",
            "19459/20850 (epoch 46), train_loss = 1.908, time/batch = 0.517\n",
            "19460/20850 (epoch 46), train_loss = 1.908, time/batch = 0.518\n",
            "19461/20850 (epoch 46), train_loss = 1.917, time/batch = 0.525\n",
            "19462/20850 (epoch 46), train_loss = 1.914, time/batch = 0.514\n",
            "19463/20850 (epoch 46), train_loss = 1.944, time/batch = 0.530\n",
            "19464/20850 (epoch 46), train_loss = 1.939, time/batch = 0.539\n",
            "19465/20850 (epoch 46), train_loss = 1.926, time/batch = 0.511\n",
            "19466/20850 (epoch 46), train_loss = 1.928, time/batch = 0.515\n",
            "19467/20850 (epoch 46), train_loss = 1.945, time/batch = 0.516\n",
            "19468/20850 (epoch 46), train_loss = 1.923, time/batch = 0.533\n",
            "19469/20850 (epoch 46), train_loss = 1.906, time/batch = 0.517\n",
            "19470/20850 (epoch 46), train_loss = 1.919, time/batch = 0.516\n",
            "19471/20850 (epoch 46), train_loss = 1.926, time/batch = 0.544\n",
            "19472/20850 (epoch 46), train_loss = 1.928, time/batch = 0.511\n",
            "19473/20850 (epoch 46), train_loss = 1.915, time/batch = 0.515\n",
            "19474/20850 (epoch 46), train_loss = 1.923, time/batch = 0.530\n",
            "19475/20850 (epoch 46), train_loss = 1.908, time/batch = 0.537\n",
            "19476/20850 (epoch 46), train_loss = 1.931, time/batch = 0.660\n",
            "19477/20850 (epoch 46), train_loss = 1.908, time/batch = 0.666\n",
            "19478/20850 (epoch 46), train_loss = 1.922, time/batch = 0.679\n",
            "19479/20850 (epoch 46), train_loss = 1.913, time/batch = 0.654\n",
            "19480/20850 (epoch 46), train_loss = 1.926, time/batch = 0.659\n",
            "19481/20850 (epoch 46), train_loss = 1.890, time/batch = 0.653\n",
            "19482/20850 (epoch 46), train_loss = 1.917, time/batch = 0.678\n",
            "19483/20850 (epoch 46), train_loss = 1.895, time/batch = 0.671\n",
            "19484/20850 (epoch 46), train_loss = 1.891, time/batch = 0.678\n",
            "19485/20850 (epoch 46), train_loss = 1.906, time/batch = 0.672\n",
            "19486/20850 (epoch 46), train_loss = 1.902, time/batch = 0.675\n",
            "19487/20850 (epoch 46), train_loss = 1.942, time/batch = 0.662\n",
            "19488/20850 (epoch 46), train_loss = 1.928, time/batch = 0.667\n",
            "19489/20850 (epoch 46), train_loss = 1.908, time/batch = 0.674\n",
            "19490/20850 (epoch 46), train_loss = 1.921, time/batch = 0.653\n",
            "19491/20850 (epoch 46), train_loss = 1.933, time/batch = 0.663\n",
            "19492/20850 (epoch 46), train_loss = 1.903, time/batch = 0.661\n",
            "19493/20850 (epoch 46), train_loss = 1.912, time/batch = 0.671\n",
            "19494/20850 (epoch 46), train_loss = 1.926, time/batch = 0.664\n",
            "19495/20850 (epoch 46), train_loss = 1.911, time/batch = 0.542\n",
            "19496/20850 (epoch 46), train_loss = 1.925, time/batch = 0.529\n",
            "19497/20850 (epoch 46), train_loss = 1.901, time/batch = 0.523\n",
            "19498/20850 (epoch 46), train_loss = 1.929, time/batch = 0.539\n",
            "19499/20850 (epoch 46), train_loss = 1.925, time/batch = 0.532\n",
            "19500/20850 (epoch 46), train_loss = 1.918, time/batch = 0.499\n",
            "19501/20850 (epoch 46), train_loss = 1.912, time/batch = 0.502\n",
            "19502/20850 (epoch 46), train_loss = 1.906, time/batch = 0.512\n",
            "19503/20850 (epoch 46), train_loss = 1.905, time/batch = 0.535\n",
            "19504/20850 (epoch 46), train_loss = 1.902, time/batch = 0.507\n",
            "19505/20850 (epoch 46), train_loss = 1.884, time/batch = 0.521\n",
            "19506/20850 (epoch 46), train_loss = 1.906, time/batch = 0.510\n",
            "19507/20850 (epoch 46), train_loss = 1.899, time/batch = 0.512\n",
            "19508/20850 (epoch 46), train_loss = 1.919, time/batch = 0.508\n",
            "19509/20850 (epoch 46), train_loss = 1.904, time/batch = 0.533\n",
            "19510/20850 (epoch 46), train_loss = 1.908, time/batch = 0.516\n",
            "19511/20850 (epoch 46), train_loss = 1.906, time/batch = 0.515\n",
            "19512/20850 (epoch 46), train_loss = 1.919, time/batch = 0.510\n",
            "19513/20850 (epoch 46), train_loss = 1.899, time/batch = 0.520\n",
            "19514/20850 (epoch 46), train_loss = 1.901, time/batch = 0.581\n",
            "19515/20850 (epoch 46), train_loss = 1.923, time/batch = 0.675\n",
            "19516/20850 (epoch 46), train_loss = 1.890, time/batch = 0.651\n",
            "19517/20850 (epoch 46), train_loss = 1.884, time/batch = 0.673\n",
            "19518/20850 (epoch 46), train_loss = 1.913, time/batch = 0.644\n",
            "19519/20850 (epoch 46), train_loss = 1.896, time/batch = 0.676\n",
            "19520/20850 (epoch 46), train_loss = 1.911, time/batch = 0.661\n",
            "19521/20850 (epoch 46), train_loss = 1.938, time/batch = 0.670\n",
            "19522/20850 (epoch 46), train_loss = 1.927, time/batch = 0.671\n",
            "19523/20850 (epoch 46), train_loss = 1.902, time/batch = 0.659\n",
            "19524/20850 (epoch 46), train_loss = 1.921, time/batch = 0.652\n",
            "19525/20850 (epoch 46), train_loss = 1.918, time/batch = 0.653\n",
            "19526/20850 (epoch 46), train_loss = 1.935, time/batch = 0.655\n",
            "19527/20850 (epoch 46), train_loss = 1.943, time/batch = 0.630\n",
            "19528/20850 (epoch 46), train_loss = 1.912, time/batch = 0.676\n",
            "19529/20850 (epoch 46), train_loss = 1.925, time/batch = 0.646\n",
            "19530/20850 (epoch 46), train_loss = 1.932, time/batch = 0.665\n",
            "19531/20850 (epoch 46), train_loss = 1.923, time/batch = 0.661\n",
            "19532/20850 (epoch 46), train_loss = 1.922, time/batch = 0.666\n",
            "19533/20850 (epoch 46), train_loss = 1.933, time/batch = 0.684\n",
            "19534/20850 (epoch 46), train_loss = 1.933, time/batch = 0.508\n",
            "19535/20850 (epoch 46), train_loss = 1.929, time/batch = 0.510\n",
            "19536/20850 (epoch 46), train_loss = 1.917, time/batch = 0.500\n",
            "19537/20850 (epoch 46), train_loss = 1.913, time/batch = 0.511\n",
            "19538/20850 (epoch 46), train_loss = 1.935, time/batch = 0.508\n",
            "19539/20850 (epoch 46), train_loss = 1.924, time/batch = 0.507\n",
            "19540/20850 (epoch 46), train_loss = 1.915, time/batch = 0.500\n",
            "19541/20850 (epoch 46), train_loss = 1.907, time/batch = 0.509\n",
            "19542/20850 (epoch 46), train_loss = 1.927, time/batch = 0.510\n",
            "19543/20850 (epoch 46), train_loss = 1.902, time/batch = 0.504\n",
            "19544/20850 (epoch 46), train_loss = 1.915, time/batch = 0.523\n",
            "19545/20850 (epoch 46), train_loss = 1.940, time/batch = 0.509\n",
            "19546/20850 (epoch 46), train_loss = 1.910, time/batch = 0.529\n",
            "19547/20850 (epoch 46), train_loss = 1.927, time/batch = 0.517\n",
            "19548/20850 (epoch 46), train_loss = 1.893, time/batch = 0.535\n",
            "19549/20850 (epoch 46), train_loss = 1.919, time/batch = 0.529\n",
            "19550/20850 (epoch 46), train_loss = 1.921, time/batch = 0.519\n",
            "19551/20850 (epoch 46), train_loss = 1.916, time/batch = 0.508\n",
            "19552/20850 (epoch 46), train_loss = 1.909, time/batch = 0.515\n",
            "19553/20850 (epoch 46), train_loss = 1.919, time/batch = 0.574\n",
            "19554/20850 (epoch 46), train_loss = 1.940, time/batch = 0.671\n",
            "19555/20850 (epoch 46), train_loss = 1.892, time/batch = 0.664\n",
            "19556/20850 (epoch 46), train_loss = 1.899, time/batch = 0.681\n",
            "19557/20850 (epoch 46), train_loss = 1.907, time/batch = 0.684\n",
            "19558/20850 (epoch 46), train_loss = 1.895, time/batch = 0.670\n",
            "19559/20850 (epoch 46), train_loss = 1.891, time/batch = 0.674\n",
            "19560/20850 (epoch 46), train_loss = 1.903, time/batch = 0.667\n",
            "19561/20850 (epoch 46), train_loss = 1.932, time/batch = 0.661\n",
            "19562/20850 (epoch 46), train_loss = 1.916, time/batch = 0.654\n",
            "19563/20850 (epoch 46), train_loss = 1.911, time/batch = 0.679\n",
            "19564/20850 (epoch 46), train_loss = 1.909, time/batch = 0.669\n",
            "19565/20850 (epoch 46), train_loss = 1.943, time/batch = 0.656\n",
            "19566/20850 (epoch 46), train_loss = 1.922, time/batch = 0.674\n",
            "19567/20850 (epoch 46), train_loss = 1.941, time/batch = 0.661\n",
            "19568/20850 (epoch 46), train_loss = 1.916, time/batch = 0.679\n",
            "19569/20850 (epoch 46), train_loss = 1.918, time/batch = 0.679\n",
            "19570/20850 (epoch 46), train_loss = 1.935, time/batch = 0.663\n",
            "19571/20850 (epoch 46), train_loss = 1.910, time/batch = 0.705\n",
            "19572/20850 (epoch 46), train_loss = 1.928, time/batch = 0.621\n",
            "19573/20850 (epoch 46), train_loss = 1.923, time/batch = 0.530\n",
            "19574/20850 (epoch 46), train_loss = 1.924, time/batch = 0.520\n",
            "19575/20850 (epoch 46), train_loss = 1.919, time/batch = 0.525\n",
            "19576/20850 (epoch 46), train_loss = 1.886, time/batch = 0.525\n",
            "19577/20850 (epoch 46), train_loss = 1.923, time/batch = 0.516\n",
            "19578/20850 (epoch 46), train_loss = 1.909, time/batch = 0.511\n",
            "19579/20850 (epoch 46), train_loss = 1.920, time/batch = 0.512\n",
            "19580/20850 (epoch 46), train_loss = 1.916, time/batch = 0.508\n",
            "19581/20850 (epoch 46), train_loss = 1.918, time/batch = 0.510\n",
            "19582/20850 (epoch 46), train_loss = 1.903, time/batch = 0.518\n",
            "19583/20850 (epoch 46), train_loss = 1.927, time/batch = 0.511\n",
            "19584/20850 (epoch 46), train_loss = 1.928, time/batch = 0.523\n",
            "19585/20850 (epoch 46), train_loss = 1.934, time/batch = 0.539\n",
            "19586/20850 (epoch 46), train_loss = 1.912, time/batch = 0.530\n",
            "19587/20850 (epoch 46), train_loss = 1.900, time/batch = 0.519\n",
            "19588/20850 (epoch 46), train_loss = 1.899, time/batch = 0.526\n",
            "19589/20850 (epoch 46), train_loss = 1.917, time/batch = 0.510\n",
            "19590/20850 (epoch 46), train_loss = 1.917, time/batch = 0.517\n",
            "19591/20850 (epoch 46), train_loss = 1.905, time/batch = 0.536\n",
            "19592/20850 (epoch 46), train_loss = 1.931, time/batch = 0.678\n",
            "19593/20850 (epoch 46), train_loss = 1.926, time/batch = 0.643\n",
            "19594/20850 (epoch 46), train_loss = 1.919, time/batch = 0.668\n",
            "19595/20850 (epoch 46), train_loss = 1.948, time/batch = 0.671\n",
            "19596/20850 (epoch 46), train_loss = 1.934, time/batch = 0.658\n",
            "19597/20850 (epoch 46), train_loss = 1.925, time/batch = 0.647\n",
            "19598/20850 (epoch 46), train_loss = 1.920, time/batch = 0.661\n",
            "19599/20850 (epoch 47), train_loss = 1.926, time/batch = 0.759\n",
            "19600/20850 (epoch 47), train_loss = 1.901, time/batch = 0.666\n",
            "19601/20850 (epoch 47), train_loss = 1.930, time/batch = 0.662\n",
            "19602/20850 (epoch 47), train_loss = 1.916, time/batch = 0.688\n",
            "19603/20850 (epoch 47), train_loss = 1.890, time/batch = 0.674\n",
            "19604/20850 (epoch 47), train_loss = 1.913, time/batch = 0.679\n",
            "19605/20850 (epoch 47), train_loss = 1.924, time/batch = 0.691\n",
            "19606/20850 (epoch 47), train_loss = 1.917, time/batch = 0.694\n",
            "19607/20850 (epoch 47), train_loss = 1.899, time/batch = 0.683\n",
            "19608/20850 (epoch 47), train_loss = 1.916, time/batch = 0.524\n",
            "19609/20850 (epoch 47), train_loss = 1.893, time/batch = 0.518\n",
            "19610/20850 (epoch 47), train_loss = 1.895, time/batch = 0.530\n",
            "19611/20850 (epoch 47), train_loss = 1.918, time/batch = 0.520\n",
            "19612/20850 (epoch 47), train_loss = 1.947, time/batch = 0.523\n",
            "19613/20850 (epoch 47), train_loss = 1.919, time/batch = 0.520\n",
            "19614/20850 (epoch 47), train_loss = 1.951, time/batch = 0.518\n",
            "19615/20850 (epoch 47), train_loss = 1.922, time/batch = 0.516\n",
            "19616/20850 (epoch 47), train_loss = 1.910, time/batch = 0.532\n",
            "19617/20850 (epoch 47), train_loss = 1.892, time/batch = 0.536\n",
            "19618/20850 (epoch 47), train_loss = 1.899, time/batch = 0.513\n",
            "19619/20850 (epoch 47), train_loss = 1.914, time/batch = 0.514\n",
            "19620/20850 (epoch 47), train_loss = 1.906, time/batch = 0.515\n",
            "19621/20850 (epoch 47), train_loss = 1.926, time/batch = 0.519\n",
            "19622/20850 (epoch 47), train_loss = 1.929, time/batch = 0.512\n",
            "19623/20850 (epoch 47), train_loss = 1.924, time/batch = 0.521\n",
            "19624/20850 (epoch 47), train_loss = 1.911, time/batch = 0.522\n",
            "19625/20850 (epoch 47), train_loss = 1.904, time/batch = 0.512\n",
            "19626/20850 (epoch 47), train_loss = 1.919, time/batch = 0.514\n",
            "19627/20850 (epoch 47), train_loss = 1.912, time/batch = 0.619\n",
            "19628/20850 (epoch 47), train_loss = 1.897, time/batch = 0.655\n",
            "19629/20850 (epoch 47), train_loss = 1.919, time/batch = 0.697\n",
            "19630/20850 (epoch 47), train_loss = 1.918, time/batch = 0.663\n",
            "19631/20850 (epoch 47), train_loss = 1.906, time/batch = 0.659\n",
            "19632/20850 (epoch 47), train_loss = 1.897, time/batch = 0.667\n",
            "19633/20850 (epoch 47), train_loss = 1.895, time/batch = 0.620\n",
            "19634/20850 (epoch 47), train_loss = 1.910, time/batch = 0.708\n",
            "19635/20850 (epoch 47), train_loss = 1.914, time/batch = 0.697\n",
            "19636/20850 (epoch 47), train_loss = 1.899, time/batch = 0.617\n",
            "19637/20850 (epoch 47), train_loss = 1.898, time/batch = 0.674\n",
            "19638/20850 (epoch 47), train_loss = 1.922, time/batch = 0.687\n",
            "19639/20850 (epoch 47), train_loss = 1.918, time/batch = 0.681\n",
            "19640/20850 (epoch 47), train_loss = 1.900, time/batch = 0.676\n",
            "19641/20850 (epoch 47), train_loss = 1.890, time/batch = 0.678\n",
            "19642/20850 (epoch 47), train_loss = 1.938, time/batch = 0.654\n",
            "19643/20850 (epoch 47), train_loss = 1.917, time/batch = 0.668\n",
            "19644/20850 (epoch 47), train_loss = 1.916, time/batch = 0.682\n",
            "19645/20850 (epoch 47), train_loss = 1.914, time/batch = 0.686\n",
            "19646/20850 (epoch 47), train_loss = 1.911, time/batch = 0.647\n",
            "19647/20850 (epoch 47), train_loss = 1.922, time/batch = 0.511\n",
            "19648/20850 (epoch 47), train_loss = 1.901, time/batch = 0.519\n",
            "19649/20850 (epoch 47), train_loss = 1.905, time/batch = 0.509\n",
            "19650/20850 (epoch 47), train_loss = 1.911, time/batch = 0.519\n",
            "19651/20850 (epoch 47), train_loss = 1.901, time/batch = 0.514\n",
            "19652/20850 (epoch 47), train_loss = 1.929, time/batch = 0.510\n",
            "19653/20850 (epoch 47), train_loss = 1.914, time/batch = 0.512\n",
            "19654/20850 (epoch 47), train_loss = 1.914, time/batch = 0.515\n",
            "19655/20850 (epoch 47), train_loss = 1.920, time/batch = 0.518\n",
            "19656/20850 (epoch 47), train_loss = 1.901, time/batch = 0.523\n",
            "19657/20850 (epoch 47), train_loss = 1.925, time/batch = 0.520\n",
            "19658/20850 (epoch 47), train_loss = 1.913, time/batch = 0.528\n",
            "19659/20850 (epoch 47), train_loss = 1.921, time/batch = 0.518\n",
            "19660/20850 (epoch 47), train_loss = 1.918, time/batch = 0.509\n",
            "19661/20850 (epoch 47), train_loss = 1.919, time/batch = 0.509\n",
            "19662/20850 (epoch 47), train_loss = 1.927, time/batch = 0.520\n",
            "19663/20850 (epoch 47), train_loss = 1.909, time/batch = 0.511\n",
            "19664/20850 (epoch 47), train_loss = 1.896, time/batch = 0.547\n",
            "19665/20850 (epoch 47), train_loss = 1.914, time/batch = 0.516\n",
            "19666/20850 (epoch 47), train_loss = 1.902, time/batch = 0.660\n",
            "19667/20850 (epoch 47), train_loss = 1.908, time/batch = 0.628\n",
            "19668/20850 (epoch 47), train_loss = 1.888, time/batch = 0.683\n",
            "19669/20850 (epoch 47), train_loss = 1.886, time/batch = 0.688\n",
            "19670/20850 (epoch 47), train_loss = 1.901, time/batch = 0.675\n",
            "19671/20850 (epoch 47), train_loss = 1.903, time/batch = 0.715\n",
            "19672/20850 (epoch 47), train_loss = 1.905, time/batch = 0.678\n",
            "19673/20850 (epoch 47), train_loss = 1.918, time/batch = 0.665\n",
            "19674/20850 (epoch 47), train_loss = 1.918, time/batch = 0.694\n",
            "19675/20850 (epoch 47), train_loss = 1.884, time/batch = 0.662\n",
            "19676/20850 (epoch 47), train_loss = 1.921, time/batch = 0.665\n",
            "19677/20850 (epoch 47), train_loss = 1.891, time/batch = 0.661\n",
            "19678/20850 (epoch 47), train_loss = 1.901, time/batch = 0.673\n",
            "19679/20850 (epoch 47), train_loss = 1.911, time/batch = 0.671\n",
            "19680/20850 (epoch 47), train_loss = 1.936, time/batch = 0.649\n",
            "19681/20850 (epoch 47), train_loss = 1.915, time/batch = 0.659\n",
            "19682/20850 (epoch 47), train_loss = 1.903, time/batch = 0.667\n",
            "19683/20850 (epoch 47), train_loss = 1.897, time/batch = 0.654\n",
            "19684/20850 (epoch 47), train_loss = 1.910, time/batch = 0.706\n",
            "19685/20850 (epoch 47), train_loss = 1.878, time/batch = 0.578\n",
            "19686/20850 (epoch 47), train_loss = 1.928, time/batch = 0.501\n",
            "19687/20850 (epoch 47), train_loss = 1.907, time/batch = 0.510\n",
            "19688/20850 (epoch 47), train_loss = 1.894, time/batch = 0.512\n",
            "19689/20850 (epoch 47), train_loss = 1.901, time/batch = 0.520\n",
            "19690/20850 (epoch 47), train_loss = 1.891, time/batch = 0.518\n",
            "19691/20850 (epoch 47), train_loss = 1.922, time/batch = 0.513\n",
            "19692/20850 (epoch 47), train_loss = 1.890, time/batch = 0.507\n",
            "19693/20850 (epoch 47), train_loss = 1.887, time/batch = 0.501\n",
            "19694/20850 (epoch 47), train_loss = 1.910, time/batch = 0.502\n",
            "19695/20850 (epoch 47), train_loss = 1.936, time/batch = 0.506\n",
            "19696/20850 (epoch 47), train_loss = 1.891, time/batch = 0.507\n",
            "19697/20850 (epoch 47), train_loss = 1.902, time/batch = 0.513\n",
            "19698/20850 (epoch 47), train_loss = 1.897, time/batch = 0.518\n",
            "19699/20850 (epoch 47), train_loss = 1.928, time/batch = 0.499\n",
            "19700/20850 (epoch 47), train_loss = 1.896, time/batch = 0.517\n",
            "19701/20850 (epoch 47), train_loss = 1.912, time/batch = 0.517\n",
            "19702/20850 (epoch 47), train_loss = 1.925, time/batch = 0.515\n",
            "19703/20850 (epoch 47), train_loss = 1.910, time/batch = 0.503\n",
            "19704/20850 (epoch 47), train_loss = 1.903, time/batch = 0.504\n",
            "19705/20850 (epoch 47), train_loss = 1.889, time/batch = 0.637\n",
            "19706/20850 (epoch 47), train_loss = 1.916, time/batch = 0.661\n",
            "19707/20850 (epoch 47), train_loss = 1.928, time/batch = 0.679\n",
            "19708/20850 (epoch 47), train_loss = 1.894, time/batch = 0.670\n",
            "19709/20850 (epoch 47), train_loss = 1.897, time/batch = 0.711\n",
            "19710/20850 (epoch 47), train_loss = 1.880, time/batch = 0.655\n",
            "19711/20850 (epoch 47), train_loss = 1.884, time/batch = 0.654\n",
            "19712/20850 (epoch 47), train_loss = 1.907, time/batch = 0.663\n",
            "19713/20850 (epoch 47), train_loss = 1.925, time/batch = 0.661\n",
            "19714/20850 (epoch 47), train_loss = 1.929, time/batch = 0.681\n",
            "19715/20850 (epoch 47), train_loss = 1.883, time/batch = 0.657\n",
            "19716/20850 (epoch 47), train_loss = 1.887, time/batch = 0.651\n",
            "19717/20850 (epoch 47), train_loss = 1.904, time/batch = 0.654\n",
            "19718/20850 (epoch 47), train_loss = 1.919, time/batch = 0.676\n",
            "19719/20850 (epoch 47), train_loss = 1.904, time/batch = 0.657\n",
            "19720/20850 (epoch 47), train_loss = 1.916, time/batch = 0.654\n",
            "19721/20850 (epoch 47), train_loss = 1.929, time/batch = 0.671\n",
            "19722/20850 (epoch 47), train_loss = 1.924, time/batch = 0.660\n",
            "19723/20850 (epoch 47), train_loss = 1.906, time/batch = 0.673\n",
            "19724/20850 (epoch 47), train_loss = 1.898, time/batch = 0.618\n",
            "19725/20850 (epoch 47), train_loss = 1.918, time/batch = 0.511\n",
            "19726/20850 (epoch 47), train_loss = 1.901, time/batch = 0.514\n",
            "19727/20850 (epoch 47), train_loss = 1.935, time/batch = 0.511\n",
            "19728/20850 (epoch 47), train_loss = 1.919, time/batch = 0.519\n",
            "19729/20850 (epoch 47), train_loss = 1.921, time/batch = 0.505\n",
            "19730/20850 (epoch 47), train_loss = 1.936, time/batch = 0.506\n",
            "19731/20850 (epoch 47), train_loss = 1.925, time/batch = 0.505\n",
            "19732/20850 (epoch 47), train_loss = 1.909, time/batch = 0.508\n",
            "19733/20850 (epoch 47), train_loss = 1.882, time/batch = 0.510\n",
            "19734/20850 (epoch 47), train_loss = 1.894, time/batch = 0.527\n",
            "19735/20850 (epoch 47), train_loss = 1.904, time/batch = 0.517\n",
            "19736/20850 (epoch 47), train_loss = 1.938, time/batch = 0.508\n",
            "19737/20850 (epoch 47), train_loss = 1.943, time/batch = 0.528\n",
            "19738/20850 (epoch 47), train_loss = 1.931, time/batch = 0.512\n",
            "19739/20850 (epoch 47), train_loss = 1.935, time/batch = 0.519\n",
            "19740/20850 (epoch 47), train_loss = 1.923, time/batch = 0.509\n",
            "19741/20850 (epoch 47), train_loss = 1.917, time/batch = 0.525\n",
            "19742/20850 (epoch 47), train_loss = 1.915, time/batch = 0.507\n",
            "19743/20850 (epoch 47), train_loss = 1.909, time/batch = 0.519\n",
            "19744/20850 (epoch 47), train_loss = 1.921, time/batch = 0.640\n",
            "19745/20850 (epoch 47), train_loss = 1.908, time/batch = 0.645\n",
            "19746/20850 (epoch 47), train_loss = 1.897, time/batch = 0.655\n",
            "19747/20850 (epoch 47), train_loss = 1.907, time/batch = 0.654\n",
            "19748/20850 (epoch 47), train_loss = 1.909, time/batch = 0.642\n",
            "19749/20850 (epoch 47), train_loss = 1.896, time/batch = 0.653\n",
            "19750/20850 (epoch 47), train_loss = 1.941, time/batch = 0.651\n",
            "19751/20850 (epoch 47), train_loss = 1.908, time/batch = 0.680\n",
            "19752/20850 (epoch 47), train_loss = 1.912, time/batch = 0.663\n",
            "19753/20850 (epoch 47), train_loss = 1.930, time/batch = 0.665\n",
            "19754/20850 (epoch 47), train_loss = 1.911, time/batch = 0.677\n",
            "19755/20850 (epoch 47), train_loss = 1.895, time/batch = 0.664\n",
            "19756/20850 (epoch 47), train_loss = 1.909, time/batch = 0.641\n",
            "19757/20850 (epoch 47), train_loss = 1.912, time/batch = 0.674\n",
            "19758/20850 (epoch 47), train_loss = 1.901, time/batch = 0.654\n",
            "19759/20850 (epoch 47), train_loss = 1.897, time/batch = 0.658\n",
            "19760/20850 (epoch 47), train_loss = 1.902, time/batch = 0.661\n",
            "19761/20850 (epoch 47), train_loss = 1.903, time/batch = 0.672\n",
            "19762/20850 (epoch 47), train_loss = 1.908, time/batch = 0.665\n",
            "19763/20850 (epoch 47), train_loss = 1.906, time/batch = 0.590\n",
            "19764/20850 (epoch 47), train_loss = 1.894, time/batch = 0.512\n",
            "19765/20850 (epoch 47), train_loss = 1.902, time/batch = 0.524\n",
            "19766/20850 (epoch 47), train_loss = 1.910, time/batch = 0.503\n",
            "19767/20850 (epoch 47), train_loss = 1.909, time/batch = 0.522\n",
            "19768/20850 (epoch 47), train_loss = 1.901, time/batch = 0.525\n",
            "19769/20850 (epoch 47), train_loss = 1.897, time/batch = 0.557\n",
            "19770/20850 (epoch 47), train_loss = 1.905, time/batch = 0.505\n",
            "19771/20850 (epoch 47), train_loss = 1.898, time/batch = 0.509\n",
            "19772/20850 (epoch 47), train_loss = 1.886, time/batch = 0.532\n",
            "19773/20850 (epoch 47), train_loss = 1.927, time/batch = 0.512\n",
            "19774/20850 (epoch 47), train_loss = 1.908, time/batch = 0.506\n",
            "19775/20850 (epoch 47), train_loss = 1.919, time/batch = 0.520\n",
            "19776/20850 (epoch 47), train_loss = 1.908, time/batch = 0.532\n",
            "19777/20850 (epoch 47), train_loss = 1.920, time/batch = 0.541\n",
            "19778/20850 (epoch 47), train_loss = 1.918, time/batch = 0.511\n",
            "19779/20850 (epoch 47), train_loss = 1.891, time/batch = 0.513\n",
            "19780/20850 (epoch 47), train_loss = 1.918, time/batch = 0.513\n",
            "19781/20850 (epoch 47), train_loss = 1.901, time/batch = 0.526\n",
            "19782/20850 (epoch 47), train_loss = 1.912, time/batch = 0.543\n",
            "19783/20850 (epoch 47), train_loss = 1.883, time/batch = 0.659\n",
            "19784/20850 (epoch 47), train_loss = 1.910, time/batch = 0.645\n",
            "19785/20850 (epoch 47), train_loss = 1.923, time/batch = 0.599\n",
            "19786/20850 (epoch 47), train_loss = 1.924, time/batch = 0.668\n",
            "19787/20850 (epoch 47), train_loss = 1.923, time/batch = 0.650\n",
            "19788/20850 (epoch 47), train_loss = 1.910, time/batch = 0.676\n",
            "19789/20850 (epoch 47), train_loss = 1.907, time/batch = 0.693\n",
            "19790/20850 (epoch 47), train_loss = 1.925, time/batch = 0.663\n",
            "19791/20850 (epoch 47), train_loss = 1.905, time/batch = 0.644\n",
            "19792/20850 (epoch 47), train_loss = 1.909, time/batch = 0.658\n",
            "19793/20850 (epoch 47), train_loss = 1.922, time/batch = 0.674\n",
            "19794/20850 (epoch 47), train_loss = 1.915, time/batch = 0.678\n",
            "19795/20850 (epoch 47), train_loss = 1.919, time/batch = 0.661\n",
            "19796/20850 (epoch 47), train_loss = 1.923, time/batch = 0.659\n",
            "19797/20850 (epoch 47), train_loss = 1.918, time/batch = 0.663\n",
            "19798/20850 (epoch 47), train_loss = 1.906, time/batch = 0.662\n",
            "19799/20850 (epoch 47), train_loss = 1.917, time/batch = 0.670\n",
            "19800/20850 (epoch 47), train_loss = 1.920, time/batch = 0.659\n",
            "19801/20850 (epoch 47), train_loss = 1.897, time/batch = 0.664\n",
            "19802/20850 (epoch 47), train_loss = 1.906, time/batch = 0.519\n",
            "19803/20850 (epoch 47), train_loss = 1.931, time/batch = 0.532\n",
            "19804/20850 (epoch 47), train_loss = 1.951, time/batch = 0.510\n",
            "19805/20850 (epoch 47), train_loss = 1.925, time/batch = 0.517\n",
            "19806/20850 (epoch 47), train_loss = 1.909, time/batch = 0.507\n",
            "19807/20850 (epoch 47), train_loss = 1.891, time/batch = 0.508\n",
            "19808/20850 (epoch 47), train_loss = 1.902, time/batch = 0.517\n",
            "19809/20850 (epoch 47), train_loss = 1.904, time/batch = 0.519\n",
            "19810/20850 (epoch 47), train_loss = 1.927, time/batch = 0.506\n",
            "19811/20850 (epoch 47), train_loss = 1.919, time/batch = 0.520\n",
            "19812/20850 (epoch 47), train_loss = 1.903, time/batch = 0.512\n",
            "19813/20850 (epoch 47), train_loss = 1.898, time/batch = 0.509\n",
            "19814/20850 (epoch 47), train_loss = 1.920, time/batch = 0.508\n",
            "19815/20850 (epoch 47), train_loss = 1.938, time/batch = 0.509\n",
            "19816/20850 (epoch 47), train_loss = 1.908, time/batch = 0.507\n",
            "19817/20850 (epoch 47), train_loss = 1.895, time/batch = 0.506\n",
            "19818/20850 (epoch 47), train_loss = 1.904, time/batch = 0.507\n",
            "19819/20850 (epoch 47), train_loss = 1.913, time/batch = 0.508\n",
            "19820/20850 (epoch 47), train_loss = 1.872, time/batch = 0.501\n",
            "19821/20850 (epoch 47), train_loss = 1.910, time/batch = 0.540\n",
            "19822/20850 (epoch 47), train_loss = 1.892, time/batch = 0.656\n",
            "19823/20850 (epoch 47), train_loss = 1.905, time/batch = 0.669\n",
            "19824/20850 (epoch 47), train_loss = 1.916, time/batch = 0.659\n",
            "19825/20850 (epoch 47), train_loss = 1.923, time/batch = 0.682\n",
            "19826/20850 (epoch 47), train_loss = 1.939, time/batch = 0.671\n",
            "19827/20850 (epoch 47), train_loss = 1.918, time/batch = 0.660\n",
            "19828/20850 (epoch 47), train_loss = 1.894, time/batch = 0.645\n",
            "19829/20850 (epoch 47), train_loss = 1.910, time/batch = 0.666\n",
            "19830/20850 (epoch 47), train_loss = 1.900, time/batch = 0.666\n",
            "19831/20850 (epoch 47), train_loss = 1.892, time/batch = 0.698\n",
            "19832/20850 (epoch 47), train_loss = 1.894, time/batch = 0.690\n",
            "19833/20850 (epoch 47), train_loss = 1.887, time/batch = 0.656\n",
            "19834/20850 (epoch 47), train_loss = 1.917, time/batch = 0.658\n",
            "19835/20850 (epoch 47), train_loss = 1.915, time/batch = 0.659\n",
            "19836/20850 (epoch 47), train_loss = 1.913, time/batch = 0.679\n",
            "19837/20850 (epoch 47), train_loss = 1.913, time/batch = 0.669\n",
            "19838/20850 (epoch 47), train_loss = 1.912, time/batch = 0.663\n",
            "19839/20850 (epoch 47), train_loss = 1.921, time/batch = 0.697\n",
            "19840/20850 (epoch 47), train_loss = 1.905, time/batch = 0.628\n",
            "19841/20850 (epoch 47), train_loss = 1.912, time/batch = 0.513\n",
            "19842/20850 (epoch 47), train_loss = 1.911, time/batch = 0.525\n",
            "19843/20850 (epoch 47), train_loss = 1.912, time/batch = 0.522\n",
            "19844/20850 (epoch 47), train_loss = 1.905, time/batch = 0.507\n",
            "19845/20850 (epoch 47), train_loss = 1.905, time/batch = 0.534\n",
            "19846/20850 (epoch 47), train_loss = 1.916, time/batch = 0.511\n",
            "19847/20850 (epoch 47), train_loss = 1.928, time/batch = 0.504\n",
            "19848/20850 (epoch 47), train_loss = 1.911, time/batch = 0.506\n",
            "19849/20850 (epoch 47), train_loss = 1.942, time/batch = 0.507\n",
            "19850/20850 (epoch 47), train_loss = 1.926, time/batch = 0.512\n",
            "19851/20850 (epoch 47), train_loss = 1.894, time/batch = 0.513\n",
            "19852/20850 (epoch 47), train_loss = 1.900, time/batch = 0.512\n",
            "19853/20850 (epoch 47), train_loss = 1.905, time/batch = 0.516\n",
            "19854/20850 (epoch 47), train_loss = 1.913, time/batch = 0.516\n",
            "19855/20850 (epoch 47), train_loss = 1.910, time/batch = 0.504\n",
            "19856/20850 (epoch 47), train_loss = 1.889, time/batch = 0.510\n",
            "19857/20850 (epoch 47), train_loss = 1.912, time/batch = 0.517\n",
            "19858/20850 (epoch 47), train_loss = 1.922, time/batch = 0.510\n",
            "19859/20850 (epoch 47), train_loss = 1.912, time/batch = 0.517\n",
            "19860/20850 (epoch 47), train_loss = 1.903, time/batch = 0.628\n",
            "19861/20850 (epoch 47), train_loss = 1.920, time/batch = 0.650\n",
            "19862/20850 (epoch 47), train_loss = 1.927, time/batch = 0.670\n",
            "19863/20850 (epoch 47), train_loss = 1.907, time/batch = 0.652\n",
            "19864/20850 (epoch 47), train_loss = 1.892, time/batch = 0.662\n",
            "19865/20850 (epoch 47), train_loss = 1.899, time/batch = 0.665\n",
            "19866/20850 (epoch 47), train_loss = 1.924, time/batch = 0.698\n",
            "19867/20850 (epoch 47), train_loss = 1.915, time/batch = 0.661\n",
            "19868/20850 (epoch 47), train_loss = 1.879, time/batch = 0.681\n",
            "19869/20850 (epoch 47), train_loss = 1.906, time/batch = 0.672\n",
            "19870/20850 (epoch 47), train_loss = 1.918, time/batch = 0.642\n",
            "19871/20850 (epoch 47), train_loss = 1.933, time/batch = 0.707\n",
            "19872/20850 (epoch 47), train_loss = 1.894, time/batch = 0.676\n",
            "19873/20850 (epoch 47), train_loss = 1.896, time/batch = 0.682\n",
            "19874/20850 (epoch 47), train_loss = 1.903, time/batch = 0.698\n",
            "19875/20850 (epoch 47), train_loss = 1.911, time/batch = 0.678\n",
            "19876/20850 (epoch 47), train_loss = 1.893, time/batch = 0.666\n",
            "19877/20850 (epoch 47), train_loss = 1.901, time/batch = 0.662\n",
            "19878/20850 (epoch 47), train_loss = 1.901, time/batch = 0.663\n",
            "19879/20850 (epoch 47), train_loss = 1.900, time/batch = 0.531\n",
            "19880/20850 (epoch 47), train_loss = 1.932, time/batch = 0.505\n",
            "19881/20850 (epoch 47), train_loss = 1.925, time/batch = 0.509\n",
            "19882/20850 (epoch 47), train_loss = 1.912, time/batch = 0.505\n",
            "19883/20850 (epoch 47), train_loss = 1.918, time/batch = 0.516\n",
            "19884/20850 (epoch 47), train_loss = 1.932, time/batch = 0.502\n",
            "19885/20850 (epoch 47), train_loss = 1.909, time/batch = 0.502\n",
            "19886/20850 (epoch 47), train_loss = 1.891, time/batch = 0.509\n",
            "19887/20850 (epoch 47), train_loss = 1.902, time/batch = 0.507\n",
            "19888/20850 (epoch 47), train_loss = 1.919, time/batch = 0.511\n",
            "19889/20850 (epoch 47), train_loss = 1.912, time/batch = 0.510\n",
            "19890/20850 (epoch 47), train_loss = 1.907, time/batch = 0.506\n",
            "19891/20850 (epoch 47), train_loss = 1.912, time/batch = 0.523\n",
            "19892/20850 (epoch 47), train_loss = 1.892, time/batch = 0.527\n",
            "19893/20850 (epoch 47), train_loss = 1.919, time/batch = 0.532\n",
            "19894/20850 (epoch 47), train_loss = 1.901, time/batch = 0.522\n",
            "19895/20850 (epoch 47), train_loss = 1.907, time/batch = 0.520\n",
            "19896/20850 (epoch 47), train_loss = 1.903, time/batch = 0.512\n",
            "19897/20850 (epoch 47), train_loss = 1.915, time/batch = 0.506\n",
            "19898/20850 (epoch 47), train_loss = 1.881, time/batch = 0.574\n",
            "19899/20850 (epoch 47), train_loss = 1.908, time/batch = 0.650\n",
            "19900/20850 (epoch 47), train_loss = 1.885, time/batch = 0.647\n",
            "19901/20850 (epoch 47), train_loss = 1.880, time/batch = 0.664\n",
            "19902/20850 (epoch 47), train_loss = 1.894, time/batch = 0.653\n",
            "19903/20850 (epoch 47), train_loss = 1.882, time/batch = 0.679\n",
            "19904/20850 (epoch 47), train_loss = 1.929, time/batch = 0.670\n",
            "19905/20850 (epoch 47), train_loss = 1.920, time/batch = 0.663\n",
            "19906/20850 (epoch 47), train_loss = 1.897, time/batch = 0.682\n",
            "19907/20850 (epoch 47), train_loss = 1.913, time/batch = 0.679\n",
            "19908/20850 (epoch 47), train_loss = 1.923, time/batch = 0.657\n",
            "19909/20850 (epoch 47), train_loss = 1.886, time/batch = 0.657\n",
            "19910/20850 (epoch 47), train_loss = 1.896, time/batch = 0.656\n",
            "19911/20850 (epoch 47), train_loss = 1.912, time/batch = 0.683\n",
            "19912/20850 (epoch 47), train_loss = 1.898, time/batch = 0.675\n",
            "19913/20850 (epoch 47), train_loss = 1.912, time/batch = 0.655\n",
            "19914/20850 (epoch 47), train_loss = 1.890, time/batch = 0.674\n",
            "19915/20850 (epoch 47), train_loss = 1.922, time/batch = 0.696\n",
            "19916/20850 (epoch 47), train_loss = 1.920, time/batch = 0.668\n",
            "19917/20850 (epoch 47), train_loss = 1.907, time/batch = 0.666\n",
            "19918/20850 (epoch 47), train_loss = 1.905, time/batch = 0.507\n",
            "19919/20850 (epoch 47), train_loss = 1.892, time/batch = 0.528\n",
            "19920/20850 (epoch 47), train_loss = 1.890, time/batch = 0.516\n",
            "19921/20850 (epoch 47), train_loss = 1.892, time/batch = 0.549\n",
            "19922/20850 (epoch 47), train_loss = 1.870, time/batch = 0.510\n",
            "19923/20850 (epoch 47), train_loss = 1.889, time/batch = 0.537\n",
            "19924/20850 (epoch 47), train_loss = 1.887, time/batch = 0.529\n",
            "19925/20850 (epoch 47), train_loss = 1.902, time/batch = 0.522\n",
            "19926/20850 (epoch 47), train_loss = 1.885, time/batch = 0.504\n",
            "19927/20850 (epoch 47), train_loss = 1.895, time/batch = 0.537\n",
            "19928/20850 (epoch 47), train_loss = 1.894, time/batch = 0.520\n",
            "19929/20850 (epoch 47), train_loss = 1.907, time/batch = 0.509\n",
            "19930/20850 (epoch 47), train_loss = 1.892, time/batch = 0.501\n",
            "19931/20850 (epoch 47), train_loss = 1.895, time/batch = 0.510\n",
            "19932/20850 (epoch 47), train_loss = 1.908, time/batch = 0.512\n",
            "19933/20850 (epoch 47), train_loss = 1.880, time/batch = 0.508\n",
            "19934/20850 (epoch 47), train_loss = 1.873, time/batch = 0.511\n",
            "19935/20850 (epoch 47), train_loss = 1.898, time/batch = 0.513\n",
            "19936/20850 (epoch 47), train_loss = 1.881, time/batch = 0.516\n",
            "19937/20850 (epoch 47), train_loss = 1.903, time/batch = 0.558\n",
            "19938/20850 (epoch 47), train_loss = 1.926, time/batch = 0.670\n",
            "19939/20850 (epoch 47), train_loss = 1.913, time/batch = 0.718\n",
            "19940/20850 (epoch 47), train_loss = 1.896, time/batch = 0.673\n",
            "19941/20850 (epoch 47), train_loss = 1.915, time/batch = 0.660\n",
            "19942/20850 (epoch 47), train_loss = 1.903, time/batch = 0.666\n",
            "19943/20850 (epoch 47), train_loss = 1.920, time/batch = 0.680\n",
            "19944/20850 (epoch 47), train_loss = 1.928, time/batch = 0.690\n",
            "19945/20850 (epoch 47), train_loss = 1.903, time/batch = 0.672\n",
            "19946/20850 (epoch 47), train_loss = 1.912, time/batch = 0.671\n",
            "19947/20850 (epoch 47), train_loss = 1.922, time/batch = 0.651\n",
            "19948/20850 (epoch 47), train_loss = 1.914, time/batch = 0.659\n",
            "19949/20850 (epoch 47), train_loss = 1.907, time/batch = 0.675\n",
            "19950/20850 (epoch 47), train_loss = 1.921, time/batch = 0.698\n",
            "19951/20850 (epoch 47), train_loss = 1.923, time/batch = 0.647\n",
            "19952/20850 (epoch 47), train_loss = 1.921, time/batch = 0.672\n",
            "19953/20850 (epoch 47), train_loss = 1.908, time/batch = 0.652\n",
            "19954/20850 (epoch 47), train_loss = 1.897, time/batch = 0.662\n",
            "19955/20850 (epoch 47), train_loss = 1.918, time/batch = 0.648\n",
            "19956/20850 (epoch 47), train_loss = 1.910, time/batch = 0.660\n",
            "19957/20850 (epoch 47), train_loss = 1.903, time/batch = 0.516\n",
            "19958/20850 (epoch 47), train_loss = 1.894, time/batch = 0.512\n",
            "19959/20850 (epoch 47), train_loss = 1.913, time/batch = 0.514\n",
            "19960/20850 (epoch 47), train_loss = 1.888, time/batch = 0.504\n",
            "19961/20850 (epoch 47), train_loss = 1.902, time/batch = 0.529\n",
            "19962/20850 (epoch 47), train_loss = 1.926, time/batch = 0.510\n",
            "19963/20850 (epoch 47), train_loss = 1.898, time/batch = 0.527\n",
            "19964/20850 (epoch 47), train_loss = 1.920, time/batch = 0.508\n",
            "19965/20850 (epoch 47), train_loss = 1.884, time/batch = 0.510\n",
            "19966/20850 (epoch 47), train_loss = 1.912, time/batch = 0.506\n",
            "19967/20850 (epoch 47), train_loss = 1.912, time/batch = 0.519\n",
            "19968/20850 (epoch 47), train_loss = 1.909, time/batch = 0.524\n",
            "19969/20850 (epoch 47), train_loss = 1.905, time/batch = 0.528\n",
            "19970/20850 (epoch 47), train_loss = 1.911, time/batch = 0.521\n",
            "19971/20850 (epoch 47), train_loss = 1.927, time/batch = 0.547\n",
            "19972/20850 (epoch 47), train_loss = 1.880, time/batch = 0.511\n",
            "19973/20850 (epoch 47), train_loss = 1.889, time/batch = 0.527\n",
            "19974/20850 (epoch 47), train_loss = 1.896, time/batch = 0.519\n",
            "19975/20850 (epoch 47), train_loss = 1.885, time/batch = 0.534\n",
            "19976/20850 (epoch 47), train_loss = 1.874, time/batch = 0.639\n",
            "19977/20850 (epoch 47), train_loss = 1.890, time/batch = 0.669\n",
            "19978/20850 (epoch 47), train_loss = 1.917, time/batch = 0.677\n",
            "19979/20850 (epoch 47), train_loss = 1.911, time/batch = 0.683\n",
            "19980/20850 (epoch 47), train_loss = 1.901, time/batch = 0.656\n",
            "19981/20850 (epoch 47), train_loss = 1.901, time/batch = 0.660\n",
            "19982/20850 (epoch 47), train_loss = 1.929, time/batch = 0.633\n",
            "19983/20850 (epoch 47), train_loss = 1.907, time/batch = 0.672\n",
            "19984/20850 (epoch 47), train_loss = 1.933, time/batch = 0.654\n",
            "19985/20850 (epoch 47), train_loss = 1.908, time/batch = 0.663\n",
            "19986/20850 (epoch 47), train_loss = 1.910, time/batch = 0.673\n",
            "19987/20850 (epoch 47), train_loss = 1.924, time/batch = 0.679\n",
            "19988/20850 (epoch 47), train_loss = 1.903, time/batch = 0.687\n",
            "19989/20850 (epoch 47), train_loss = 1.918, time/batch = 0.668\n",
            "19990/20850 (epoch 47), train_loss = 1.910, time/batch = 0.662\n",
            "19991/20850 (epoch 47), train_loss = 1.909, time/batch = 0.665\n",
            "19992/20850 (epoch 47), train_loss = 1.909, time/batch = 0.658\n",
            "19993/20850 (epoch 47), train_loss = 1.874, time/batch = 0.693\n",
            "19994/20850 (epoch 47), train_loss = 1.914, time/batch = 0.673\n",
            "19995/20850 (epoch 47), train_loss = 1.903, time/batch = 0.599\n",
            "19996/20850 (epoch 47), train_loss = 1.910, time/batch = 0.515\n",
            "19997/20850 (epoch 47), train_loss = 1.908, time/batch = 0.525\n",
            "19998/20850 (epoch 47), train_loss = 1.904, time/batch = 0.519\n",
            "19999/20850 (epoch 47), train_loss = 1.893, time/batch = 0.521\n",
            "20000/20850 (epoch 47), train_loss = 1.920, time/batch = 0.525\n",
            "model saved to /content/drive/Shareddrives/Materials UNI/UNIPI/ISPR/Midterm3 Assignment_4/save/save0/model.ckpt\n",
            "20001/20850 (epoch 47), train_loss = 1.919, time/batch = 0.587\n",
            "20002/20850 (epoch 47), train_loss = 1.921, time/batch = 0.520\n",
            "20003/20850 (epoch 47), train_loss = 1.896, time/batch = 0.505\n",
            "20004/20850 (epoch 47), train_loss = 1.888, time/batch = 0.515\n",
            "20005/20850 (epoch 47), train_loss = 1.889, time/batch = 0.526\n",
            "20006/20850 (epoch 47), train_loss = 1.909, time/batch = 0.516\n",
            "20007/20850 (epoch 47), train_loss = 1.907, time/batch = 0.519\n",
            "20008/20850 (epoch 47), train_loss = 1.893, time/batch = 0.532\n",
            "20009/20850 (epoch 47), train_loss = 1.921, time/batch = 0.525\n",
            "20010/20850 (epoch 47), train_loss = 1.910, time/batch = 0.520\n",
            "20011/20850 (epoch 47), train_loss = 1.914, time/batch = 0.553\n",
            "20012/20850 (epoch 47), train_loss = 1.942, time/batch = 0.646\n",
            "20013/20850 (epoch 47), train_loss = 1.924, time/batch = 0.643\n",
            "20014/20850 (epoch 47), train_loss = 1.921, time/batch = 0.656\n",
            "20015/20850 (epoch 47), train_loss = 1.912, time/batch = 0.710\n",
            "20016/20850 (epoch 48), train_loss = 1.921, time/batch = 0.754\n",
            "20017/20850 (epoch 48), train_loss = 1.895, time/batch = 0.710\n",
            "20018/20850 (epoch 48), train_loss = 1.922, time/batch = 0.688\n",
            "20019/20850 (epoch 48), train_loss = 1.910, time/batch = 0.687\n",
            "20020/20850 (epoch 48), train_loss = 1.882, time/batch = 0.651\n",
            "20021/20850 (epoch 48), train_loss = 1.903, time/batch = 0.701\n",
            "20022/20850 (epoch 48), train_loss = 1.915, time/batch = 0.678\n",
            "20023/20850 (epoch 48), train_loss = 1.907, time/batch = 0.716\n",
            "20024/20850 (epoch 48), train_loss = 1.889, time/batch = 0.673\n",
            "20025/20850 (epoch 48), train_loss = 1.905, time/batch = 0.677\n",
            "20026/20850 (epoch 48), train_loss = 1.883, time/batch = 0.718\n",
            "20027/20850 (epoch 48), train_loss = 1.882, time/batch = 0.659\n",
            "20028/20850 (epoch 48), train_loss = 1.906, time/batch = 0.517\n",
            "20029/20850 (epoch 48), train_loss = 1.936, time/batch = 0.514\n",
            "20030/20850 (epoch 48), train_loss = 1.912, time/batch = 0.511\n",
            "20031/20850 (epoch 48), train_loss = 1.942, time/batch = 0.512\n",
            "20032/20850 (epoch 48), train_loss = 1.914, time/batch = 0.514\n",
            "20033/20850 (epoch 48), train_loss = 1.900, time/batch = 0.527\n",
            "20034/20850 (epoch 48), train_loss = 1.879, time/batch = 0.511\n",
            "20035/20850 (epoch 48), train_loss = 1.894, time/batch = 0.510\n",
            "20036/20850 (epoch 48), train_loss = 1.906, time/batch = 0.522\n",
            "20037/20850 (epoch 48), train_loss = 1.898, time/batch = 0.534\n",
            "20038/20850 (epoch 48), train_loss = 1.920, time/batch = 0.510\n",
            "20039/20850 (epoch 48), train_loss = 1.916, time/batch = 0.523\n",
            "20040/20850 (epoch 48), train_loss = 1.913, time/batch = 0.506\n",
            "20041/20850 (epoch 48), train_loss = 1.898, time/batch = 0.506\n",
            "20042/20850 (epoch 48), train_loss = 1.890, time/batch = 0.505\n",
            "20043/20850 (epoch 48), train_loss = 1.910, time/batch = 0.509\n",
            "20044/20850 (epoch 48), train_loss = 1.903, time/batch = 0.502\n",
            "20045/20850 (epoch 48), train_loss = 1.887, time/batch = 0.525\n",
            "20046/20850 (epoch 48), train_loss = 1.908, time/batch = 0.514\n",
            "20047/20850 (epoch 48), train_loss = 1.907, time/batch = 0.666\n",
            "20048/20850 (epoch 48), train_loss = 1.899, time/batch = 0.683\n",
            "20049/20850 (epoch 48), train_loss = 1.891, time/batch = 0.675\n",
            "20050/20850 (epoch 48), train_loss = 1.884, time/batch = 0.669\n",
            "20051/20850 (epoch 48), train_loss = 1.897, time/batch = 0.634\n",
            "20052/20850 (epoch 48), train_loss = 1.900, time/batch = 0.687\n",
            "20053/20850 (epoch 48), train_loss = 1.889, time/batch = 0.681\n",
            "20054/20850 (epoch 48), train_loss = 1.896, time/batch = 0.659\n",
            "20055/20850 (epoch 48), train_loss = 1.912, time/batch = 0.659\n",
            "20056/20850 (epoch 48), train_loss = 1.906, time/batch = 0.655\n",
            "20057/20850 (epoch 48), train_loss = 1.893, time/batch = 0.672\n",
            "20058/20850 (epoch 48), train_loss = 1.884, time/batch = 0.650\n",
            "20059/20850 (epoch 48), train_loss = 1.928, time/batch = 0.657\n",
            "20060/20850 (epoch 48), train_loss = 1.912, time/batch = 0.657\n",
            "20061/20850 (epoch 48), train_loss = 1.904, time/batch = 0.665\n",
            "20062/20850 (epoch 48), train_loss = 1.908, time/batch = 0.632\n",
            "20063/20850 (epoch 48), train_loss = 1.901, time/batch = 0.668\n",
            "20064/20850 (epoch 48), train_loss = 1.914, time/batch = 0.681\n",
            "20065/20850 (epoch 48), train_loss = 1.890, time/batch = 0.666\n",
            "20066/20850 (epoch 48), train_loss = 1.895, time/batch = 0.656\n",
            "20067/20850 (epoch 48), train_loss = 1.900, time/batch = 0.512\n",
            "20068/20850 (epoch 48), train_loss = 1.889, time/batch = 0.521\n",
            "20069/20850 (epoch 48), train_loss = 1.921, time/batch = 0.502\n",
            "20070/20850 (epoch 48), train_loss = 1.907, time/batch = 0.542\n",
            "20071/20850 (epoch 48), train_loss = 1.902, time/batch = 0.521\n",
            "20072/20850 (epoch 48), train_loss = 1.907, time/batch = 0.519\n",
            "20073/20850 (epoch 48), train_loss = 1.901, time/batch = 0.512\n",
            "20074/20850 (epoch 48), train_loss = 1.914, time/batch = 0.528\n",
            "20075/20850 (epoch 48), train_loss = 1.907, time/batch = 0.508\n",
            "20076/20850 (epoch 48), train_loss = 1.909, time/batch = 0.516\n",
            "20077/20850 (epoch 48), train_loss = 1.910, time/batch = 0.501\n",
            "20078/20850 (epoch 48), train_loss = 1.909, time/batch = 0.525\n",
            "20079/20850 (epoch 48), train_loss = 1.918, time/batch = 0.507\n",
            "20080/20850 (epoch 48), train_loss = 1.897, time/batch = 0.529\n",
            "20081/20850 (epoch 48), train_loss = 1.894, time/batch = 0.515\n",
            "20082/20850 (epoch 48), train_loss = 1.908, time/batch = 0.510\n",
            "20083/20850 (epoch 48), train_loss = 1.887, time/batch = 0.513\n",
            "20084/20850 (epoch 48), train_loss = 1.893, time/batch = 0.520\n",
            "20085/20850 (epoch 48), train_loss = 1.879, time/batch = 0.510\n",
            "20086/20850 (epoch 48), train_loss = 1.877, time/batch = 0.623\n",
            "20087/20850 (epoch 48), train_loss = 1.894, time/batch = 0.640\n",
            "20088/20850 (epoch 48), train_loss = 1.898, time/batch = 0.662\n",
            "20089/20850 (epoch 48), train_loss = 1.899, time/batch = 0.676\n",
            "20090/20850 (epoch 48), train_loss = 1.907, time/batch = 0.667\n",
            "20091/20850 (epoch 48), train_loss = 1.909, time/batch = 0.672\n",
            "20092/20850 (epoch 48), train_loss = 1.869, time/batch = 0.662\n",
            "20093/20850 (epoch 48), train_loss = 1.903, time/batch = 0.654\n",
            "20094/20850 (epoch 48), train_loss = 1.882, time/batch = 0.660\n",
            "20095/20850 (epoch 48), train_loss = 1.887, time/batch = 0.690\n",
            "20096/20850 (epoch 48), train_loss = 1.902, time/batch = 0.679\n",
            "20097/20850 (epoch 48), train_loss = 1.926, time/batch = 0.632\n",
            "20098/20850 (epoch 48), train_loss = 1.903, time/batch = 0.688\n",
            "20099/20850 (epoch 48), train_loss = 1.893, time/batch = 0.668\n",
            "20100/20850 (epoch 48), train_loss = 1.888, time/batch = 0.677\n",
            "20101/20850 (epoch 48), train_loss = 1.901, time/batch = 0.667\n",
            "20102/20850 (epoch 48), train_loss = 1.872, time/batch = 0.662\n",
            "20103/20850 (epoch 48), train_loss = 1.921, time/batch = 0.670\n",
            "20104/20850 (epoch 48), train_loss = 1.899, time/batch = 0.649\n",
            "20105/20850 (epoch 48), train_loss = 1.885, time/batch = 0.592\n",
            "20106/20850 (epoch 48), train_loss = 1.891, time/batch = 0.517\n",
            "20107/20850 (epoch 48), train_loss = 1.883, time/batch = 0.522\n",
            "20108/20850 (epoch 48), train_loss = 1.909, time/batch = 0.526\n",
            "20109/20850 (epoch 48), train_loss = 1.880, time/batch = 0.514\n",
            "20110/20850 (epoch 48), train_loss = 1.877, time/batch = 0.512\n",
            "20111/20850 (epoch 48), train_loss = 1.898, time/batch = 0.518\n",
            "20112/20850 (epoch 48), train_loss = 1.925, time/batch = 0.533\n",
            "20113/20850 (epoch 48), train_loss = 1.881, time/batch = 0.508\n",
            "20114/20850 (epoch 48), train_loss = 1.889, time/batch = 0.506\n",
            "20115/20850 (epoch 48), train_loss = 1.883, time/batch = 0.510\n",
            "20116/20850 (epoch 48), train_loss = 1.915, time/batch = 0.516\n",
            "20117/20850 (epoch 48), train_loss = 1.887, time/batch = 0.529\n",
            "20118/20850 (epoch 48), train_loss = 1.908, time/batch = 0.509\n",
            "20119/20850 (epoch 48), train_loss = 1.916, time/batch = 0.520\n",
            "20120/20850 (epoch 48), train_loss = 1.898, time/batch = 0.516\n",
            "20121/20850 (epoch 48), train_loss = 1.897, time/batch = 0.506\n",
            "20122/20850 (epoch 48), train_loss = 1.881, time/batch = 0.506\n",
            "20123/20850 (epoch 48), train_loss = 1.908, time/batch = 0.513\n",
            "20124/20850 (epoch 48), train_loss = 1.912, time/batch = 0.526\n",
            "20125/20850 (epoch 48), train_loss = 1.882, time/batch = 0.632\n",
            "20126/20850 (epoch 48), train_loss = 1.886, time/batch = 0.681\n",
            "20127/20850 (epoch 48), train_loss = 1.872, time/batch = 0.663\n",
            "20128/20850 (epoch 48), train_loss = 1.880, time/batch = 0.644\n",
            "20129/20850 (epoch 48), train_loss = 1.895, time/batch = 0.639\n",
            "20130/20850 (epoch 48), train_loss = 1.916, time/batch = 0.621\n",
            "20131/20850 (epoch 48), train_loss = 1.918, time/batch = 0.623\n",
            "20132/20850 (epoch 48), train_loss = 1.876, time/batch = 0.653\n",
            "20133/20850 (epoch 48), train_loss = 1.875, time/batch = 0.675\n",
            "20134/20850 (epoch 48), train_loss = 1.891, time/batch = 0.650\n",
            "20135/20850 (epoch 48), train_loss = 1.914, time/batch = 0.670\n",
            "20136/20850 (epoch 48), train_loss = 1.893, time/batch = 0.650\n",
            "20137/20850 (epoch 48), train_loss = 1.903, time/batch = 0.657\n",
            "20138/20850 (epoch 48), train_loss = 1.913, time/batch = 0.652\n",
            "20139/20850 (epoch 48), train_loss = 1.916, time/batch = 0.676\n",
            "20140/20850 (epoch 48), train_loss = 1.895, time/batch = 0.681\n",
            "20141/20850 (epoch 48), train_loss = 1.889, time/batch = 0.648\n",
            "20142/20850 (epoch 48), train_loss = 1.908, time/batch = 0.660\n",
            "20143/20850 (epoch 48), train_loss = 1.892, time/batch = 0.652\n",
            "20144/20850 (epoch 48), train_loss = 1.924, time/batch = 0.601\n",
            "20145/20850 (epoch 48), train_loss = 1.909, time/batch = 0.510\n",
            "20146/20850 (epoch 48), train_loss = 1.917, time/batch = 0.511\n",
            "20147/20850 (epoch 48), train_loss = 1.924, time/batch = 0.536\n",
            "20148/20850 (epoch 48), train_loss = 1.914, time/batch = 0.496\n",
            "20149/20850 (epoch 48), train_loss = 1.898, time/batch = 0.525\n",
            "20150/20850 (epoch 48), train_loss = 1.875, time/batch = 0.503\n",
            "20151/20850 (epoch 48), train_loss = 1.887, time/batch = 0.509\n",
            "20152/20850 (epoch 48), train_loss = 1.896, time/batch = 0.518\n",
            "20153/20850 (epoch 48), train_loss = 1.922, time/batch = 0.505\n",
            "20154/20850 (epoch 48), train_loss = 1.927, time/batch = 0.501\n",
            "20155/20850 (epoch 48), train_loss = 1.921, time/batch = 0.508\n",
            "20156/20850 (epoch 48), train_loss = 1.926, time/batch = 0.492\n",
            "20157/20850 (epoch 48), train_loss = 1.909, time/batch = 0.516\n",
            "20158/20850 (epoch 48), train_loss = 1.909, time/batch = 0.526\n",
            "20159/20850 (epoch 48), train_loss = 1.902, time/batch = 0.509\n",
            "20160/20850 (epoch 48), train_loss = 1.900, time/batch = 0.505\n",
            "20161/20850 (epoch 48), train_loss = 1.912, time/batch = 0.509\n",
            "20162/20850 (epoch 48), train_loss = 1.901, time/batch = 0.515\n",
            "20163/20850 (epoch 48), train_loss = 1.892, time/batch = 0.518\n",
            "20164/20850 (epoch 48), train_loss = 1.895, time/batch = 0.652\n",
            "20165/20850 (epoch 48), train_loss = 1.902, time/batch = 0.682\n",
            "20166/20850 (epoch 48), train_loss = 1.884, time/batch = 0.669\n",
            "20167/20850 (epoch 48), train_loss = 1.931, time/batch = 0.657\n",
            "20168/20850 (epoch 48), train_loss = 1.900, time/batch = 0.664\n",
            "20169/20850 (epoch 48), train_loss = 1.904, time/batch = 0.673\n",
            "20170/20850 (epoch 48), train_loss = 1.921, time/batch = 0.692\n",
            "20171/20850 (epoch 48), train_loss = 1.897, time/batch = 0.672\n",
            "20172/20850 (epoch 48), train_loss = 1.883, time/batch = 0.665\n",
            "20173/20850 (epoch 48), train_loss = 1.901, time/batch = 0.683\n",
            "20174/20850 (epoch 48), train_loss = 1.904, time/batch = 0.652\n",
            "20175/20850 (epoch 48), train_loss = 1.888, time/batch = 0.646\n",
            "20176/20850 (epoch 48), train_loss = 1.886, time/batch = 0.677\n",
            "20177/20850 (epoch 48), train_loss = 1.898, time/batch = 0.651\n",
            "20178/20850 (epoch 48), train_loss = 1.894, time/batch = 0.643\n",
            "20179/20850 (epoch 48), train_loss = 1.898, time/batch = 0.655\n",
            "20180/20850 (epoch 48), train_loss = 1.896, time/batch = 0.662\n",
            "20181/20850 (epoch 48), train_loss = 1.888, time/batch = 0.649\n",
            "20182/20850 (epoch 48), train_loss = 1.898, time/batch = 0.636\n",
            "20183/20850 (epoch 48), train_loss = 1.899, time/batch = 0.584\n",
            "20184/20850 (epoch 48), train_loss = 1.898, time/batch = 0.523\n",
            "20185/20850 (epoch 48), train_loss = 1.889, time/batch = 0.515\n",
            "20186/20850 (epoch 48), train_loss = 1.889, time/batch = 0.507\n",
            "20187/20850 (epoch 48), train_loss = 1.892, time/batch = 0.505\n",
            "20188/20850 (epoch 48), train_loss = 1.892, time/batch = 0.507\n",
            "20189/20850 (epoch 48), train_loss = 1.879, time/batch = 0.501\n",
            "20190/20850 (epoch 48), train_loss = 1.920, time/batch = 0.525\n",
            "20191/20850 (epoch 48), train_loss = 1.900, time/batch = 0.517\n",
            "20192/20850 (epoch 48), train_loss = 1.908, time/batch = 0.502\n",
            "20193/20850 (epoch 48), train_loss = 1.894, time/batch = 0.540\n",
            "20194/20850 (epoch 48), train_loss = 1.915, time/batch = 0.501\n",
            "20195/20850 (epoch 48), train_loss = 1.906, time/batch = 0.499\n",
            "20196/20850 (epoch 48), train_loss = 1.887, time/batch = 0.506\n",
            "20197/20850 (epoch 48), train_loss = 1.907, time/batch = 0.499\n",
            "20198/20850 (epoch 48), train_loss = 1.890, time/batch = 0.506\n",
            "20199/20850 (epoch 48), train_loss = 1.901, time/batch = 0.506\n",
            "20200/20850 (epoch 48), train_loss = 1.878, time/batch = 0.531\n",
            "20201/20850 (epoch 48), train_loss = 1.902, time/batch = 0.518\n",
            "20202/20850 (epoch 48), train_loss = 1.922, time/batch = 0.516\n",
            "20203/20850 (epoch 48), train_loss = 1.922, time/batch = 0.636\n",
            "20204/20850 (epoch 48), train_loss = 1.912, time/batch = 0.642\n",
            "20205/20850 (epoch 48), train_loss = 1.896, time/batch = 0.652\n",
            "20206/20850 (epoch 48), train_loss = 1.896, time/batch = 0.640\n",
            "20207/20850 (epoch 48), train_loss = 1.920, time/batch = 0.654\n",
            "20208/20850 (epoch 48), train_loss = 1.893, time/batch = 0.639\n",
            "20209/20850 (epoch 48), train_loss = 1.899, time/batch = 0.656\n",
            "20210/20850 (epoch 48), train_loss = 1.908, time/batch = 0.667\n",
            "20211/20850 (epoch 48), train_loss = 1.907, time/batch = 0.659\n",
            "20212/20850 (epoch 48), train_loss = 1.909, time/batch = 0.671\n",
            "20213/20850 (epoch 48), train_loss = 1.915, time/batch = 0.672\n",
            "20214/20850 (epoch 48), train_loss = 1.904, time/batch = 0.685\n",
            "20215/20850 (epoch 48), train_loss = 1.893, time/batch = 0.627\n",
            "20216/20850 (epoch 48), train_loss = 1.912, time/batch = 0.652\n",
            "20217/20850 (epoch 48), train_loss = 1.913, time/batch = 0.663\n",
            "20218/20850 (epoch 48), train_loss = 1.888, time/batch = 0.657\n",
            "20219/20850 (epoch 48), train_loss = 1.898, time/batch = 0.659\n",
            "20220/20850 (epoch 48), train_loss = 1.929, time/batch = 0.677\n",
            "20221/20850 (epoch 48), train_loss = 1.938, time/batch = 0.664\n",
            "20222/20850 (epoch 48), train_loss = 1.918, time/batch = 0.588\n",
            "20223/20850 (epoch 48), train_loss = 1.898, time/batch = 0.498\n",
            "20224/20850 (epoch 48), train_loss = 1.883, time/batch = 0.500\n",
            "20225/20850 (epoch 48), train_loss = 1.890, time/batch = 0.508\n",
            "20226/20850 (epoch 48), train_loss = 1.894, time/batch = 0.508\n",
            "20227/20850 (epoch 48), train_loss = 1.918, time/batch = 0.524\n",
            "20228/20850 (epoch 48), train_loss = 1.913, time/batch = 0.522\n",
            "20229/20850 (epoch 48), train_loss = 1.896, time/batch = 0.502\n",
            "20230/20850 (epoch 48), train_loss = 1.894, time/batch = 0.507\n",
            "20231/20850 (epoch 48), train_loss = 1.911, time/batch = 0.503\n",
            "20232/20850 (epoch 48), train_loss = 1.927, time/batch = 0.525\n",
            "20233/20850 (epoch 48), train_loss = 1.901, time/batch = 0.511\n",
            "20234/20850 (epoch 48), train_loss = 1.882, time/batch = 0.516\n",
            "20235/20850 (epoch 48), train_loss = 1.894, time/batch = 0.502\n",
            "20236/20850 (epoch 48), train_loss = 1.902, time/batch = 0.504\n",
            "20237/20850 (epoch 48), train_loss = 1.860, time/batch = 0.506\n",
            "20238/20850 (epoch 48), train_loss = 1.903, time/batch = 0.521\n",
            "20239/20850 (epoch 48), train_loss = 1.884, time/batch = 0.513\n",
            "20240/20850 (epoch 48), train_loss = 1.899, time/batch = 0.520\n",
            "20241/20850 (epoch 48), train_loss = 1.906, time/batch = 0.510\n",
            "20242/20850 (epoch 48), train_loss = 1.917, time/batch = 0.662\n",
            "20243/20850 (epoch 48), train_loss = 1.928, time/batch = 0.652\n",
            "20244/20850 (epoch 48), train_loss = 1.906, time/batch = 0.665\n",
            "20245/20850 (epoch 48), train_loss = 1.882, time/batch = 0.655\n",
            "20246/20850 (epoch 48), train_loss = 1.899, time/batch = 0.610\n",
            "20247/20850 (epoch 48), train_loss = 1.888, time/batch = 0.671\n",
            "20248/20850 (epoch 48), train_loss = 1.886, time/batch = 0.660\n",
            "20249/20850 (epoch 48), train_loss = 1.885, time/batch = 0.660\n",
            "20250/20850 (epoch 48), train_loss = 1.873, time/batch = 0.661\n",
            "20251/20850 (epoch 48), train_loss = 1.904, time/batch = 0.662\n",
            "20252/20850 (epoch 48), train_loss = 1.911, time/batch = 0.672\n",
            "20253/20850 (epoch 48), train_loss = 1.906, time/batch = 0.659\n",
            "20254/20850 (epoch 48), train_loss = 1.910, time/batch = 0.647\n",
            "20255/20850 (epoch 48), train_loss = 1.905, time/batch = 0.632\n",
            "20256/20850 (epoch 48), train_loss = 1.913, time/batch = 0.681\n",
            "20257/20850 (epoch 48), train_loss = 1.896, time/batch = 0.651\n",
            "20258/20850 (epoch 48), train_loss = 1.905, time/batch = 0.658\n",
            "20259/20850 (epoch 48), train_loss = 1.900, time/batch = 0.685\n",
            "20260/20850 (epoch 48), train_loss = 1.902, time/batch = 0.654\n",
            "20261/20850 (epoch 48), train_loss = 1.893, time/batch = 0.540\n",
            "20262/20850 (epoch 48), train_loss = 1.896, time/batch = 0.512\n",
            "20263/20850 (epoch 48), train_loss = 1.907, time/batch = 0.526\n",
            "20264/20850 (epoch 48), train_loss = 1.919, time/batch = 0.519\n",
            "20265/20850 (epoch 48), train_loss = 1.901, time/batch = 0.530\n",
            "20266/20850 (epoch 48), train_loss = 1.930, time/batch = 0.503\n",
            "20267/20850 (epoch 48), train_loss = 1.918, time/batch = 0.517\n",
            "20268/20850 (epoch 48), train_loss = 1.884, time/batch = 0.512\n",
            "20269/20850 (epoch 48), train_loss = 1.888, time/batch = 0.517\n",
            "20270/20850 (epoch 48), train_loss = 1.894, time/batch = 0.508\n",
            "20271/20850 (epoch 48), train_loss = 1.900, time/batch = 0.511\n",
            "20272/20850 (epoch 48), train_loss = 1.902, time/batch = 0.526\n",
            "20273/20850 (epoch 48), train_loss = 1.875, time/batch = 0.515\n",
            "20274/20850 (epoch 48), train_loss = 1.903, time/batch = 0.538\n",
            "20275/20850 (epoch 48), train_loss = 1.915, time/batch = 0.520\n",
            "20276/20850 (epoch 48), train_loss = 1.902, time/batch = 0.505\n",
            "20277/20850 (epoch 48), train_loss = 1.891, time/batch = 0.538\n",
            "20278/20850 (epoch 48), train_loss = 1.911, time/batch = 0.519\n",
            "20279/20850 (epoch 48), train_loss = 1.920, time/batch = 0.515\n",
            "20280/20850 (epoch 48), train_loss = 1.900, time/batch = 0.551\n",
            "20281/20850 (epoch 48), train_loss = 1.886, time/batch = 0.663\n",
            "20282/20850 (epoch 48), train_loss = 1.888, time/batch = 0.665\n",
            "20283/20850 (epoch 48), train_loss = 1.913, time/batch = 0.657\n",
            "20284/20850 (epoch 48), train_loss = 1.902, time/batch = 0.661\n",
            "20285/20850 (epoch 48), train_loss = 1.868, time/batch = 0.651\n",
            "20286/20850 (epoch 48), train_loss = 1.901, time/batch = 0.663\n",
            "20287/20850 (epoch 48), train_loss = 1.909, time/batch = 0.687\n",
            "20288/20850 (epoch 48), train_loss = 1.923, time/batch = 0.666\n",
            "20289/20850 (epoch 48), train_loss = 1.896, time/batch = 0.672\n",
            "20290/20850 (epoch 48), train_loss = 1.883, time/batch = 0.653\n",
            "20291/20850 (epoch 48), train_loss = 1.896, time/batch = 0.672\n",
            "20292/20850 (epoch 48), train_loss = 1.903, time/batch = 0.652\n",
            "20293/20850 (epoch 48), train_loss = 1.885, time/batch = 0.674\n",
            "20294/20850 (epoch 48), train_loss = 1.890, time/batch = 0.659\n",
            "20295/20850 (epoch 48), train_loss = 1.895, time/batch = 0.672\n",
            "20296/20850 (epoch 48), train_loss = 1.892, time/batch = 0.657\n",
            "20297/20850 (epoch 48), train_loss = 1.926, time/batch = 0.655\n",
            "20298/20850 (epoch 48), train_loss = 1.916, time/batch = 0.689\n",
            "20299/20850 (epoch 48), train_loss = 1.901, time/batch = 0.672\n",
            "20300/20850 (epoch 48), train_loss = 1.908, time/batch = 0.518\n",
            "20301/20850 (epoch 48), train_loss = 1.925, time/batch = 0.522\n",
            "20302/20850 (epoch 48), train_loss = 1.900, time/batch = 0.536\n",
            "20303/20850 (epoch 48), train_loss = 1.881, time/batch = 0.518\n",
            "20304/20850 (epoch 48), train_loss = 1.891, time/batch = 0.508\n",
            "20305/20850 (epoch 48), train_loss = 1.905, time/batch = 0.526\n",
            "20306/20850 (epoch 48), train_loss = 1.902, time/batch = 0.508\n",
            "20307/20850 (epoch 48), train_loss = 1.897, time/batch = 0.524\n",
            "20308/20850 (epoch 48), train_loss = 1.900, time/batch = 0.515\n",
            "20309/20850 (epoch 48), train_loss = 1.886, time/batch = 0.536\n",
            "20310/20850 (epoch 48), train_loss = 1.907, time/batch = 0.531\n",
            "20311/20850 (epoch 48), train_loss = 1.885, time/batch = 0.512\n",
            "20312/20850 (epoch 48), train_loss = 1.898, time/batch = 0.527\n",
            "20313/20850 (epoch 48), train_loss = 1.894, time/batch = 0.521\n",
            "20314/20850 (epoch 48), train_loss = 1.908, time/batch = 0.525\n",
            "20315/20850 (epoch 48), train_loss = 1.872, time/batch = 0.511\n",
            "20316/20850 (epoch 48), train_loss = 1.900, time/batch = 0.518\n",
            "20317/20850 (epoch 48), train_loss = 1.870, time/batch = 0.518\n",
            "20318/20850 (epoch 48), train_loss = 1.872, time/batch = 0.509\n",
            "20319/20850 (epoch 48), train_loss = 1.885, time/batch = 0.614\n",
            "20320/20850 (epoch 48), train_loss = 1.879, time/batch = 0.660\n",
            "20321/20850 (epoch 48), train_loss = 1.923, time/batch = 0.674\n",
            "20322/20850 (epoch 48), train_loss = 1.909, time/batch = 0.685\n",
            "20323/20850 (epoch 48), train_loss = 1.885, time/batch = 0.671\n",
            "20324/20850 (epoch 48), train_loss = 1.903, time/batch = 0.669\n",
            "20325/20850 (epoch 48), train_loss = 1.912, time/batch = 0.688\n",
            "20326/20850 (epoch 48), train_loss = 1.885, time/batch = 0.672\n",
            "20327/20850 (epoch 48), train_loss = 1.889, time/batch = 0.655\n",
            "20328/20850 (epoch 48), train_loss = 1.903, time/batch = 0.658\n",
            "20329/20850 (epoch 48), train_loss = 1.889, time/batch = 0.699\n",
            "20330/20850 (epoch 48), train_loss = 1.902, time/batch = 0.681\n",
            "20331/20850 (epoch 48), train_loss = 1.883, time/batch = 0.675\n",
            "20332/20850 (epoch 48), train_loss = 1.911, time/batch = 0.672\n",
            "20333/20850 (epoch 48), train_loss = 1.916, time/batch = 0.676\n",
            "20334/20850 (epoch 48), train_loss = 1.899, time/batch = 0.702\n",
            "20335/20850 (epoch 48), train_loss = 1.892, time/batch = 0.679\n",
            "20336/20850 (epoch 48), train_loss = 1.886, time/batch = 0.668\n",
            "20337/20850 (epoch 48), train_loss = 1.885, time/batch = 0.681\n",
            "20338/20850 (epoch 48), train_loss = 1.881, time/batch = 0.580\n",
            "20339/20850 (epoch 48), train_loss = 1.860, time/batch = 0.511\n",
            "20340/20850 (epoch 48), train_loss = 1.884, time/batch = 0.513\n",
            "20341/20850 (epoch 48), train_loss = 1.878, time/batch = 0.509\n",
            "20342/20850 (epoch 48), train_loss = 1.896, time/batch = 0.498\n",
            "20343/20850 (epoch 48), train_loss = 1.880, time/batch = 0.512\n",
            "20344/20850 (epoch 48), train_loss = 1.886, time/batch = 0.517\n",
            "20345/20850 (epoch 48), train_loss = 1.890, time/batch = 0.517\n",
            "20346/20850 (epoch 48), train_loss = 1.900, time/batch = 0.504\n",
            "20347/20850 (epoch 48), train_loss = 1.887, time/batch = 0.510\n",
            "20348/20850 (epoch 48), train_loss = 1.882, time/batch = 0.505\n",
            "20349/20850 (epoch 48), train_loss = 1.902, time/batch = 0.528\n",
            "20350/20850 (epoch 48), train_loss = 1.874, time/batch = 0.523\n",
            "20351/20850 (epoch 48), train_loss = 1.863, time/batch = 0.504\n",
            "20352/20850 (epoch 48), train_loss = 1.889, time/batch = 0.507\n",
            "20353/20850 (epoch 48), train_loss = 1.876, time/batch = 0.515\n",
            "20354/20850 (epoch 48), train_loss = 1.895, time/batch = 0.500\n",
            "20355/20850 (epoch 48), train_loss = 1.922, time/batch = 0.538\n",
            "20356/20850 (epoch 48), train_loss = 1.905, time/batch = 0.517\n",
            "20357/20850 (epoch 48), train_loss = 1.879, time/batch = 0.531\n",
            "20358/20850 (epoch 48), train_loss = 1.905, time/batch = 0.676\n",
            "20359/20850 (epoch 48), train_loss = 1.902, time/batch = 0.661\n",
            "20360/20850 (epoch 48), train_loss = 1.917, time/batch = 0.639\n",
            "20361/20850 (epoch 48), train_loss = 1.926, time/batch = 0.653\n",
            "20362/20850 (epoch 48), train_loss = 1.893, time/batch = 0.665\n",
            "20363/20850 (epoch 48), train_loss = 1.906, time/batch = 0.660\n",
            "20364/20850 (epoch 48), train_loss = 1.909, time/batch = 0.664\n",
            "20365/20850 (epoch 48), train_loss = 1.905, time/batch = 0.671\n",
            "20366/20850 (epoch 48), train_loss = 1.900, time/batch = 0.643\n",
            "20367/20850 (epoch 48), train_loss = 1.914, time/batch = 0.654\n",
            "20368/20850 (epoch 48), train_loss = 1.914, time/batch = 0.660\n",
            "20369/20850 (epoch 48), train_loss = 1.909, time/batch = 0.669\n",
            "20370/20850 (epoch 48), train_loss = 1.895, time/batch = 0.671\n",
            "20371/20850 (epoch 48), train_loss = 1.893, time/batch = 0.666\n",
            "20372/20850 (epoch 48), train_loss = 1.911, time/batch = 0.651\n",
            "20373/20850 (epoch 48), train_loss = 1.902, time/batch = 0.678\n",
            "20374/20850 (epoch 48), train_loss = 1.898, time/batch = 0.663\n",
            "20375/20850 (epoch 48), train_loss = 1.889, time/batch = 0.667\n",
            "20376/20850 (epoch 48), train_loss = 1.904, time/batch = 0.730\n",
            "20377/20850 (epoch 48), train_loss = 1.878, time/batch = 0.532\n",
            "20378/20850 (epoch 48), train_loss = 1.891, time/batch = 0.521\n",
            "20379/20850 (epoch 48), train_loss = 1.920, time/batch = 0.521\n",
            "20380/20850 (epoch 48), train_loss = 1.891, time/batch = 0.510\n",
            "20381/20850 (epoch 48), train_loss = 1.911, time/batch = 0.519\n",
            "20382/20850 (epoch 48), train_loss = 1.874, time/batch = 0.514\n",
            "20383/20850 (epoch 48), train_loss = 1.902, time/batch = 0.516\n",
            "20384/20850 (epoch 48), train_loss = 1.905, time/batch = 0.519\n",
            "20385/20850 (epoch 48), train_loss = 1.900, time/batch = 0.507\n",
            "20386/20850 (epoch 48), train_loss = 1.895, time/batch = 0.516\n",
            "20387/20850 (epoch 48), train_loss = 1.900, time/batch = 0.506\n",
            "20388/20850 (epoch 48), train_loss = 1.921, time/batch = 0.516\n",
            "20389/20850 (epoch 48), train_loss = 1.872, time/batch = 0.507\n",
            "20390/20850 (epoch 48), train_loss = 1.879, time/batch = 0.511\n",
            "20391/20850 (epoch 48), train_loss = 1.888, time/batch = 0.514\n",
            "20392/20850 (epoch 48), train_loss = 1.876, time/batch = 0.515\n",
            "20393/20850 (epoch 48), train_loss = 1.874, time/batch = 0.504\n",
            "20394/20850 (epoch 48), train_loss = 1.882, time/batch = 0.513\n",
            "20395/20850 (epoch 48), train_loss = 1.906, time/batch = 0.510\n",
            "20396/20850 (epoch 48), train_loss = 1.901, time/batch = 0.543\n",
            "20397/20850 (epoch 48), train_loss = 1.890, time/batch = 0.649\n",
            "20398/20850 (epoch 48), train_loss = 1.884, time/batch = 0.643\n",
            "20399/20850 (epoch 48), train_loss = 1.921, time/batch = 0.654\n",
            "20400/20850 (epoch 48), train_loss = 1.901, time/batch = 0.652\n",
            "20401/20850 (epoch 48), train_loss = 1.924, time/batch = 0.667\n",
            "20402/20850 (epoch 48), train_loss = 1.895, time/batch = 0.654\n",
            "20403/20850 (epoch 48), train_loss = 1.901, time/batch = 0.650\n",
            "20404/20850 (epoch 48), train_loss = 1.914, time/batch = 0.668\n",
            "20405/20850 (epoch 48), train_loss = 1.897, time/batch = 0.665\n",
            "20406/20850 (epoch 48), train_loss = 1.905, time/batch = 0.696\n",
            "20407/20850 (epoch 48), train_loss = 1.903, time/batch = 0.677\n",
            "20408/20850 (epoch 48), train_loss = 1.900, time/batch = 0.664\n",
            "20409/20850 (epoch 48), train_loss = 1.901, time/batch = 0.665\n",
            "20410/20850 (epoch 48), train_loss = 1.865, time/batch = 0.679\n",
            "20411/20850 (epoch 48), train_loss = 1.903, time/batch = 0.651\n",
            "20412/20850 (epoch 48), train_loss = 1.893, time/batch = 0.668\n",
            "20413/20850 (epoch 48), train_loss = 1.902, time/batch = 0.659\n",
            "20414/20850 (epoch 48), train_loss = 1.898, time/batch = 0.675\n",
            "20415/20850 (epoch 48), train_loss = 1.898, time/batch = 0.644\n",
            "20416/20850 (epoch 48), train_loss = 1.883, time/batch = 0.530\n",
            "20417/20850 (epoch 48), train_loss = 1.911, time/batch = 0.516\n",
            "20418/20850 (epoch 48), train_loss = 1.906, time/batch = 0.520\n",
            "20419/20850 (epoch 48), train_loss = 1.914, time/batch = 0.522\n",
            "20420/20850 (epoch 48), train_loss = 1.887, time/batch = 0.519\n",
            "20421/20850 (epoch 48), train_loss = 1.881, time/batch = 0.523\n",
            "20422/20850 (epoch 48), train_loss = 1.878, time/batch = 0.521\n",
            "20423/20850 (epoch 48), train_loss = 1.900, time/batch = 0.521\n",
            "20424/20850 (epoch 48), train_loss = 1.904, time/batch = 0.532\n",
            "20425/20850 (epoch 48), train_loss = 1.888, time/batch = 0.517\n",
            "20426/20850 (epoch 48), train_loss = 1.912, time/batch = 0.514\n",
            "20427/20850 (epoch 48), train_loss = 1.903, time/batch = 0.513\n",
            "20428/20850 (epoch 48), train_loss = 1.900, time/batch = 0.519\n",
            "20429/20850 (epoch 48), train_loss = 1.932, time/batch = 0.527\n",
            "20430/20850 (epoch 48), train_loss = 1.915, time/batch = 0.521\n",
            "20431/20850 (epoch 48), train_loss = 1.910, time/batch = 0.507\n",
            "20432/20850 (epoch 48), train_loss = 1.903, time/batch = 0.541\n",
            "20433/20850 (epoch 49), train_loss = 1.906, time/batch = 0.650\n",
            "20434/20850 (epoch 49), train_loss = 1.882, time/batch = 0.675\n",
            "20435/20850 (epoch 49), train_loss = 1.915, time/batch = 0.683\n",
            "20436/20850 (epoch 49), train_loss = 1.893, time/batch = 0.668\n",
            "20437/20850 (epoch 49), train_loss = 1.872, time/batch = 0.668\n",
            "20438/20850 (epoch 49), train_loss = 1.895, time/batch = 0.678\n",
            "20439/20850 (epoch 49), train_loss = 1.911, time/batch = 0.695\n",
            "20440/20850 (epoch 49), train_loss = 1.896, time/batch = 0.645\n",
            "20441/20850 (epoch 49), train_loss = 1.880, time/batch = 0.681\n",
            "20442/20850 (epoch 49), train_loss = 1.897, time/batch = 0.685\n",
            "20443/20850 (epoch 49), train_loss = 1.876, time/batch = 0.673\n",
            "20444/20850 (epoch 49), train_loss = 1.872, time/batch = 0.676\n",
            "20445/20850 (epoch 49), train_loss = 1.904, time/batch = 0.666\n",
            "20446/20850 (epoch 49), train_loss = 1.926, time/batch = 0.705\n",
            "20447/20850 (epoch 49), train_loss = 1.901, time/batch = 0.662\n",
            "20448/20850 (epoch 49), train_loss = 1.933, time/batch = 0.663\n",
            "20449/20850 (epoch 49), train_loss = 1.904, time/batch = 0.698\n",
            "20450/20850 (epoch 49), train_loss = 1.888, time/batch = 0.670\n",
            "20451/20850 (epoch 49), train_loss = 1.873, time/batch = 0.635\n",
            "20452/20850 (epoch 49), train_loss = 1.883, time/batch = 0.508\n",
            "20453/20850 (epoch 49), train_loss = 1.895, time/batch = 0.522\n",
            "20454/20850 (epoch 49), train_loss = 1.892, time/batch = 0.554\n",
            "20455/20850 (epoch 49), train_loss = 1.911, time/batch = 0.511\n",
            "20456/20850 (epoch 49), train_loss = 1.908, time/batch = 0.508\n",
            "20457/20850 (epoch 49), train_loss = 1.905, time/batch = 0.524\n",
            "20458/20850 (epoch 49), train_loss = 1.889, time/batch = 0.517\n",
            "20459/20850 (epoch 49), train_loss = 1.881, time/batch = 0.529\n",
            "20460/20850 (epoch 49), train_loss = 1.899, time/batch = 0.520\n",
            "20461/20850 (epoch 49), train_loss = 1.894, time/batch = 0.509\n",
            "20462/20850 (epoch 49), train_loss = 1.877, time/batch = 0.512\n",
            "20463/20850 (epoch 49), train_loss = 1.897, time/batch = 0.521\n",
            "20464/20850 (epoch 49), train_loss = 1.898, time/batch = 0.519\n",
            "20465/20850 (epoch 49), train_loss = 1.887, time/batch = 0.512\n",
            "20466/20850 (epoch 49), train_loss = 1.878, time/batch = 0.533\n",
            "20467/20850 (epoch 49), train_loss = 1.879, time/batch = 0.528\n",
            "20468/20850 (epoch 49), train_loss = 1.889, time/batch = 0.531\n",
            "20469/20850 (epoch 49), train_loss = 1.889, time/batch = 0.527\n",
            "20470/20850 (epoch 49), train_loss = 1.876, time/batch = 0.550\n",
            "20471/20850 (epoch 49), train_loss = 1.886, time/batch = 0.662\n",
            "20472/20850 (epoch 49), train_loss = 1.904, time/batch = 0.648\n",
            "20473/20850 (epoch 49), train_loss = 1.895, time/batch = 0.694\n",
            "20474/20850 (epoch 49), train_loss = 1.882, time/batch = 0.647\n",
            "20475/20850 (epoch 49), train_loss = 1.874, time/batch = 0.670\n",
            "20476/20850 (epoch 49), train_loss = 1.919, time/batch = 0.663\n",
            "20477/20850 (epoch 49), train_loss = 1.900, time/batch = 0.708\n",
            "20478/20850 (epoch 49), train_loss = 1.895, time/batch = 0.662\n",
            "20479/20850 (epoch 49), train_loss = 1.896, time/batch = 0.677\n",
            "20480/20850 (epoch 49), train_loss = 1.894, time/batch = 0.651\n",
            "20481/20850 (epoch 49), train_loss = 1.905, time/batch = 0.675\n",
            "20482/20850 (epoch 49), train_loss = 1.882, time/batch = 0.666\n",
            "20483/20850 (epoch 49), train_loss = 1.885, time/batch = 0.681\n",
            "20484/20850 (epoch 49), train_loss = 1.894, time/batch = 0.672\n",
            "20485/20850 (epoch 49), train_loss = 1.884, time/batch = 0.665\n",
            "20486/20850 (epoch 49), train_loss = 1.914, time/batch = 0.670\n",
            "20487/20850 (epoch 49), train_loss = 1.901, time/batch = 0.678\n",
            "20488/20850 (epoch 49), train_loss = 1.895, time/batch = 0.685\n",
            "20489/20850 (epoch 49), train_loss = 1.899, time/batch = 0.630\n",
            "20490/20850 (epoch 49), train_loss = 1.887, time/batch = 0.526\n",
            "20491/20850 (epoch 49), train_loss = 1.906, time/batch = 0.514\n",
            "20492/20850 (epoch 49), train_loss = 1.897, time/batch = 0.515\n",
            "20493/20850 (epoch 49), train_loss = 1.900, time/batch = 0.510\n",
            "20494/20850 (epoch 49), train_loss = 1.904, time/batch = 0.524\n",
            "20495/20850 (epoch 49), train_loss = 1.899, time/batch = 0.516\n",
            "20496/20850 (epoch 49), train_loss = 1.906, time/batch = 0.537\n",
            "20497/20850 (epoch 49), train_loss = 1.888, time/batch = 0.517\n",
            "20498/20850 (epoch 49), train_loss = 1.881, time/batch = 0.514\n",
            "20499/20850 (epoch 49), train_loss = 1.898, time/batch = 0.519\n",
            "20500/20850 (epoch 49), train_loss = 1.883, time/batch = 0.517\n",
            "20501/20850 (epoch 49), train_loss = 1.885, time/batch = 0.515\n",
            "20502/20850 (epoch 49), train_loss = 1.869, time/batch = 0.510\n",
            "20503/20850 (epoch 49), train_loss = 1.866, time/batch = 0.511\n",
            "20504/20850 (epoch 49), train_loss = 1.884, time/batch = 0.525\n",
            "20505/20850 (epoch 49), train_loss = 1.889, time/batch = 0.529\n",
            "20506/20850 (epoch 49), train_loss = 1.892, time/batch = 0.512\n",
            "20507/20850 (epoch 49), train_loss = 1.901, time/batch = 0.542\n",
            "20508/20850 (epoch 49), train_loss = 1.898, time/batch = 0.520\n",
            "20509/20850 (epoch 49), train_loss = 1.865, time/batch = 0.630\n",
            "20510/20850 (epoch 49), train_loss = 1.904, time/batch = 0.656\n",
            "20511/20850 (epoch 49), train_loss = 1.875, time/batch = 0.657\n",
            "20512/20850 (epoch 49), train_loss = 1.880, time/batch = 0.650\n",
            "20513/20850 (epoch 49), train_loss = 1.892, time/batch = 0.648\n",
            "20514/20850 (epoch 49), train_loss = 1.921, time/batch = 0.661\n",
            "20515/20850 (epoch 49), train_loss = 1.897, time/batch = 0.672\n",
            "20516/20850 (epoch 49), train_loss = 1.889, time/batch = 0.673\n",
            "20517/20850 (epoch 49), train_loss = 1.883, time/batch = 0.678\n",
            "20518/20850 (epoch 49), train_loss = 1.892, time/batch = 0.675\n",
            "20519/20850 (epoch 49), train_loss = 1.865, time/batch = 0.672\n",
            "20520/20850 (epoch 49), train_loss = 1.913, time/batch = 0.680\n",
            "20521/20850 (epoch 49), train_loss = 1.891, time/batch = 0.645\n",
            "20522/20850 (epoch 49), train_loss = 1.876, time/batch = 0.700\n",
            "20523/20850 (epoch 49), train_loss = 1.884, time/batch = 0.678\n",
            "20524/20850 (epoch 49), train_loss = 1.876, time/batch = 0.671\n",
            "20525/20850 (epoch 49), train_loss = 1.905, time/batch = 0.672\n",
            "20526/20850 (epoch 49), train_loss = 1.871, time/batch = 0.674\n",
            "20527/20850 (epoch 49), train_loss = 1.868, time/batch = 0.684\n",
            "20528/20850 (epoch 49), train_loss = 1.893, time/batch = 0.602\n",
            "20529/20850 (epoch 49), train_loss = 1.917, time/batch = 0.508\n",
            "20530/20850 (epoch 49), train_loss = 1.868, time/batch = 0.505\n",
            "20531/20850 (epoch 49), train_loss = 1.881, time/batch = 0.508\n",
            "20532/20850 (epoch 49), train_loss = 1.874, time/batch = 0.506\n",
            "20533/20850 (epoch 49), train_loss = 1.904, time/batch = 0.505\n",
            "20534/20850 (epoch 49), train_loss = 1.874, time/batch = 0.529\n",
            "20535/20850 (epoch 49), train_loss = 1.893, time/batch = 0.502\n",
            "20536/20850 (epoch 49), train_loss = 1.910, time/batch = 0.509\n",
            "20537/20850 (epoch 49), train_loss = 1.892, time/batch = 0.504\n",
            "20538/20850 (epoch 49), train_loss = 1.886, time/batch = 0.509\n",
            "20539/20850 (epoch 49), train_loss = 1.870, time/batch = 0.529\n",
            "20540/20850 (epoch 49), train_loss = 1.893, time/batch = 0.528\n",
            "20541/20850 (epoch 49), train_loss = 1.906, time/batch = 0.502\n",
            "20542/20850 (epoch 49), train_loss = 1.870, time/batch = 0.521\n",
            "20543/20850 (epoch 49), train_loss = 1.881, time/batch = 0.501\n",
            "20544/20850 (epoch 49), train_loss = 1.864, time/batch = 0.518\n",
            "20545/20850 (epoch 49), train_loss = 1.868, time/batch = 0.500\n",
            "20546/20850 (epoch 49), train_loss = 1.889, time/batch = 0.508\n",
            "20547/20850 (epoch 49), train_loss = 1.904, time/batch = 0.534\n",
            "20548/20850 (epoch 49), train_loss = 1.913, time/batch = 0.613\n",
            "20549/20850 (epoch 49), train_loss = 1.867, time/batch = 0.704\n",
            "20550/20850 (epoch 49), train_loss = 1.869, time/batch = 0.665\n",
            "20551/20850 (epoch 49), train_loss = 1.882, time/batch = 0.657\n",
            "20552/20850 (epoch 49), train_loss = 1.905, time/batch = 0.656\n",
            "20553/20850 (epoch 49), train_loss = 1.888, time/batch = 0.643\n",
            "20554/20850 (epoch 49), train_loss = 1.896, time/batch = 0.685\n",
            "20555/20850 (epoch 49), train_loss = 1.905, time/batch = 0.654\n",
            "20556/20850 (epoch 49), train_loss = 1.905, time/batch = 0.659\n",
            "20557/20850 (epoch 49), train_loss = 1.886, time/batch = 0.655\n",
            "20558/20850 (epoch 49), train_loss = 1.885, time/batch = 0.655\n",
            "20559/20850 (epoch 49), train_loss = 1.900, time/batch = 0.656\n",
            "20560/20850 (epoch 49), train_loss = 1.883, time/batch = 0.659\n",
            "20561/20850 (epoch 49), train_loss = 1.919, time/batch = 0.644\n",
            "20562/20850 (epoch 49), train_loss = 1.901, time/batch = 0.647\n",
            "20563/20850 (epoch 49), train_loss = 1.904, time/batch = 0.658\n",
            "20564/20850 (epoch 49), train_loss = 1.919, time/batch = 0.660\n",
            "20565/20850 (epoch 49), train_loss = 1.905, time/batch = 0.679\n",
            "20566/20850 (epoch 49), train_loss = 1.893, time/batch = 0.664\n",
            "20567/20850 (epoch 49), train_loss = 1.865, time/batch = 0.524\n",
            "20568/20850 (epoch 49), train_loss = 1.873, time/batch = 0.514\n",
            "20569/20850 (epoch 49), train_loss = 1.890, time/batch = 0.517\n",
            "20570/20850 (epoch 49), train_loss = 1.918, time/batch = 0.533\n",
            "20571/20850 (epoch 49), train_loss = 1.924, time/batch = 0.508\n",
            "20572/20850 (epoch 49), train_loss = 1.910, time/batch = 0.503\n",
            "20573/20850 (epoch 49), train_loss = 1.917, time/batch = 0.513\n",
            "20574/20850 (epoch 49), train_loss = 1.901, time/batch = 0.511\n",
            "20575/20850 (epoch 49), train_loss = 1.905, time/batch = 0.510\n",
            "20576/20850 (epoch 49), train_loss = 1.900, time/batch = 0.522\n",
            "20577/20850 (epoch 49), train_loss = 1.891, time/batch = 0.524\n",
            "20578/20850 (epoch 49), train_loss = 1.903, time/batch = 0.513\n",
            "20579/20850 (epoch 49), train_loss = 1.890, time/batch = 0.522\n",
            "20580/20850 (epoch 49), train_loss = 1.880, time/batch = 0.516\n",
            "20581/20850 (epoch 49), train_loss = 1.888, time/batch = 0.534\n",
            "20582/20850 (epoch 49), train_loss = 1.891, time/batch = 0.510\n",
            "20583/20850 (epoch 49), train_loss = 1.875, time/batch = 0.513\n",
            "20584/20850 (epoch 49), train_loss = 1.919, time/batch = 0.539\n",
            "20585/20850 (epoch 49), train_loss = 1.892, time/batch = 0.521\n",
            "20586/20850 (epoch 49), train_loss = 1.900, time/batch = 0.562\n",
            "20587/20850 (epoch 49), train_loss = 1.917, time/batch = 0.606\n",
            "20588/20850 (epoch 49), train_loss = 1.887, time/batch = 0.672\n",
            "20589/20850 (epoch 49), train_loss = 1.876, time/batch = 0.651\n",
            "20590/20850 (epoch 49), train_loss = 1.890, time/batch = 0.643\n",
            "20591/20850 (epoch 49), train_loss = 1.893, time/batch = 0.661\n",
            "20592/20850 (epoch 49), train_loss = 1.882, time/batch = 0.652\n",
            "20593/20850 (epoch 49), train_loss = 1.877, time/batch = 0.660\n",
            "20594/20850 (epoch 49), train_loss = 1.883, time/batch = 0.661\n",
            "20595/20850 (epoch 49), train_loss = 1.887, time/batch = 0.656\n",
            "20596/20850 (epoch 49), train_loss = 1.889, time/batch = 0.683\n",
            "20597/20850 (epoch 49), train_loss = 1.886, time/batch = 0.671\n",
            "20598/20850 (epoch 49), train_loss = 1.880, time/batch = 0.664\n",
            "20599/20850 (epoch 49), train_loss = 1.886, time/batch = 0.661\n",
            "20600/20850 (epoch 49), train_loss = 1.889, time/batch = 0.654\n",
            "20601/20850 (epoch 49), train_loss = 1.892, time/batch = 0.654\n",
            "20602/20850 (epoch 49), train_loss = 1.875, time/batch = 0.668\n",
            "20603/20850 (epoch 49), train_loss = 1.877, time/batch = 0.645\n",
            "20604/20850 (epoch 49), train_loss = 1.888, time/batch = 0.685\n",
            "20605/20850 (epoch 49), train_loss = 1.881, time/batch = 0.645\n",
            "20606/20850 (epoch 49), train_loss = 1.869, time/batch = 0.507\n",
            "20607/20850 (epoch 49), train_loss = 1.911, time/batch = 0.504\n",
            "20608/20850 (epoch 49), train_loss = 1.892, time/batch = 0.506\n",
            "20609/20850 (epoch 49), train_loss = 1.906, time/batch = 0.522\n",
            "20610/20850 (epoch 49), train_loss = 1.885, time/batch = 0.520\n",
            "20611/20850 (epoch 49), train_loss = 1.906, time/batch = 0.504\n",
            "20612/20850 (epoch 49), train_loss = 1.902, time/batch = 0.518\n",
            "20613/20850 (epoch 49), train_loss = 1.875, time/batch = 0.501\n",
            "20614/20850 (epoch 49), train_loss = 1.899, time/batch = 0.515\n",
            "20615/20850 (epoch 49), train_loss = 1.888, time/batch = 0.528\n",
            "20616/20850 (epoch 49), train_loss = 1.895, time/batch = 0.505\n",
            "20617/20850 (epoch 49), train_loss = 1.866, time/batch = 0.512\n",
            "20618/20850 (epoch 49), train_loss = 1.891, time/batch = 0.517\n",
            "20619/20850 (epoch 49), train_loss = 1.910, time/batch = 0.515\n",
            "20620/20850 (epoch 49), train_loss = 1.911, time/batch = 0.503\n",
            "20621/20850 (epoch 49), train_loss = 1.904, time/batch = 0.515\n",
            "20622/20850 (epoch 49), train_loss = 1.895, time/batch = 0.517\n",
            "20623/20850 (epoch 49), train_loss = 1.891, time/batch = 0.501\n",
            "20624/20850 (epoch 49), train_loss = 1.904, time/batch = 0.509\n",
            "20625/20850 (epoch 49), train_loss = 1.890, time/batch = 0.601\n",
            "20626/20850 (epoch 49), train_loss = 1.893, time/batch = 0.661\n",
            "20627/20850 (epoch 49), train_loss = 1.901, time/batch = 0.666\n",
            "20628/20850 (epoch 49), train_loss = 1.899, time/batch = 0.647\n",
            "20629/20850 (epoch 49), train_loss = 1.895, time/batch = 0.645\n",
            "20630/20850 (epoch 49), train_loss = 1.906, time/batch = 0.646\n",
            "20631/20850 (epoch 49), train_loss = 1.898, time/batch = 0.665\n",
            "20632/20850 (epoch 49), train_loss = 1.886, time/batch = 0.668\n",
            "20633/20850 (epoch 49), train_loss = 1.904, time/batch = 0.691\n",
            "20634/20850 (epoch 49), train_loss = 1.899, time/batch = 0.670\n",
            "20635/20850 (epoch 49), train_loss = 1.877, time/batch = 0.667\n",
            "20636/20850 (epoch 49), train_loss = 1.891, time/batch = 0.658\n",
            "20637/20850 (epoch 49), train_loss = 1.918, time/batch = 0.678\n",
            "20638/20850 (epoch 49), train_loss = 1.929, time/batch = 0.661\n",
            "20639/20850 (epoch 49), train_loss = 1.912, time/batch = 0.655\n",
            "20640/20850 (epoch 49), train_loss = 1.893, time/batch = 0.661\n",
            "20641/20850 (epoch 49), train_loss = 1.873, time/batch = 0.688\n",
            "20642/20850 (epoch 49), train_loss = 1.883, time/batch = 0.668\n",
            "20643/20850 (epoch 49), train_loss = 1.887, time/batch = 0.640\n",
            "20644/20850 (epoch 49), train_loss = 1.910, time/batch = 0.619\n",
            "20645/20850 (epoch 49), train_loss = 1.903, time/batch = 0.513\n",
            "20646/20850 (epoch 49), train_loss = 1.885, time/batch = 0.526\n",
            "20647/20850 (epoch 49), train_loss = 1.887, time/batch = 0.511\n",
            "20648/20850 (epoch 49), train_loss = 1.901, time/batch = 0.516\n",
            "20649/20850 (epoch 49), train_loss = 1.919, time/batch = 0.513\n",
            "20650/20850 (epoch 49), train_loss = 1.896, time/batch = 0.505\n",
            "20651/20850 (epoch 49), train_loss = 1.874, time/batch = 0.517\n",
            "20652/20850 (epoch 49), train_loss = 1.888, time/batch = 0.512\n",
            "20653/20850 (epoch 49), train_loss = 1.898, time/batch = 0.523\n",
            "20654/20850 (epoch 49), train_loss = 1.857, time/batch = 0.524\n",
            "20655/20850 (epoch 49), train_loss = 1.896, time/batch = 0.510\n",
            "20656/20850 (epoch 49), train_loss = 1.873, time/batch = 0.510\n",
            "20657/20850 (epoch 49), train_loss = 1.891, time/batch = 0.515\n",
            "20658/20850 (epoch 49), train_loss = 1.899, time/batch = 0.532\n",
            "20659/20850 (epoch 49), train_loss = 1.907, time/batch = 0.498\n",
            "20660/20850 (epoch 49), train_loss = 1.920, time/batch = 0.510\n",
            "20661/20850 (epoch 49), train_loss = 1.902, time/batch = 0.506\n",
            "20662/20850 (epoch 49), train_loss = 1.877, time/batch = 0.530\n",
            "20663/20850 (epoch 49), train_loss = 1.894, time/batch = 0.513\n",
            "20664/20850 (epoch 49), train_loss = 1.880, time/batch = 0.654\n",
            "20665/20850 (epoch 49), train_loss = 1.876, time/batch = 0.664\n",
            "20666/20850 (epoch 49), train_loss = 1.874, time/batch = 0.659\n",
            "20667/20850 (epoch 49), train_loss = 1.867, time/batch = 0.679\n",
            "20668/20850 (epoch 49), train_loss = 1.898, time/batch = 0.647\n",
            "20669/20850 (epoch 49), train_loss = 1.903, time/batch = 0.681\n",
            "20670/20850 (epoch 49), train_loss = 1.895, time/batch = 0.688\n",
            "20671/20850 (epoch 49), train_loss = 1.903, time/batch = 0.673\n",
            "20672/20850 (epoch 49), train_loss = 1.894, time/batch = 0.684\n",
            "20673/20850 (epoch 49), train_loss = 1.901, time/batch = 0.682\n",
            "20674/20850 (epoch 49), train_loss = 1.884, time/batch = 0.684\n",
            "20675/20850 (epoch 49), train_loss = 1.895, time/batch = 0.677\n",
            "20676/20850 (epoch 49), train_loss = 1.891, time/batch = 0.646\n",
            "20677/20850 (epoch 49), train_loss = 1.893, time/batch = 0.692\n",
            "20678/20850 (epoch 49), train_loss = 1.887, time/batch = 0.695\n",
            "20679/20850 (epoch 49), train_loss = 1.888, time/batch = 0.672\n",
            "20680/20850 (epoch 49), train_loss = 1.900, time/batch = 0.668\n",
            "20681/20850 (epoch 49), train_loss = 1.903, time/batch = 0.683\n",
            "20682/20850 (epoch 49), train_loss = 1.893, time/batch = 0.680\n",
            "20683/20850 (epoch 49), train_loss = 1.924, time/batch = 0.612\n",
            "20684/20850 (epoch 49), train_loss = 1.908, time/batch = 0.525\n",
            "20685/20850 (epoch 49), train_loss = 1.879, time/batch = 0.520\n",
            "20686/20850 (epoch 49), train_loss = 1.880, time/batch = 0.508\n",
            "20687/20850 (epoch 49), train_loss = 1.886, time/batch = 0.529\n",
            "20688/20850 (epoch 49), train_loss = 1.895, time/batch = 0.512\n",
            "20689/20850 (epoch 49), train_loss = 1.895, time/batch = 0.517\n",
            "20690/20850 (epoch 49), train_loss = 1.866, time/batch = 0.543\n",
            "20691/20850 (epoch 49), train_loss = 1.895, time/batch = 0.521\n",
            "20692/20850 (epoch 49), train_loss = 1.906, time/batch = 0.555\n",
            "20693/20850 (epoch 49), train_loss = 1.895, time/batch = 0.527\n",
            "20694/20850 (epoch 49), train_loss = 1.885, time/batch = 0.516\n",
            "20695/20850 (epoch 49), train_loss = 1.900, time/batch = 0.519\n",
            "20696/20850 (epoch 49), train_loss = 1.909, time/batch = 0.527\n",
            "20697/20850 (epoch 49), train_loss = 1.886, time/batch = 0.500\n",
            "20698/20850 (epoch 49), train_loss = 1.879, time/batch = 0.518\n",
            "20699/20850 (epoch 49), train_loss = 1.880, time/batch = 0.510\n",
            "20700/20850 (epoch 49), train_loss = 1.904, time/batch = 0.532\n",
            "20701/20850 (epoch 49), train_loss = 1.892, time/batch = 0.515\n",
            "20702/20850 (epoch 49), train_loss = 1.863, time/batch = 0.586\n",
            "20703/20850 (epoch 49), train_loss = 1.889, time/batch = 0.649\n",
            "20704/20850 (epoch 49), train_loss = 1.895, time/batch = 0.671\n",
            "20705/20850 (epoch 49), train_loss = 1.917, time/batch = 0.666\n",
            "20706/20850 (epoch 49), train_loss = 1.882, time/batch = 0.658\n",
            "20707/20850 (epoch 49), train_loss = 1.877, time/batch = 0.655\n",
            "20708/20850 (epoch 49), train_loss = 1.893, time/batch = 0.605\n",
            "20709/20850 (epoch 49), train_loss = 1.891, time/batch = 0.667\n",
            "20710/20850 (epoch 49), train_loss = 1.878, time/batch = 0.670\n",
            "20711/20850 (epoch 49), train_loss = 1.880, time/batch = 0.669\n",
            "20712/20850 (epoch 49), train_loss = 1.882, time/batch = 0.687\n",
            "20713/20850 (epoch 49), train_loss = 1.884, time/batch = 0.670\n",
            "20714/20850 (epoch 49), train_loss = 1.916, time/batch = 0.679\n",
            "20715/20850 (epoch 49), train_loss = 1.909, time/batch = 0.684\n",
            "20716/20850 (epoch 49), train_loss = 1.898, time/batch = 0.643\n",
            "20717/20850 (epoch 49), train_loss = 1.899, time/batch = 0.672\n",
            "20718/20850 (epoch 49), train_loss = 1.914, time/batch = 0.694\n",
            "20719/20850 (epoch 49), train_loss = 1.890, time/batch = 0.681\n",
            "20720/20850 (epoch 49), train_loss = 1.877, time/batch = 0.665\n",
            "20721/20850 (epoch 49), train_loss = 1.885, time/batch = 0.644\n",
            "20722/20850 (epoch 49), train_loss = 1.896, time/batch = 0.561\n",
            "20723/20850 (epoch 49), train_loss = 1.895, time/batch = 0.512\n",
            "20724/20850 (epoch 49), train_loss = 1.894, time/batch = 0.514\n",
            "20725/20850 (epoch 49), train_loss = 1.893, time/batch = 0.518\n",
            "20726/20850 (epoch 49), train_loss = 1.874, time/batch = 0.534\n",
            "20727/20850 (epoch 49), train_loss = 1.902, time/batch = 0.528\n",
            "20728/20850 (epoch 49), train_loss = 1.880, time/batch = 0.520\n",
            "20729/20850 (epoch 49), train_loss = 1.889, time/batch = 0.522\n",
            "20730/20850 (epoch 49), train_loss = 1.887, time/batch = 0.507\n",
            "20731/20850 (epoch 49), train_loss = 1.897, time/batch = 0.497\n",
            "20732/20850 (epoch 49), train_loss = 1.861, time/batch = 0.517\n",
            "20733/20850 (epoch 49), train_loss = 1.896, time/batch = 0.535\n",
            "20734/20850 (epoch 49), train_loss = 1.862, time/batch = 0.526\n",
            "20735/20850 (epoch 49), train_loss = 1.862, time/batch = 0.513\n",
            "20736/20850 (epoch 49), train_loss = 1.877, time/batch = 0.516\n",
            "20737/20850 (epoch 49), train_loss = 1.863, time/batch = 0.520\n",
            "20738/20850 (epoch 49), train_loss = 1.913, time/batch = 0.513\n",
            "20739/20850 (epoch 49), train_loss = 1.901, time/batch = 0.505\n",
            "20740/20850 (epoch 49), train_loss = 1.880, time/batch = 0.528\n",
            "20741/20850 (epoch 49), train_loss = 1.892, time/batch = 0.585\n",
            "20742/20850 (epoch 49), train_loss = 1.905, time/batch = 0.665\n",
            "20743/20850 (epoch 49), train_loss = 1.874, time/batch = 0.630\n",
            "20744/20850 (epoch 49), train_loss = 1.882, time/batch = 0.654\n",
            "20745/20850 (epoch 49), train_loss = 1.892, time/batch = 0.675\n",
            "20746/20850 (epoch 49), train_loss = 1.876, time/batch = 0.667\n",
            "20747/20850 (epoch 49), train_loss = 1.895, time/batch = 0.674\n",
            "20748/20850 (epoch 49), train_loss = 1.874, time/batch = 0.668\n",
            "20749/20850 (epoch 49), train_loss = 1.903, time/batch = 0.650\n",
            "20750/20850 (epoch 49), train_loss = 1.900, time/batch = 0.649\n",
            "20751/20850 (epoch 49), train_loss = 1.888, time/batch = 0.671\n",
            "20752/20850 (epoch 49), train_loss = 1.884, time/batch = 0.666\n",
            "20753/20850 (epoch 49), train_loss = 1.880, time/batch = 0.666\n",
            "20754/20850 (epoch 49), train_loss = 1.877, time/batch = 0.664\n",
            "20755/20850 (epoch 49), train_loss = 1.874, time/batch = 0.631\n",
            "20756/20850 (epoch 49), train_loss = 1.854, time/batch = 0.702\n",
            "20757/20850 (epoch 49), train_loss = 1.873, time/batch = 0.660\n",
            "20758/20850 (epoch 49), train_loss = 1.871, time/batch = 0.657\n",
            "20759/20850 (epoch 49), train_loss = 1.886, time/batch = 0.665\n",
            "20760/20850 (epoch 49), train_loss = 1.874, time/batch = 0.651\n",
            "20761/20850 (epoch 49), train_loss = 1.878, time/batch = 0.515\n",
            "20762/20850 (epoch 49), train_loss = 1.876, time/batch = 0.511\n",
            "20763/20850 (epoch 49), train_loss = 1.891, time/batch = 0.529\n",
            "20764/20850 (epoch 49), train_loss = 1.876, time/batch = 0.517\n",
            "20765/20850 (epoch 49), train_loss = 1.876, time/batch = 0.500\n",
            "20766/20850 (epoch 49), train_loss = 1.892, time/batch = 0.507\n",
            "20767/20850 (epoch 49), train_loss = 1.866, time/batch = 0.517\n",
            "20768/20850 (epoch 49), train_loss = 1.854, time/batch = 0.506\n",
            "20769/20850 (epoch 49), train_loss = 1.885, time/batch = 0.522\n",
            "20770/20850 (epoch 49), train_loss = 1.865, time/batch = 0.520\n",
            "20771/20850 (epoch 49), train_loss = 1.880, time/batch = 0.536\n",
            "20772/20850 (epoch 49), train_loss = 1.914, time/batch = 0.531\n",
            "20773/20850 (epoch 49), train_loss = 1.896, time/batch = 0.508\n",
            "20774/20850 (epoch 49), train_loss = 1.877, time/batch = 0.514\n",
            "20775/20850 (epoch 49), train_loss = 1.898, time/batch = 0.499\n",
            "20776/20850 (epoch 49), train_loss = 1.886, time/batch = 0.515\n",
            "20777/20850 (epoch 49), train_loss = 1.907, time/batch = 0.508\n",
            "20778/20850 (epoch 49), train_loss = 1.910, time/batch = 0.531\n",
            "20779/20850 (epoch 49), train_loss = 1.887, time/batch = 0.501\n",
            "20780/20850 (epoch 49), train_loss = 1.894, time/batch = 0.589\n",
            "20781/20850 (epoch 49), train_loss = 1.898, time/batch = 0.644\n",
            "20782/20850 (epoch 49), train_loss = 1.896, time/batch = 0.645\n",
            "20783/20850 (epoch 49), train_loss = 1.891, time/batch = 0.677\n",
            "20784/20850 (epoch 49), train_loss = 1.904, time/batch = 0.680\n",
            "20785/20850 (epoch 49), train_loss = 1.912, time/batch = 0.661\n",
            "20786/20850 (epoch 49), train_loss = 1.900, time/batch = 0.655\n",
            "20787/20850 (epoch 49), train_loss = 1.890, time/batch = 0.620\n",
            "20788/20850 (epoch 49), train_loss = 1.879, time/batch = 0.651\n",
            "20789/20850 (epoch 49), train_loss = 1.897, time/batch = 0.669\n",
            "20790/20850 (epoch 49), train_loss = 1.892, time/batch = 0.651\n",
            "20791/20850 (epoch 49), train_loss = 1.888, time/batch = 0.649\n",
            "20792/20850 (epoch 49), train_loss = 1.879, time/batch = 0.668\n",
            "20793/20850 (epoch 49), train_loss = 1.897, time/batch = 0.672\n",
            "20794/20850 (epoch 49), train_loss = 1.869, time/batch = 0.669\n",
            "20795/20850 (epoch 49), train_loss = 1.885, time/batch = 0.668\n",
            "20796/20850 (epoch 49), train_loss = 1.911, time/batch = 0.663\n",
            "20797/20850 (epoch 49), train_loss = 1.883, time/batch = 0.666\n",
            "20798/20850 (epoch 49), train_loss = 1.899, time/batch = 0.677\n",
            "20799/20850 (epoch 49), train_loss = 1.864, time/batch = 0.643\n",
            "20800/20850 (epoch 49), train_loss = 1.893, time/batch = 0.506\n",
            "20801/20850 (epoch 49), train_loss = 1.893, time/batch = 0.520\n",
            "20802/20850 (epoch 49), train_loss = 1.887, time/batch = 0.523\n",
            "20803/20850 (epoch 49), train_loss = 1.886, time/batch = 0.521\n",
            "20804/20850 (epoch 49), train_loss = 1.889, time/batch = 0.518\n",
            "20805/20850 (epoch 49), train_loss = 1.916, time/batch = 0.542\n",
            "20806/20850 (epoch 49), train_loss = 1.859, time/batch = 0.505\n",
            "20807/20850 (epoch 49), train_loss = 1.870, time/batch = 0.511\n",
            "20808/20850 (epoch 49), train_loss = 1.875, time/batch = 0.525\n",
            "20809/20850 (epoch 49), train_loss = 1.869, time/batch = 0.521\n",
            "20810/20850 (epoch 49), train_loss = 1.859, time/batch = 0.511\n",
            "20811/20850 (epoch 49), train_loss = 1.870, time/batch = 0.517\n",
            "20812/20850 (epoch 49), train_loss = 1.899, time/batch = 0.505\n",
            "20813/20850 (epoch 49), train_loss = 1.893, time/batch = 0.517\n",
            "20814/20850 (epoch 49), train_loss = 1.881, time/batch = 0.498\n",
            "20815/20850 (epoch 49), train_loss = 1.880, time/batch = 0.523\n",
            "20816/20850 (epoch 49), train_loss = 1.916, time/batch = 0.539\n",
            "20817/20850 (epoch 49), train_loss = 1.888, time/batch = 0.538\n",
            "20818/20850 (epoch 49), train_loss = 1.913, time/batch = 0.512\n",
            "20819/20850 (epoch 49), train_loss = 1.891, time/batch = 0.656\n",
            "20820/20850 (epoch 49), train_loss = 1.891, time/batch = 0.696\n",
            "20821/20850 (epoch 49), train_loss = 1.902, time/batch = 0.681\n",
            "20822/20850 (epoch 49), train_loss = 1.883, time/batch = 0.726\n",
            "20823/20850 (epoch 49), train_loss = 1.902, time/batch = 0.682\n",
            "20824/20850 (epoch 49), train_loss = 1.894, time/batch = 0.659\n",
            "20825/20850 (epoch 49), train_loss = 1.889, time/batch = 0.663\n",
            "20826/20850 (epoch 49), train_loss = 1.888, time/batch = 0.684\n",
            "20827/20850 (epoch 49), train_loss = 1.858, time/batch = 0.676\n",
            "20828/20850 (epoch 49), train_loss = 1.896, time/batch = 0.700\n",
            "20829/20850 (epoch 49), train_loss = 1.882, time/batch = 0.695\n",
            "20830/20850 (epoch 49), train_loss = 1.892, time/batch = 0.681\n",
            "20831/20850 (epoch 49), train_loss = 1.885, time/batch = 0.703\n",
            "20832/20850 (epoch 49), train_loss = 1.887, time/batch = 0.663\n",
            "20833/20850 (epoch 49), train_loss = 1.872, time/batch = 0.663\n",
            "20834/20850 (epoch 49), train_loss = 1.903, time/batch = 0.677\n",
            "20835/20850 (epoch 49), train_loss = 1.906, time/batch = 0.662\n",
            "20836/20850 (epoch 49), train_loss = 1.907, time/batch = 0.657\n",
            "20837/20850 (epoch 49), train_loss = 1.882, time/batch = 0.678\n",
            "20838/20850 (epoch 49), train_loss = 1.867, time/batch = 0.585\n",
            "20839/20850 (epoch 49), train_loss = 1.873, time/batch = 0.516\n",
            "20840/20850 (epoch 49), train_loss = 1.890, time/batch = 0.525\n",
            "20841/20850 (epoch 49), train_loss = 1.888, time/batch = 0.513\n",
            "20842/20850 (epoch 49), train_loss = 1.878, time/batch = 0.511\n",
            "20843/20850 (epoch 49), train_loss = 1.901, time/batch = 0.531\n",
            "20844/20850 (epoch 49), train_loss = 1.890, time/batch = 0.524\n",
            "20845/20850 (epoch 49), train_loss = 1.892, time/batch = 0.514\n",
            "20846/20850 (epoch 49), train_loss = 1.921, time/batch = 0.509\n",
            "20847/20850 (epoch 49), train_loss = 1.903, time/batch = 0.525\n",
            "20848/20850 (epoch 49), train_loss = 1.896, time/batch = 0.510\n",
            "20849/20850 (epoch 49), train_loss = 1.894, time/batch = 0.523\n",
            "model saved to /content/drive/Shareddrives/Materials UNI/UNIPI/ISPR/Midterm3 Assignment_4/save/save0/model.ckpt\n",
            "loading preprocessed files\n",
            "WARNING:tensorflow:From /content/char-rnn-tensorflow/model.py:36: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From /content/char-rnn-tensorflow/model.py:39: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/char-rnn-tensorflow/model.py:46: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/char-rnn-tensorflow/model.py:47: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/char-rnn-tensorflow/model.py:57: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/envs/myenv/lib/python3.6/site-packages/tensorflow_core/contrib/rnn/python/ops/rnn_cell.py:1529: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.add_weight` method instead.\n",
            "WARNING:tensorflow:From /content/char-rnn-tensorflow/model.py:86: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/envs/myenv/lib/python3.6/site-packages/tensorflow_core/python/ops/clip_ops.py:301: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /content/char-rnn-tensorflow/model.py:92: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/char-rnn-tensorflow/model.py:98: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/char-rnn-tensorflow/model.py:100: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "WARNING:tensorflow:From train.py:99: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2023-05-12 09:18:54.552859: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2023-05-12 09:18:54.579057: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-05-12 09:18:54.579296: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2023-05-12 09:18:54.579697: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2023-05-12 09:18:54.581534: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "2023-05-12 09:18:54.583065: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
            "2023-05-12 09:18:54.591329: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
            "2023-05-12 09:18:54.593686: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2023-05-12 09:18:54.595416: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2023-05-12 09:18:54.599762: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2023-05-12 09:18:54.599902: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-05-12 09:18:54.600153: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-05-12 09:18:54.600316: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2023-05-12 09:18:54.600747: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
            "2023-05-12 09:18:54.610533: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2199995000 Hz\n",
            "2023-05-12 09:18:54.610718: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x557cd7faf9d0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2023-05-12 09:18:54.610736: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2023-05-12 09:18:54.818674: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-05-12 09:18:54.818978: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x557cd54421a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2023-05-12 09:18:54.819006: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2023-05-12 09:18:54.819246: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-05-12 09:18:54.819420: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2023-05-12 09:18:54.819505: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2023-05-12 09:18:54.819526: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "2023-05-12 09:18:54.819547: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
            "2023-05-12 09:18:54.819567: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
            "2023-05-12 09:18:54.819586: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2023-05-12 09:18:54.819622: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2023-05-12 09:18:54.819647: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2023-05-12 09:18:54.819720: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-05-12 09:18:54.819949: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-05-12 09:18:54.820100: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2023-05-12 09:18:54.820176: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2023-05-12 09:18:54.820548: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2023-05-12 09:18:54.820573: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2023-05-12 09:18:54.820587: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2023-05-12 09:18:54.820720: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-05-12 09:18:54.820928: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-05-12 09:18:54.821088: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2023-05-12 09:18:54.821127: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14248 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "WARNING:tensorflow:From train.py:101: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
            "\n",
            "WARNING:tensorflow:From train.py:102: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "WARNING:tensorflow:From train.py:106: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From train.py:107: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "WARNING:tensorflow:From train.py:107: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From train.py:112: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "2023-05-12 09:19:32.425802: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "0/18950 (epoch 0), train_loss = 6.218, time/batch = 31.516\n",
            "model saved to /content/drive/Shareddrives/Materials UNI/UNIPI/ISPR/Midterm3 Assignment_4/save/save1/model.ckpt\n",
            "1/18950 (epoch 0), train_loss = 6.202, time/batch = 0.893\n",
            "2/18950 (epoch 0), train_loss = 6.167, time/batch = 1.022\n",
            "3/18950 (epoch 0), train_loss = 5.985, time/batch = 0.927\n",
            "4/18950 (epoch 0), train_loss = 3.905, time/batch = 0.842\n",
            "5/18950 (epoch 0), train_loss = 3.401, time/batch = 0.789\n",
            "6/18950 (epoch 0), train_loss = 3.161, time/batch = 0.828\n",
            "7/18950 (epoch 0), train_loss = 3.084, time/batch = 0.761\n",
            "8/18950 (epoch 0), train_loss = 3.116, time/batch = 0.774\n",
            "9/18950 (epoch 0), train_loss = 3.146, time/batch = 0.761\n",
            "10/18950 (epoch 0), train_loss = 3.189, time/batch = 0.735\n",
            "11/18950 (epoch 0), train_loss = 3.173, time/batch = 0.739\n",
            "12/18950 (epoch 0), train_loss = 3.137, time/batch = 0.742\n",
            "13/18950 (epoch 0), train_loss = 3.126, time/batch = 0.698\n",
            "14/18950 (epoch 0), train_loss = 3.106, time/batch = 0.731\n",
            "15/18950 (epoch 0), train_loss = 3.088, time/batch = 0.735\n",
            "16/18950 (epoch 0), train_loss = 3.107, time/batch = 0.725\n",
            "17/18950 (epoch 0), train_loss = 3.110, time/batch = 0.692\n",
            "18/18950 (epoch 0), train_loss = 3.118, time/batch = 0.550\n",
            "19/18950 (epoch 0), train_loss = 3.119, time/batch = 0.540\n",
            "20/18950 (epoch 0), train_loss = 3.055, time/batch = 0.542\n",
            "21/18950 (epoch 0), train_loss = 3.076, time/batch = 0.547\n",
            "22/18950 (epoch 0), train_loss = 3.084, time/batch = 0.550\n",
            "23/18950 (epoch 0), train_loss = 3.058, time/batch = 0.541\n",
            "24/18950 (epoch 0), train_loss = 3.074, time/batch = 0.547\n",
            "25/18950 (epoch 0), train_loss = 3.104, time/batch = 0.546\n",
            "26/18950 (epoch 0), train_loss = 3.081, time/batch = 0.519\n",
            "27/18950 (epoch 0), train_loss = 3.044, time/batch = 0.510\n",
            "28/18950 (epoch 0), train_loss = 3.063, time/batch = 0.519\n",
            "29/18950 (epoch 0), train_loss = 3.101, time/batch = 0.531\n",
            "30/18950 (epoch 0), train_loss = 3.065, time/batch = 0.519\n",
            "31/18950 (epoch 0), train_loss = 3.083, time/batch = 0.519\n",
            "32/18950 (epoch 0), train_loss = 3.097, time/batch = 0.514\n",
            "33/18950 (epoch 0), train_loss = 3.084, time/batch = 0.522\n",
            "34/18950 (epoch 0), train_loss = 3.083, time/batch = 0.504\n",
            "35/18950 (epoch 0), train_loss = 3.088, time/batch = 0.505\n",
            "36/18950 (epoch 0), train_loss = 3.083, time/batch = 0.555\n",
            "37/18950 (epoch 0), train_loss = 3.121, time/batch = 0.661\n",
            "38/18950 (epoch 0), train_loss = 3.057, time/batch = 0.669\n",
            "39/18950 (epoch 0), train_loss = 3.065, time/batch = 0.649\n",
            "40/18950 (epoch 0), train_loss = 3.064, time/batch = 0.702\n",
            "41/18950 (epoch 0), train_loss = 3.050, time/batch = 0.652\n",
            "42/18950 (epoch 0), train_loss = 3.087, time/batch = 0.675\n",
            "43/18950 (epoch 0), train_loss = 3.079, time/batch = 0.658\n",
            "44/18950 (epoch 0), train_loss = 3.068, time/batch = 0.679\n",
            "45/18950 (epoch 0), train_loss = 3.073, time/batch = 0.690\n",
            "46/18950 (epoch 0), train_loss = 3.129, time/batch = 0.685\n",
            "47/18950 (epoch 0), train_loss = 3.063, time/batch = 0.709\n",
            "48/18950 (epoch 0), train_loss = 3.063, time/batch = 0.675\n",
            "49/18950 (epoch 0), train_loss = 3.049, time/batch = 0.716\n",
            "50/18950 (epoch 0), train_loss = 3.037, time/batch = 0.680\n",
            "51/18950 (epoch 0), train_loss = 3.044, time/batch = 0.661\n",
            "52/18950 (epoch 0), train_loss = 3.063, time/batch = 0.695\n",
            "53/18950 (epoch 0), train_loss = 3.055, time/batch = 0.670\n",
            "54/18950 (epoch 0), train_loss = 3.076, time/batch = 0.680\n",
            "55/18950 (epoch 0), train_loss = 3.109, time/batch = 0.617\n",
            "56/18950 (epoch 0), train_loss = 3.054, time/batch = 0.523\n",
            "57/18950 (epoch 0), train_loss = 3.091, time/batch = 0.509\n",
            "58/18950 (epoch 0), train_loss = 3.056, time/batch = 0.537\n",
            "59/18950 (epoch 0), train_loss = 3.064, time/batch = 0.513\n",
            "60/18950 (epoch 0), train_loss = 3.057, time/batch = 0.520\n",
            "61/18950 (epoch 0), train_loss = 3.078, time/batch = 0.527\n",
            "62/18950 (epoch 0), train_loss = 3.058, time/batch = 0.518\n",
            "63/18950 (epoch 0), train_loss = 3.066, time/batch = 0.518\n",
            "64/18950 (epoch 0), train_loss = 3.084, time/batch = 0.516\n",
            "65/18950 (epoch 0), train_loss = 3.083, time/batch = 0.528\n",
            "66/18950 (epoch 0), train_loss = 3.064, time/batch = 0.527\n",
            "67/18950 (epoch 0), train_loss = 3.052, time/batch = 0.513\n",
            "68/18950 (epoch 0), train_loss = 3.047, time/batch = 0.530\n",
            "69/18950 (epoch 0), train_loss = 3.071, time/batch = 0.526\n",
            "70/18950 (epoch 0), train_loss = 3.074, time/batch = 0.555\n",
            "71/18950 (epoch 0), train_loss = 3.082, time/batch = 0.569\n",
            "72/18950 (epoch 0), train_loss = 3.079, time/batch = 0.534\n",
            "73/18950 (epoch 0), train_loss = 3.088, time/batch = 0.551\n",
            "74/18950 (epoch 0), train_loss = 3.069, time/batch = 0.625\n",
            "75/18950 (epoch 0), train_loss = 3.056, time/batch = 0.686\n",
            "76/18950 (epoch 0), train_loss = 3.048, time/batch = 0.712\n",
            "77/18950 (epoch 0), train_loss = 3.052, time/batch = 0.685\n",
            "78/18950 (epoch 0), train_loss = 3.032, time/batch = 0.680\n",
            "79/18950 (epoch 0), train_loss = 3.044, time/batch = 0.702\n",
            "80/18950 (epoch 0), train_loss = 3.055, time/batch = 0.705\n",
            "81/18950 (epoch 0), train_loss = 3.073, time/batch = 0.643\n",
            "82/18950 (epoch 0), train_loss = 3.082, time/batch = 0.694\n",
            "83/18950 (epoch 0), train_loss = 3.109, time/batch = 0.690\n",
            "84/18950 (epoch 0), train_loss = 3.089, time/batch = 0.675\n",
            "85/18950 (epoch 0), train_loss = 3.079, time/batch = 0.671\n",
            "86/18950 (epoch 0), train_loss = 3.122, time/batch = 0.671\n",
            "87/18950 (epoch 0), train_loss = 3.088, time/batch = 0.675\n",
            "88/18950 (epoch 0), train_loss = 3.072, time/batch = 0.691\n",
            "89/18950 (epoch 0), train_loss = 3.070, time/batch = 0.676\n",
            "90/18950 (epoch 0), train_loss = 3.092, time/batch = 0.673\n",
            "91/18950 (epoch 0), train_loss = 3.072, time/batch = 0.679\n",
            "92/18950 (epoch 0), train_loss = 3.055, time/batch = 0.619\n",
            "93/18950 (epoch 0), train_loss = 3.077, time/batch = 0.539\n",
            "94/18950 (epoch 0), train_loss = 3.091, time/batch = 0.528\n",
            "95/18950 (epoch 0), train_loss = 3.059, time/batch = 0.517\n",
            "96/18950 (epoch 0), train_loss = 3.076, time/batch = 0.511\n",
            "97/18950 (epoch 0), train_loss = 3.084, time/batch = 0.522\n",
            "98/18950 (epoch 0), train_loss = 3.071, time/batch = 0.510\n",
            "99/18950 (epoch 0), train_loss = 3.077, time/batch = 0.537\n",
            "100/18950 (epoch 0), train_loss = 3.090, time/batch = 0.523\n",
            "101/18950 (epoch 0), train_loss = 3.103, time/batch = 0.523\n",
            "102/18950 (epoch 0), train_loss = 3.063, time/batch = 0.519\n",
            "103/18950 (epoch 0), train_loss = 3.083, time/batch = 0.522\n",
            "104/18950 (epoch 0), train_loss = 3.076, time/batch = 0.552\n",
            "105/18950 (epoch 0), train_loss = 3.047, time/batch = 0.568\n",
            "106/18950 (epoch 0), train_loss = 3.066, time/batch = 0.532\n",
            "107/18950 (epoch 0), train_loss = 3.058, time/batch = 0.530\n",
            "108/18950 (epoch 0), train_loss = 3.062, time/batch = 0.547\n",
            "109/18950 (epoch 0), train_loss = 3.074, time/batch = 0.548\n",
            "110/18950 (epoch 0), train_loss = 3.089, time/batch = 0.518\n",
            "111/18950 (epoch 0), train_loss = 3.094, time/batch = 0.621\n",
            "112/18950 (epoch 0), train_loss = 3.077, time/batch = 0.690\n",
            "113/18950 (epoch 0), train_loss = 3.069, time/batch = 0.676\n",
            "114/18950 (epoch 0), train_loss = 3.056, time/batch = 0.707\n",
            "115/18950 (epoch 0), train_loss = 3.066, time/batch = 0.691\n",
            "116/18950 (epoch 0), train_loss = 3.080, time/batch = 0.698\n",
            "117/18950 (epoch 0), train_loss = 3.067, time/batch = 0.684\n",
            "118/18950 (epoch 0), train_loss = 3.094, time/batch = 0.693\n",
            "119/18950 (epoch 0), train_loss = 3.070, time/batch = 0.681\n",
            "120/18950 (epoch 0), train_loss = 3.068, time/batch = 0.679\n",
            "121/18950 (epoch 0), train_loss = 3.080, time/batch = 0.740\n",
            "122/18950 (epoch 0), train_loss = 3.112, time/batch = 0.673\n",
            "123/18950 (epoch 0), train_loss = 3.076, time/batch = 0.638\n",
            "124/18950 (epoch 0), train_loss = 3.069, time/batch = 0.674\n",
            "125/18950 (epoch 0), train_loss = 3.048, time/batch = 0.671\n",
            "126/18950 (epoch 0), train_loss = 3.057, time/batch = 0.680\n",
            "127/18950 (epoch 0), train_loss = 3.045, time/batch = 0.690\n",
            "128/18950 (epoch 0), train_loss = 3.051, time/batch = 0.691\n",
            "129/18950 (epoch 0), train_loss = 3.114, time/batch = 0.684\n",
            "130/18950 (epoch 0), train_loss = 3.062, time/batch = 0.541\n",
            "131/18950 (epoch 0), train_loss = 3.052, time/batch = 0.527\n",
            "132/18950 (epoch 0), train_loss = 3.078, time/batch = 0.524\n",
            "133/18950 (epoch 0), train_loss = 3.051, time/batch = 0.521\n",
            "134/18950 (epoch 0), train_loss = 3.050, time/batch = 0.531\n",
            "135/18950 (epoch 0), train_loss = 3.048, time/batch = 0.531\n",
            "136/18950 (epoch 0), train_loss = 3.044, time/batch = 0.531\n",
            "137/18950 (epoch 0), train_loss = 3.046, time/batch = 0.515\n",
            "138/18950 (epoch 0), train_loss = 3.066, time/batch = 0.528\n",
            "139/18950 (epoch 0), train_loss = 3.035, time/batch = 0.543\n",
            "140/18950 (epoch 0), train_loss = 3.057, time/batch = 0.534\n",
            "141/18950 (epoch 0), train_loss = 3.070, time/batch = 0.538\n",
            "142/18950 (epoch 0), train_loss = 3.081, time/batch = 0.541\n",
            "143/18950 (epoch 0), train_loss = 3.055, time/batch = 0.531\n",
            "144/18950 (epoch 0), train_loss = 3.087, time/batch = 0.530\n",
            "145/18950 (epoch 0), train_loss = 3.096, time/batch = 0.531\n",
            "146/18950 (epoch 0), train_loss = 3.096, time/batch = 0.534\n",
            "147/18950 (epoch 0), train_loss = 3.069, time/batch = 0.522\n",
            "148/18950 (epoch 0), train_loss = 3.042, time/batch = 0.533\n",
            "149/18950 (epoch 0), train_loss = 3.087, time/batch = 0.668\n",
            "150/18950 (epoch 0), train_loss = 3.074, time/batch = 0.688\n",
            "151/18950 (epoch 0), train_loss = 3.048, time/batch = 0.695\n",
            "152/18950 (epoch 0), train_loss = 3.040, time/batch = 0.684\n",
            "153/18950 (epoch 0), train_loss = 3.050, time/batch = 0.707\n",
            "154/18950 (epoch 0), train_loss = 3.072, time/batch = 0.702\n",
            "155/18950 (epoch 0), train_loss = 3.083, time/batch = 0.727\n",
            "156/18950 (epoch 0), train_loss = 3.079, time/batch = 0.700\n",
            "157/18950 (epoch 0), train_loss = 3.075, time/batch = 0.700\n",
            "158/18950 (epoch 0), train_loss = 3.073, time/batch = 0.685\n",
            "159/18950 (epoch 0), train_loss = 3.066, time/batch = 0.681\n",
            "160/18950 (epoch 0), train_loss = 3.073, time/batch = 0.685\n",
            "161/18950 (epoch 0), train_loss = 3.032, time/batch = 0.727\n",
            "162/18950 (epoch 0), train_loss = 3.090, time/batch = 0.685\n",
            "163/18950 (epoch 0), train_loss = 3.049, time/batch = 0.718\n",
            "164/18950 (epoch 0), train_loss = 3.047, time/batch = 0.713\n",
            "165/18950 (epoch 0), train_loss = 3.032, time/batch = 0.679\n",
            "166/18950 (epoch 0), train_loss = 3.048, time/batch = 0.709\n",
            "167/18950 (epoch 0), train_loss = 3.083, time/batch = 0.600\n",
            "168/18950 (epoch 0), train_loss = 3.128, time/batch = 0.526\n",
            "169/18950 (epoch 0), train_loss = 3.086, time/batch = 0.526\n",
            "170/18950 (epoch 0), train_loss = 3.045, time/batch = 0.514\n",
            "171/18950 (epoch 0), train_loss = 3.030, time/batch = 0.545\n",
            "172/18950 (epoch 0), train_loss = 3.058, time/batch = 0.527\n",
            "173/18950 (epoch 0), train_loss = 3.042, time/batch = 0.558\n",
            "174/18950 (epoch 0), train_loss = 3.066, time/batch = 0.532\n",
            "175/18950 (epoch 0), train_loss = 3.040, time/batch = 0.526\n",
            "176/18950 (epoch 0), train_loss = 3.066, time/batch = 0.524\n",
            "177/18950 (epoch 0), train_loss = 3.064, time/batch = 0.534\n",
            "178/18950 (epoch 0), train_loss = 3.056, time/batch = 0.526\n",
            "179/18950 (epoch 0), train_loss = 3.060, time/batch = 0.536\n",
            "180/18950 (epoch 0), train_loss = 3.104, time/batch = 0.535\n",
            "181/18950 (epoch 0), train_loss = 3.055, time/batch = 0.528\n",
            "182/18950 (epoch 0), train_loss = 3.083, time/batch = 0.523\n",
            "183/18950 (epoch 0), train_loss = 3.061, time/batch = 0.535\n",
            "184/18950 (epoch 0), train_loss = 3.063, time/batch = 0.529\n",
            "185/18950 (epoch 0), train_loss = 3.067, time/batch = 0.534\n",
            "186/18950 (epoch 0), train_loss = 3.058, time/batch = 0.603\n",
            "187/18950 (epoch 0), train_loss = 3.062, time/batch = 0.698\n",
            "188/18950 (epoch 0), train_loss = 3.056, time/batch = 0.708\n",
            "189/18950 (epoch 0), train_loss = 3.084, time/batch = 0.685\n",
            "190/18950 (epoch 0), train_loss = 3.044, time/batch = 0.670\n",
            "191/18950 (epoch 0), train_loss = 3.063, time/batch = 0.688\n",
            "192/18950 (epoch 0), train_loss = 3.024, time/batch = 0.696\n",
            "193/18950 (epoch 0), train_loss = 3.049, time/batch = 0.679\n",
            "194/18950 (epoch 0), train_loss = 3.033, time/batch = 0.686\n",
            "195/18950 (epoch 0), train_loss = 3.011, time/batch = 0.713\n",
            "196/18950 (epoch 0), train_loss = 3.018, time/batch = 0.667\n",
            "197/18950 (epoch 0), train_loss = 3.020, time/batch = 0.665\n",
            "198/18950 (epoch 0), train_loss = 3.087, time/batch = 0.713\n",
            "199/18950 (epoch 0), train_loss = 3.065, time/batch = 0.680\n",
            "200/18950 (epoch 0), train_loss = 3.044, time/batch = 0.691\n",
            "201/18950 (epoch 0), train_loss = 3.034, time/batch = 0.691\n",
            "202/18950 (epoch 0), train_loss = 3.025, time/batch = 0.682\n",
            "203/18950 (epoch 0), train_loss = 3.016, time/batch = 0.737\n",
            "204/18950 (epoch 0), train_loss = 3.050, time/batch = 0.713\n",
            "205/18950 (epoch 0), train_loss = 3.059, time/batch = 0.531\n",
            "206/18950 (epoch 0), train_loss = 3.026, time/batch = 0.545\n",
            "207/18950 (epoch 0), train_loss = 3.038, time/batch = 0.528\n",
            "208/18950 (epoch 0), train_loss = 3.056, time/batch = 0.527\n",
            "209/18950 (epoch 0), train_loss = 3.060, time/batch = 0.537\n",
            "210/18950 (epoch 0), train_loss = 3.064, time/batch = 0.538\n",
            "211/18950 (epoch 0), train_loss = 3.066, time/batch = 0.546\n",
            "212/18950 (epoch 0), train_loss = 3.051, time/batch = 0.560\n",
            "213/18950 (epoch 0), train_loss = 3.029, time/batch = 0.537\n",
            "214/18950 (epoch 0), train_loss = 3.031, time/batch = 0.542\n",
            "215/18950 (epoch 0), train_loss = 3.045, time/batch = 0.524\n",
            "216/18950 (epoch 0), train_loss = 3.063, time/batch = 0.547\n",
            "217/18950 (epoch 0), train_loss = 3.031, time/batch = 0.527\n",
            "218/18950 (epoch 0), train_loss = 3.031, time/batch = 0.531\n",
            "219/18950 (epoch 0), train_loss = 3.024, time/batch = 0.539\n",
            "220/18950 (epoch 0), train_loss = 3.009, time/batch = 0.552\n",
            "221/18950 (epoch 0), train_loss = 3.012, time/batch = 0.528\n",
            "222/18950 (epoch 0), train_loss = 3.023, time/batch = 0.537\n",
            "223/18950 (epoch 0), train_loss = 3.064, time/batch = 0.601\n",
            "224/18950 (epoch 0), train_loss = 3.034, time/batch = 0.681\n",
            "225/18950 (epoch 0), train_loss = 3.062, time/batch = 0.681\n",
            "226/18950 (epoch 0), train_loss = 3.050, time/batch = 0.701\n",
            "227/18950 (epoch 0), train_loss = 3.052, time/batch = 0.698\n",
            "228/18950 (epoch 0), train_loss = 3.045, time/batch = 0.699\n",
            "229/18950 (epoch 0), train_loss = 3.063, time/batch = 0.713\n",
            "230/18950 (epoch 0), train_loss = 3.055, time/batch = 0.692\n",
            "231/18950 (epoch 0), train_loss = 3.068, time/batch = 0.698\n",
            "232/18950 (epoch 0), train_loss = 3.051, time/batch = 0.702\n",
            "233/18950 (epoch 0), train_loss = 3.015, time/batch = 0.687\n",
            "234/18950 (epoch 0), train_loss = 3.034, time/batch = 0.690\n",
            "235/18950 (epoch 0), train_loss = 3.014, time/batch = 0.680\n",
            "236/18950 (epoch 0), train_loss = 3.038, time/batch = 0.685\n",
            "237/18950 (epoch 0), train_loss = 3.026, time/batch = 0.710\n",
            "238/18950 (epoch 0), train_loss = 3.030, time/batch = 0.690\n",
            "239/18950 (epoch 0), train_loss = 3.021, time/batch = 0.689\n",
            "240/18950 (epoch 0), train_loss = 3.016, time/batch = 0.682\n",
            "241/18950 (epoch 0), train_loss = 3.039, time/batch = 0.638\n",
            "242/18950 (epoch 0), train_loss = 3.070, time/batch = 0.523\n",
            "243/18950 (epoch 0), train_loss = 3.035, time/batch = 0.552\n",
            "244/18950 (epoch 0), train_loss = 3.038, time/batch = 0.528\n",
            "245/18950 (epoch 0), train_loss = 3.060, time/batch = 0.544\n",
            "246/18950 (epoch 0), train_loss = 3.052, time/batch = 0.543\n",
            "247/18950 (epoch 0), train_loss = 3.034, time/batch = 0.535\n",
            "248/18950 (epoch 0), train_loss = 3.027, time/batch = 0.527\n",
            "249/18950 (epoch 0), train_loss = 3.043, time/batch = 0.533\n",
            "250/18950 (epoch 0), train_loss = 3.013, time/batch = 0.525\n",
            "251/18950 (epoch 0), train_loss = 3.010, time/batch = 0.534\n",
            "252/18950 (epoch 0), train_loss = 3.022, time/batch = 0.525\n",
            "253/18950 (epoch 0), train_loss = 3.039, time/batch = 0.556\n",
            "254/18950 (epoch 0), train_loss = 3.024, time/batch = 0.572\n",
            "255/18950 (epoch 0), train_loss = 2.992, time/batch = 0.557\n",
            "256/18950 (epoch 0), train_loss = 2.971, time/batch = 0.554\n",
            "257/18950 (epoch 0), train_loss = 2.977, time/batch = 0.529\n",
            "258/18950 (epoch 0), train_loss = 2.960, time/batch = 0.570\n",
            "259/18950 (epoch 0), train_loss = 2.972, time/batch = 0.528\n",
            "260/18950 (epoch 0), train_loss = 2.974, time/batch = 0.696\n",
            "261/18950 (epoch 0), train_loss = 2.956, time/batch = 0.720\n",
            "262/18950 (epoch 0), train_loss = 2.964, time/batch = 0.748\n",
            "263/18950 (epoch 0), train_loss = 2.991, time/batch = 0.681\n",
            "264/18950 (epoch 0), train_loss = 2.975, time/batch = 0.718\n",
            "265/18950 (epoch 0), train_loss = 2.969, time/batch = 0.728\n",
            "266/18950 (epoch 0), train_loss = 2.956, time/batch = 0.739\n",
            "267/18950 (epoch 0), train_loss = 2.957, time/batch = 0.715\n",
            "268/18950 (epoch 0), train_loss = 2.946, time/batch = 0.728\n",
            "269/18950 (epoch 0), train_loss = 2.925, time/batch = 0.697\n",
            "270/18950 (epoch 0), train_loss = 2.931, time/batch = 0.736\n",
            "271/18950 (epoch 0), train_loss = 2.942, time/batch = 0.716\n",
            "272/18950 (epoch 0), train_loss = 2.909, time/batch = 0.708\n",
            "273/18950 (epoch 0), train_loss = 2.958, time/batch = 0.709\n",
            "274/18950 (epoch 0), train_loss = 2.918, time/batch = 0.705\n",
            "275/18950 (epoch 0), train_loss = 2.962, time/batch = 0.726\n",
            "276/18950 (epoch 0), train_loss = 2.938, time/batch = 0.717\n",
            "277/18950 (epoch 0), train_loss = 2.918, time/batch = 0.723\n",
            "278/18950 (epoch 0), train_loss = 2.928, time/batch = 0.547\n",
            "279/18950 (epoch 0), train_loss = 2.929, time/batch = 0.532\n",
            "280/18950 (epoch 0), train_loss = 2.908, time/batch = 0.547\n",
            "281/18950 (epoch 0), train_loss = 2.931, time/batch = 0.547\n",
            "282/18950 (epoch 0), train_loss = 2.926, time/batch = 0.544\n",
            "283/18950 (epoch 0), train_loss = 2.917, time/batch = 0.542\n",
            "284/18950 (epoch 0), train_loss = 2.907, time/batch = 0.542\n",
            "285/18950 (epoch 0), train_loss = 2.879, time/batch = 0.547\n",
            "286/18950 (epoch 0), train_loss = 2.911, time/batch = 0.545\n",
            "287/18950 (epoch 0), train_loss = 2.885, time/batch = 0.546\n",
            "288/18950 (epoch 0), train_loss = 2.918, time/batch = 0.546\n",
            "289/18950 (epoch 0), train_loss = 2.924, time/batch = 0.557\n",
            "290/18950 (epoch 0), train_loss = 2.922, time/batch = 0.547\n",
            "291/18950 (epoch 0), train_loss = 2.896, time/batch = 0.549\n",
            "292/18950 (epoch 0), train_loss = 2.886, time/batch = 0.554\n",
            "293/18950 (epoch 0), train_loss = 2.906, time/batch = 0.557\n",
            "294/18950 (epoch 0), train_loss = 2.950, time/batch = 0.555\n",
            "295/18950 (epoch 0), train_loss = 2.909, time/batch = 0.551\n",
            "296/18950 (epoch 0), train_loss = 2.901, time/batch = 0.636\n",
            "297/18950 (epoch 0), train_loss = 2.909, time/batch = 0.691\n",
            "298/18950 (epoch 0), train_loss = 2.905, time/batch = 0.711\n",
            "299/18950 (epoch 0), train_loss = 2.895, time/batch = 0.706\n",
            "300/18950 (epoch 0), train_loss = 2.897, time/batch = 0.692\n",
            "301/18950 (epoch 0), train_loss = 2.881, time/batch = 0.708\n",
            "302/18950 (epoch 0), train_loss = 2.864, time/batch = 0.737\n",
            "303/18950 (epoch 0), train_loss = 2.876, time/batch = 0.718\n",
            "304/18950 (epoch 0), train_loss = 2.863, time/batch = 0.700\n",
            "305/18950 (epoch 0), train_loss = 2.910, time/batch = 0.700\n",
            "306/18950 (epoch 0), train_loss = 2.903, time/batch = 0.707\n",
            "307/18950 (epoch 0), train_loss = 2.868, time/batch = 0.686\n",
            "308/18950 (epoch 0), train_loss = 2.887, time/batch = 0.745\n",
            "309/18950 (epoch 0), train_loss = 2.881, time/batch = 0.728\n",
            "310/18950 (epoch 0), train_loss = 2.848, time/batch = 0.719\n",
            "311/18950 (epoch 0), train_loss = 2.853, time/batch = 0.668\n",
            "312/18950 (epoch 0), train_loss = 2.855, time/batch = 0.710\n",
            "313/18950 (epoch 0), train_loss = 2.885, time/batch = 0.708\n",
            "314/18950 (epoch 0), train_loss = 2.925, time/batch = 0.662\n",
            "315/18950 (epoch 0), train_loss = 2.909, time/batch = 0.537\n",
            "316/18950 (epoch 0), train_loss = 2.907, time/batch = 0.551\n",
            "317/18950 (epoch 0), train_loss = 2.876, time/batch = 0.552\n",
            "318/18950 (epoch 0), train_loss = 2.864, time/batch = 0.539\n",
            "319/18950 (epoch 0), train_loss = 2.864, time/batch = 0.540\n",
            "320/18950 (epoch 0), train_loss = 2.841, time/batch = 0.548\n",
            "321/18950 (epoch 0), train_loss = 2.882, time/batch = 0.559\n",
            "322/18950 (epoch 0), train_loss = 2.885, time/batch = 0.541\n",
            "323/18950 (epoch 0), train_loss = 2.872, time/batch = 0.577\n",
            "324/18950 (epoch 0), train_loss = 2.872, time/batch = 0.536\n",
            "325/18950 (epoch 0), train_loss = 2.888, time/batch = 0.542\n",
            "326/18950 (epoch 0), train_loss = 2.885, time/batch = 0.548\n",
            "327/18950 (epoch 0), train_loss = 2.864, time/batch = 0.553\n",
            "328/18950 (epoch 0), train_loss = 2.870, time/batch = 0.547\n",
            "329/18950 (epoch 0), train_loss = 2.859, time/batch = 0.547\n",
            "330/18950 (epoch 0), train_loss = 2.859, time/batch = 0.556\n",
            "331/18950 (epoch 0), train_loss = 2.884, time/batch = 0.539\n",
            "332/18950 (epoch 0), train_loss = 2.855, time/batch = 0.568\n",
            "333/18950 (epoch 0), train_loss = 2.852, time/batch = 0.694\n",
            "334/18950 (epoch 0), train_loss = 2.848, time/batch = 0.710\n",
            "335/18950 (epoch 0), train_loss = 2.888, time/batch = 0.693\n",
            "336/18950 (epoch 0), train_loss = 2.854, time/batch = 0.696\n",
            "337/18950 (epoch 0), train_loss = 2.859, time/batch = 0.733\n",
            "338/18950 (epoch 0), train_loss = 2.861, time/batch = 0.723\n",
            "339/18950 (epoch 0), train_loss = 2.831, time/batch = 0.714\n",
            "340/18950 (epoch 0), train_loss = 2.870, time/batch = 0.728\n",
            "341/18950 (epoch 0), train_loss = 2.882, time/batch = 0.713\n",
            "342/18950 (epoch 0), train_loss = 2.898, time/batch = 0.717\n",
            "343/18950 (epoch 0), train_loss = 2.846, time/batch = 0.683\n",
            "344/18950 (epoch 0), train_loss = 2.839, time/batch = 0.692\n",
            "345/18950 (epoch 0), train_loss = 2.848, time/batch = 0.722\n",
            "346/18950 (epoch 0), train_loss = 2.826, time/batch = 0.700\n",
            "347/18950 (epoch 0), train_loss = 2.851, time/batch = 0.698\n",
            "348/18950 (epoch 0), train_loss = 2.880, time/batch = 0.709\n",
            "349/18950 (epoch 0), train_loss = 2.852, time/batch = 0.753\n",
            "350/18950 (epoch 0), train_loss = 2.850, time/batch = 0.684\n",
            "351/18950 (epoch 0), train_loss = 2.841, time/batch = 0.548\n",
            "352/18950 (epoch 0), train_loss = 2.857, time/batch = 0.547\n",
            "353/18950 (epoch 0), train_loss = 2.816, time/batch = 0.607\n",
            "354/18950 (epoch 0), train_loss = 2.834, time/batch = 0.542\n",
            "355/18950 (epoch 0), train_loss = 2.846, time/batch = 0.566\n",
            "356/18950 (epoch 0), train_loss = 2.835, time/batch = 0.557\n",
            "357/18950 (epoch 0), train_loss = 2.866, time/batch = 0.543\n",
            "358/18950 (epoch 0), train_loss = 2.809, time/batch = 0.538\n",
            "359/18950 (epoch 0), train_loss = 2.840, time/batch = 0.546\n",
            "360/18950 (epoch 0), train_loss = 2.823, time/batch = 0.548\n",
            "361/18950 (epoch 0), train_loss = 2.829, time/batch = 0.543\n",
            "362/18950 (epoch 0), train_loss = 2.831, time/batch = 0.534\n",
            "363/18950 (epoch 0), train_loss = 2.846, time/batch = 0.553\n",
            "364/18950 (epoch 0), train_loss = 2.860, time/batch = 0.538\n",
            "365/18950 (epoch 0), train_loss = 2.851, time/batch = 0.549\n",
            "366/18950 (epoch 0), train_loss = 2.868, time/batch = 0.529\n",
            "367/18950 (epoch 0), train_loss = 2.846, time/batch = 0.557\n",
            "368/18950 (epoch 0), train_loss = 2.821, time/batch = 0.538\n",
            "369/18950 (epoch 0), train_loss = 2.839, time/batch = 0.684\n",
            "370/18950 (epoch 0), train_loss = 2.841, time/batch = 0.704\n",
            "371/18950 (epoch 0), train_loss = 2.832, time/batch = 0.710\n",
            "372/18950 (epoch 0), train_loss = 2.841, time/batch = 0.698\n",
            "373/18950 (epoch 0), train_loss = 2.832, time/batch = 0.720\n",
            "374/18950 (epoch 0), train_loss = 2.790, time/batch = 0.687\n",
            "375/18950 (epoch 0), train_loss = 2.816, time/batch = 0.702\n",
            "376/18950 (epoch 0), train_loss = 2.824, time/batch = 0.687\n",
            "377/18950 (epoch 0), train_loss = 2.830, time/batch = 0.682\n",
            "378/18950 (epoch 0), train_loss = 2.801, time/batch = 0.695\n",
            "379/18950 (epoch 1), train_loss = 2.832, time/batch = 0.801\n",
            "380/18950 (epoch 1), train_loss = 2.801, time/batch = 0.695\n",
            "381/18950 (epoch 1), train_loss = 2.798, time/batch = 0.712\n",
            "382/18950 (epoch 1), train_loss = 2.802, time/batch = 0.707\n",
            "383/18950 (epoch 1), train_loss = 2.831, time/batch = 0.706\n",
            "384/18950 (epoch 1), train_loss = 2.782, time/batch = 0.542\n",
            "385/18950 (epoch 1), train_loss = 2.767, time/batch = 0.544\n",
            "386/18950 (epoch 1), train_loss = 2.762, time/batch = 0.533\n",
            "387/18950 (epoch 1), train_loss = 2.776, time/batch = 0.541\n",
            "388/18950 (epoch 1), train_loss = 2.771, time/batch = 0.541\n",
            "389/18950 (epoch 1), train_loss = 2.806, time/batch = 0.557\n",
            "390/18950 (epoch 1), train_loss = 2.801, time/batch = 0.536\n",
            "391/18950 (epoch 1), train_loss = 2.790, time/batch = 0.533\n",
            "392/18950 (epoch 1), train_loss = 2.800, time/batch = 0.539\n",
            "393/18950 (epoch 1), train_loss = 2.811, time/batch = 0.567\n",
            "394/18950 (epoch 1), train_loss = 2.784, time/batch = 0.535\n",
            "395/18950 (epoch 1), train_loss = 2.803, time/batch = 0.535\n",
            "396/18950 (epoch 1), train_loss = 2.790, time/batch = 0.559\n",
            "397/18950 (epoch 1), train_loss = 2.792, time/batch = 0.545\n",
            "398/18950 (epoch 1), train_loss = 2.808, time/batch = 0.544\n",
            "399/18950 (epoch 1), train_loss = 2.758, time/batch = 0.552\n",
            "400/18950 (epoch 1), train_loss = 2.783, time/batch = 0.538\n",
            "401/18950 (epoch 1), train_loss = 2.787, time/batch = 0.552\n",
            "402/18950 (epoch 1), train_loss = 2.757, time/batch = 0.656\n",
            "403/18950 (epoch 1), train_loss = 2.767, time/batch = 0.713\n",
            "404/18950 (epoch 1), train_loss = 2.795, time/batch = 0.721\n",
            "405/18950 (epoch 1), train_loss = 2.784, time/batch = 0.717\n",
            "406/18950 (epoch 1), train_loss = 2.750, time/batch = 0.706\n",
            "407/18950 (epoch 1), train_loss = 2.771, time/batch = 0.683\n",
            "408/18950 (epoch 1), train_loss = 2.808, time/batch = 0.707\n",
            "409/18950 (epoch 1), train_loss = 2.760, time/batch = 0.726\n",
            "410/18950 (epoch 1), train_loss = 2.777, time/batch = 0.696\n",
            "411/18950 (epoch 1), train_loss = 2.799, time/batch = 0.712\n",
            "412/18950 (epoch 1), train_loss = 2.785, time/batch = 0.710\n",
            "413/18950 (epoch 1), train_loss = 2.788, time/batch = 0.689\n",
            "414/18950 (epoch 1), train_loss = 2.788, time/batch = 0.721\n",
            "415/18950 (epoch 1), train_loss = 2.782, time/batch = 0.708\n",
            "416/18950 (epoch 1), train_loss = 2.815, time/batch = 0.714\n",
            "417/18950 (epoch 1), train_loss = 2.754, time/batch = 0.713\n",
            "418/18950 (epoch 1), train_loss = 2.760, time/batch = 0.710\n",
            "419/18950 (epoch 1), train_loss = 2.755, time/batch = 0.677\n",
            "420/18950 (epoch 1), train_loss = 2.741, time/batch = 0.606\n",
            "421/18950 (epoch 1), train_loss = 2.790, time/batch = 0.536\n",
            "422/18950 (epoch 1), train_loss = 2.773, time/batch = 0.551\n",
            "423/18950 (epoch 1), train_loss = 2.762, time/batch = 0.547\n",
            "424/18950 (epoch 1), train_loss = 2.767, time/batch = 0.548\n",
            "425/18950 (epoch 1), train_loss = 2.827, time/batch = 0.541\n",
            "426/18950 (epoch 1), train_loss = 2.757, time/batch = 0.548\n",
            "427/18950 (epoch 1), train_loss = 2.751, time/batch = 0.544\n",
            "428/18950 (epoch 1), train_loss = 2.743, time/batch = 0.546\n",
            "429/18950 (epoch 1), train_loss = 2.734, time/batch = 0.542\n",
            "430/18950 (epoch 1), train_loss = 2.735, time/batch = 0.539\n",
            "431/18950 (epoch 1), train_loss = 2.764, time/batch = 0.546\n",
            "432/18950 (epoch 1), train_loss = 2.745, time/batch = 0.531\n",
            "433/18950 (epoch 1), train_loss = 2.770, time/batch = 0.554\n",
            "434/18950 (epoch 1), train_loss = 2.805, time/batch = 0.541\n",
            "435/18950 (epoch 1), train_loss = 2.746, time/batch = 0.550\n",
            "436/18950 (epoch 1), train_loss = 2.777, time/batch = 0.538\n",
            "437/18950 (epoch 1), train_loss = 2.750, time/batch = 0.551\n",
            "438/18950 (epoch 1), train_loss = 2.751, time/batch = 0.557\n",
            "439/18950 (epoch 1), train_loss = 2.747, time/batch = 0.695\n",
            "440/18950 (epoch 1), train_loss = 2.763, time/batch = 0.728\n",
            "441/18950 (epoch 1), train_loss = 2.747, time/batch = 0.706\n",
            "442/18950 (epoch 1), train_loss = 2.755, time/batch = 0.711\n",
            "443/18950 (epoch 1), train_loss = 2.773, time/batch = 0.720\n",
            "444/18950 (epoch 1), train_loss = 2.768, time/batch = 0.760\n",
            "445/18950 (epoch 1), train_loss = 2.754, time/batch = 0.742\n",
            "446/18950 (epoch 1), train_loss = 2.740, time/batch = 0.717\n",
            "447/18950 (epoch 1), train_loss = 2.733, time/batch = 0.725\n",
            "448/18950 (epoch 1), train_loss = 2.756, time/batch = 0.716\n",
            "449/18950 (epoch 1), train_loss = 2.766, time/batch = 0.767\n",
            "450/18950 (epoch 1), train_loss = 2.775, time/batch = 0.715\n",
            "451/18950 (epoch 1), train_loss = 2.767, time/batch = 0.753\n",
            "452/18950 (epoch 1), train_loss = 2.776, time/batch = 0.709\n",
            "453/18950 (epoch 1), train_loss = 2.746, time/batch = 0.725\n",
            "454/18950 (epoch 1), train_loss = 2.732, time/batch = 0.702\n",
            "455/18950 (epoch 1), train_loss = 2.726, time/batch = 0.733\n",
            "456/18950 (epoch 1), train_loss = 2.737, time/batch = 0.640\n",
            "457/18950 (epoch 1), train_loss = 2.714, time/batch = 0.550\n",
            "458/18950 (epoch 1), train_loss = 2.722, time/batch = 0.552\n",
            "459/18950 (epoch 1), train_loss = 2.729, time/batch = 0.550\n",
            "460/18950 (epoch 1), train_loss = 2.760, time/batch = 0.548\n",
            "461/18950 (epoch 1), train_loss = 2.764, time/batch = 0.554\n",
            "462/18950 (epoch 1), train_loss = 2.789, time/batch = 0.533\n",
            "463/18950 (epoch 1), train_loss = 2.774, time/batch = 0.552\n",
            "464/18950 (epoch 1), train_loss = 2.765, time/batch = 0.563\n",
            "465/18950 (epoch 1), train_loss = 2.802, time/batch = 0.558\n",
            "466/18950 (epoch 1), train_loss = 2.771, time/batch = 0.536\n",
            "467/18950 (epoch 1), train_loss = 2.748, time/batch = 0.535\n",
            "468/18950 (epoch 1), train_loss = 2.745, time/batch = 0.547\n",
            "469/18950 (epoch 1), train_loss = 2.776, time/batch = 0.547\n",
            "470/18950 (epoch 1), train_loss = 2.748, time/batch = 0.537\n",
            "471/18950 (epoch 1), train_loss = 2.734, time/batch = 0.550\n",
            "472/18950 (epoch 1), train_loss = 2.752, time/batch = 0.566\n",
            "473/18950 (epoch 1), train_loss = 2.767, time/batch = 0.566\n",
            "474/18950 (epoch 1), train_loss = 2.734, time/batch = 0.583\n",
            "475/18950 (epoch 1), train_loss = 2.748, time/batch = 0.717\n",
            "476/18950 (epoch 1), train_loss = 2.761, time/batch = 0.728\n",
            "477/18950 (epoch 1), train_loss = 2.750, time/batch = 0.708\n",
            "478/18950 (epoch 1), train_loss = 2.748, time/batch = 0.706\n",
            "479/18950 (epoch 1), train_loss = 2.761, time/batch = 0.693\n",
            "480/18950 (epoch 1), train_loss = 2.778, time/batch = 0.693\n",
            "481/18950 (epoch 1), train_loss = 2.736, time/batch = 0.705\n",
            "482/18950 (epoch 1), train_loss = 2.754, time/batch = 0.711\n",
            "483/18950 (epoch 1), train_loss = 2.751, time/batch = 0.723\n",
            "484/18950 (epoch 1), train_loss = 2.719, time/batch = 0.710\n",
            "485/18950 (epoch 1), train_loss = 2.736, time/batch = 0.717\n",
            "486/18950 (epoch 1), train_loss = 2.728, time/batch = 0.740\n",
            "487/18950 (epoch 1), train_loss = 2.738, time/batch = 0.739\n",
            "488/18950 (epoch 1), train_loss = 2.744, time/batch = 0.689\n",
            "489/18950 (epoch 1), train_loss = 2.759, time/batch = 0.677\n",
            "490/18950 (epoch 1), train_loss = 2.763, time/batch = 0.735\n",
            "491/18950 (epoch 1), train_loss = 2.743, time/batch = 0.733\n",
            "492/18950 (epoch 1), train_loss = 2.736, time/batch = 0.675\n",
            "493/18950 (epoch 1), train_loss = 2.724, time/batch = 0.562\n",
            "494/18950 (epoch 1), train_loss = 2.730, time/batch = 0.563\n",
            "495/18950 (epoch 1), train_loss = 2.751, time/batch = 0.557\n",
            "496/18950 (epoch 1), train_loss = 2.730, time/batch = 0.599\n",
            "497/18950 (epoch 1), train_loss = 2.758, time/batch = 0.566\n",
            "498/18950 (epoch 1), train_loss = 2.738, time/batch = 0.557\n",
            "499/18950 (epoch 1), train_loss = 2.731, time/batch = 0.552\n",
            "500/18950 (epoch 1), train_loss = 2.737, time/batch = 0.557\n",
            "501/18950 (epoch 1), train_loss = 2.779, time/batch = 0.542\n",
            "502/18950 (epoch 1), train_loss = 2.742, time/batch = 0.561\n",
            "503/18950 (epoch 1), train_loss = 2.736, time/batch = 0.539\n",
            "504/18950 (epoch 1), train_loss = 2.704, time/batch = 0.559\n",
            "505/18950 (epoch 1), train_loss = 2.724, time/batch = 0.543\n",
            "506/18950 (epoch 1), train_loss = 2.706, time/batch = 0.563\n",
            "507/18950 (epoch 1), train_loss = 2.704, time/batch = 0.564\n",
            "508/18950 (epoch 1), train_loss = 2.776, time/batch = 0.586\n",
            "509/18950 (epoch 1), train_loss = 2.721, time/batch = 0.571\n",
            "510/18950 (epoch 1), train_loss = 2.711, time/batch = 0.641\n",
            "511/18950 (epoch 1), train_loss = 2.734, time/batch = 0.716\n",
            "512/18950 (epoch 1), train_loss = 2.703, time/batch = 0.704\n",
            "513/18950 (epoch 1), train_loss = 2.706, time/batch = 0.719\n",
            "514/18950 (epoch 1), train_loss = 2.702, time/batch = 0.741\n",
            "515/18950 (epoch 1), train_loss = 2.696, time/batch = 0.712\n",
            "516/18950 (epoch 1), train_loss = 2.703, time/batch = 0.714\n",
            "517/18950 (epoch 1), train_loss = 2.713, time/batch = 0.709\n",
            "518/18950 (epoch 1), train_loss = 2.691, time/batch = 0.717\n",
            "519/18950 (epoch 1), train_loss = 2.705, time/batch = 0.750\n",
            "520/18950 (epoch 1), train_loss = 2.727, time/batch = 0.725\n",
            "521/18950 (epoch 1), train_loss = 2.727, time/batch = 0.695\n",
            "522/18950 (epoch 1), train_loss = 2.712, time/batch = 0.717\n",
            "523/18950 (epoch 1), train_loss = 2.740, time/batch = 0.709\n",
            "524/18950 (epoch 1), train_loss = 2.743, time/batch = 0.713\n",
            "525/18950 (epoch 1), train_loss = 2.746, time/batch = 0.712\n",
            "526/18950 (epoch 1), train_loss = 2.723, time/batch = 0.732\n",
            "527/18950 (epoch 1), train_loss = 2.694, time/batch = 0.738\n",
            "528/18950 (epoch 1), train_loss = 2.734, time/batch = 0.558\n",
            "529/18950 (epoch 1), train_loss = 2.727, time/batch = 0.555\n",
            "530/18950 (epoch 1), train_loss = 2.702, time/batch = 0.546\n",
            "531/18950 (epoch 1), train_loss = 2.688, time/batch = 0.569\n",
            "532/18950 (epoch 1), train_loss = 2.704, time/batch = 0.556\n",
            "533/18950 (epoch 1), train_loss = 2.727, time/batch = 0.552\n",
            "534/18950 (epoch 1), train_loss = 2.738, time/batch = 0.568\n",
            "535/18950 (epoch 1), train_loss = 2.727, time/batch = 0.555\n",
            "536/18950 (epoch 1), train_loss = 2.723, time/batch = 0.553\n",
            "537/18950 (epoch 1), train_loss = 2.724, time/batch = 0.544\n",
            "538/18950 (epoch 1), train_loss = 2.711, time/batch = 0.588\n",
            "539/18950 (epoch 1), train_loss = 2.724, time/batch = 0.546\n",
            "540/18950 (epoch 1), train_loss = 2.676, time/batch = 0.581\n",
            "541/18950 (epoch 1), train_loss = 2.732, time/batch = 0.584\n",
            "542/18950 (epoch 1), train_loss = 2.694, time/batch = 0.563\n",
            "543/18950 (epoch 1), train_loss = 2.690, time/batch = 0.572\n",
            "544/18950 (epoch 1), train_loss = 2.668, time/batch = 0.554\n",
            "545/18950 (epoch 1), train_loss = 2.692, time/batch = 0.602\n",
            "546/18950 (epoch 1), train_loss = 2.726, time/batch = 0.702\n",
            "547/18950 (epoch 1), train_loss = 2.768, time/batch = 0.669\n",
            "548/18950 (epoch 1), train_loss = 2.730, time/batch = 0.673\n",
            "549/18950 (epoch 1), train_loss = 2.694, time/batch = 0.707\n",
            "550/18950 (epoch 1), train_loss = 2.676, time/batch = 0.712\n",
            "551/18950 (epoch 1), train_loss = 2.706, time/batch = 0.689\n",
            "552/18950 (epoch 1), train_loss = 2.687, time/batch = 0.751\n",
            "553/18950 (epoch 1), train_loss = 2.710, time/batch = 0.739\n",
            "554/18950 (epoch 1), train_loss = 2.681, time/batch = 0.740\n",
            "555/18950 (epoch 1), train_loss = 2.707, time/batch = 0.720\n",
            "556/18950 (epoch 1), train_loss = 2.713, time/batch = 0.719\n",
            "557/18950 (epoch 1), train_loss = 2.696, time/batch = 0.707\n",
            "558/18950 (epoch 1), train_loss = 2.708, time/batch = 0.726\n",
            "559/18950 (epoch 1), train_loss = 2.750, time/batch = 0.702\n",
            "560/18950 (epoch 1), train_loss = 2.705, time/batch = 0.714\n",
            "561/18950 (epoch 1), train_loss = 2.732, time/batch = 0.711\n",
            "562/18950 (epoch 1), train_loss = 2.707, time/batch = 0.725\n",
            "563/18950 (epoch 1), train_loss = 2.712, time/batch = 0.695\n",
            "564/18950 (epoch 1), train_loss = 2.709, time/batch = 0.576\n",
            "565/18950 (epoch 1), train_loss = 2.705, time/batch = 0.557\n",
            "566/18950 (epoch 1), train_loss = 2.705, time/batch = 0.559\n",
            "567/18950 (epoch 1), train_loss = 2.697, time/batch = 0.554\n",
            "568/18950 (epoch 1), train_loss = 2.732, time/batch = 0.560\n",
            "569/18950 (epoch 1), train_loss = 2.680, time/batch = 0.551\n",
            "570/18950 (epoch 1), train_loss = 2.706, time/batch = 0.582\n",
            "571/18950 (epoch 1), train_loss = 2.672, time/batch = 0.561\n",
            "572/18950 (epoch 1), train_loss = 2.691, time/batch = 0.558\n",
            "573/18950 (epoch 1), train_loss = 2.671, time/batch = 0.551\n",
            "574/18950 (epoch 1), train_loss = 2.652, time/batch = 0.561\n",
            "575/18950 (epoch 1), train_loss = 2.665, time/batch = 0.550\n",
            "576/18950 (epoch 1), train_loss = 2.665, time/batch = 0.554\n",
            "577/18950 (epoch 1), train_loss = 2.729, time/batch = 0.580\n",
            "578/18950 (epoch 1), train_loss = 2.714, time/batch = 0.570\n",
            "579/18950 (epoch 1), train_loss = 2.691, time/batch = 0.562\n",
            "580/18950 (epoch 1), train_loss = 2.677, time/batch = 0.562\n",
            "581/18950 (epoch 1), train_loss = 2.669, time/batch = 0.559\n",
            "582/18950 (epoch 1), train_loss = 2.659, time/batch = 0.716\n",
            "583/18950 (epoch 1), train_loss = 2.692, time/batch = 0.690\n",
            "584/18950 (epoch 1), train_loss = 2.700, time/batch = 0.697\n",
            "585/18950 (epoch 1), train_loss = 2.675, time/batch = 0.739\n",
            "586/18950 (epoch 1), train_loss = 2.684, time/batch = 0.715\n",
            "587/18950 (epoch 1), train_loss = 2.702, time/batch = 0.713\n",
            "588/18950 (epoch 1), train_loss = 2.712, time/batch = 0.737\n",
            "589/18950 (epoch 1), train_loss = 2.712, time/batch = 0.723\n",
            "590/18950 (epoch 1), train_loss = 2.717, time/batch = 0.738\n",
            "591/18950 (epoch 1), train_loss = 2.697, time/batch = 0.758\n",
            "592/18950 (epoch 1), train_loss = 2.679, time/batch = 0.722\n",
            "593/18950 (epoch 1), train_loss = 2.678, time/batch = 0.714\n",
            "594/18950 (epoch 1), train_loss = 2.694, time/batch = 0.696\n",
            "595/18950 (epoch 1), train_loss = 2.710, time/batch = 0.729\n",
            "596/18950 (epoch 1), train_loss = 2.681, time/batch = 0.729\n",
            "597/18950 (epoch 1), train_loss = 2.682, time/batch = 0.737\n",
            "598/18950 (epoch 1), train_loss = 2.670, time/batch = 0.725\n",
            "599/18950 (epoch 1), train_loss = 2.662, time/batch = 0.719\n",
            "600/18950 (epoch 1), train_loss = 2.666, time/batch = 0.552\n",
            "601/18950 (epoch 1), train_loss = 2.673, time/batch = 0.559\n",
            "602/18950 (epoch 1), train_loss = 2.719, time/batch = 0.547\n",
            "603/18950 (epoch 1), train_loss = 2.689, time/batch = 0.559\n",
            "604/18950 (epoch 1), train_loss = 2.716, time/batch = 0.557\n",
            "605/18950 (epoch 1), train_loss = 2.706, time/batch = 0.560\n",
            "606/18950 (epoch 1), train_loss = 2.712, time/batch = 0.554\n",
            "607/18950 (epoch 1), train_loss = 2.700, time/batch = 0.563\n",
            "608/18950 (epoch 1), train_loss = 2.720, time/batch = 0.552\n",
            "609/18950 (epoch 1), train_loss = 2.712, time/batch = 0.550\n",
            "610/18950 (epoch 1), train_loss = 2.725, time/batch = 0.546\n",
            "611/18950 (epoch 1), train_loss = 2.707, time/batch = 0.549\n",
            "612/18950 (epoch 1), train_loss = 2.673, time/batch = 0.554\n",
            "613/18950 (epoch 1), train_loss = 2.689, time/batch = 0.548\n",
            "614/18950 (epoch 1), train_loss = 2.668, time/batch = 0.562\n",
            "615/18950 (epoch 1), train_loss = 2.688, time/batch = 0.552\n",
            "616/18950 (epoch 1), train_loss = 2.681, time/batch = 0.552\n",
            "617/18950 (epoch 1), train_loss = 2.696, time/batch = 0.547\n",
            "618/18950 (epoch 1), train_loss = 2.680, time/batch = 0.693\n",
            "619/18950 (epoch 1), train_loss = 2.679, time/batch = 0.715\n",
            "620/18950 (epoch 1), train_loss = 2.704, time/batch = 0.658\n",
            "621/18950 (epoch 1), train_loss = 2.731, time/batch = 0.716\n",
            "622/18950 (epoch 1), train_loss = 2.704, time/batch = 0.701\n",
            "623/18950 (epoch 1), train_loss = 2.707, time/batch = 0.696\n",
            "624/18950 (epoch 1), train_loss = 2.723, time/batch = 0.724\n",
            "625/18950 (epoch 1), train_loss = 2.713, time/batch = 0.716\n",
            "626/18950 (epoch 1), train_loss = 2.704, time/batch = 0.737\n",
            "627/18950 (epoch 1), train_loss = 2.695, time/batch = 0.709\n",
            "628/18950 (epoch 1), train_loss = 2.712, time/batch = 0.722\n",
            "629/18950 (epoch 1), train_loss = 2.679, time/batch = 0.706\n",
            "630/18950 (epoch 1), train_loss = 2.690, time/batch = 0.736\n",
            "631/18950 (epoch 1), train_loss = 2.707, time/batch = 0.713\n",
            "632/18950 (epoch 1), train_loss = 2.733, time/batch = 0.754\n",
            "633/18950 (epoch 1), train_loss = 2.716, time/batch = 0.765\n",
            "634/18950 (epoch 1), train_loss = 2.678, time/batch = 0.728\n",
            "635/18950 (epoch 1), train_loss = 2.671, time/batch = 0.743\n",
            "636/18950 (epoch 1), train_loss = 2.677, time/batch = 0.556\n",
            "637/18950 (epoch 1), train_loss = 2.658, time/batch = 0.591\n",
            "638/18950 (epoch 1), train_loss = 2.670, time/batch = 0.561\n",
            "639/18950 (epoch 1), train_loss = 2.678, time/batch = 0.564\n",
            "640/18950 (epoch 1), train_loss = 2.662, time/batch = 0.552\n",
            "641/18950 (epoch 1), train_loss = 2.663, time/batch = 0.564\n",
            "642/18950 (epoch 1), train_loss = 2.702, time/batch = 0.584\n",
            "643/18950 (epoch 1), train_loss = 2.684, time/batch = 0.563\n",
            "644/18950 (epoch 1), train_loss = 2.684, time/batch = 0.573\n",
            "645/18950 (epoch 1), train_loss = 2.682, time/batch = 0.579\n",
            "646/18950 (epoch 1), train_loss = 2.675, time/batch = 0.565\n",
            "647/18950 (epoch 1), train_loss = 2.665, time/batch = 0.566\n",
            "648/18950 (epoch 1), train_loss = 2.647, time/batch = 0.623\n",
            "649/18950 (epoch 1), train_loss = 2.652, time/batch = 0.562\n",
            "650/18950 (epoch 1), train_loss = 2.673, time/batch = 0.560\n",
            "651/18950 (epoch 1), train_loss = 2.646, time/batch = 0.564\n",
            "652/18950 (epoch 1), train_loss = 2.688, time/batch = 0.553\n",
            "653/18950 (epoch 1), train_loss = 2.644, time/batch = 0.680\n",
            "654/18950 (epoch 1), train_loss = 2.699, time/batch = 0.726\n",
            "655/18950 (epoch 1), train_loss = 2.670, time/batch = 0.716\n",
            "656/18950 (epoch 1), train_loss = 2.659, time/batch = 0.745\n",
            "657/18950 (epoch 1), train_loss = 2.673, time/batch = 0.728\n",
            "658/18950 (epoch 1), train_loss = 2.679, time/batch = 0.734\n",
            "659/18950 (epoch 1), train_loss = 2.660, time/batch = 0.740\n",
            "660/18950 (epoch 1), train_loss = 2.682, time/batch = 0.721\n",
            "661/18950 (epoch 1), train_loss = 2.676, time/batch = 0.746\n",
            "662/18950 (epoch 1), train_loss = 2.676, time/batch = 0.731\n",
            "663/18950 (epoch 1), train_loss = 2.660, time/batch = 0.743\n",
            "664/18950 (epoch 1), train_loss = 2.634, time/batch = 0.736\n",
            "665/18950 (epoch 1), train_loss = 2.659, time/batch = 0.738\n",
            "666/18950 (epoch 1), train_loss = 2.647, time/batch = 0.702\n",
            "667/18950 (epoch 1), train_loss = 2.673, time/batch = 0.726\n",
            "668/18950 (epoch 1), train_loss = 2.687, time/batch = 0.701\n",
            "669/18950 (epoch 1), train_loss = 2.677, time/batch = 0.725\n",
            "670/18950 (epoch 1), train_loss = 2.647, time/batch = 0.705\n",
            "671/18950 (epoch 1), train_loss = 2.645, time/batch = 0.563\n",
            "672/18950 (epoch 1), train_loss = 2.681, time/batch = 0.553\n",
            "673/18950 (epoch 1), train_loss = 2.727, time/batch = 0.569\n",
            "674/18950 (epoch 1), train_loss = 2.675, time/batch = 0.570\n",
            "675/18950 (epoch 1), train_loss = 2.672, time/batch = 0.578\n",
            "676/18950 (epoch 1), train_loss = 2.677, time/batch = 0.556\n",
            "677/18950 (epoch 1), train_loss = 2.669, time/batch = 0.556\n",
            "678/18950 (epoch 1), train_loss = 2.659, time/batch = 0.574\n",
            "679/18950 (epoch 1), train_loss = 2.659, time/batch = 0.552\n",
            "680/18950 (epoch 1), train_loss = 2.647, time/batch = 0.560\n",
            "681/18950 (epoch 1), train_loss = 2.630, time/batch = 0.572\n",
            "682/18950 (epoch 1), train_loss = 2.645, time/batch = 0.567\n",
            "683/18950 (epoch 1), train_loss = 2.634, time/batch = 0.608\n",
            "684/18950 (epoch 1), train_loss = 2.689, time/batch = 0.576\n",
            "685/18950 (epoch 1), train_loss = 2.677, time/batch = 0.565\n",
            "686/18950 (epoch 1), train_loss = 2.637, time/batch = 0.565\n",
            "687/18950 (epoch 1), train_loss = 2.658, time/batch = 0.562\n",
            "688/18950 (epoch 1), train_loss = 2.651, time/batch = 0.620\n",
            "689/18950 (epoch 1), train_loss = 2.618, time/batch = 0.670\n",
            "690/18950 (epoch 1), train_loss = 2.626, time/batch = 0.721\n",
            "691/18950 (epoch 1), train_loss = 2.630, time/batch = 0.722\n",
            "692/18950 (epoch 1), train_loss = 2.662, time/batch = 0.716\n",
            "693/18950 (epoch 1), train_loss = 2.701, time/batch = 0.714\n",
            "694/18950 (epoch 1), train_loss = 2.680, time/batch = 0.743\n",
            "695/18950 (epoch 1), train_loss = 2.684, time/batch = 0.721\n",
            "696/18950 (epoch 1), train_loss = 2.648, time/batch = 0.754\n",
            "697/18950 (epoch 1), train_loss = 2.637, time/batch = 0.699\n",
            "698/18950 (epoch 1), train_loss = 2.636, time/batch = 0.721\n",
            "699/18950 (epoch 1), train_loss = 2.622, time/batch = 0.703\n",
            "700/18950 (epoch 1), train_loss = 2.659, time/batch = 0.721\n",
            "701/18950 (epoch 1), train_loss = 2.657, time/batch = 0.746\n",
            "702/18950 (epoch 1), train_loss = 2.649, time/batch = 0.724\n",
            "703/18950 (epoch 1), train_loss = 2.648, time/batch = 0.729\n",
            "704/18950 (epoch 1), train_loss = 2.663, time/batch = 0.703\n",
            "705/18950 (epoch 1), train_loss = 2.655, time/batch = 0.751\n",
            "706/18950 (epoch 1), train_loss = 2.642, time/batch = 0.631\n",
            "707/18950 (epoch 1), train_loss = 2.651, time/batch = 0.576\n",
            "708/18950 (epoch 1), train_loss = 2.639, time/batch = 0.576\n",
            "709/18950 (epoch 1), train_loss = 2.639, time/batch = 0.574\n",
            "710/18950 (epoch 1), train_loss = 2.655, time/batch = 0.550\n",
            "711/18950 (epoch 1), train_loss = 2.638, time/batch = 0.554\n",
            "712/18950 (epoch 1), train_loss = 2.629, time/batch = 0.567\n",
            "713/18950 (epoch 1), train_loss = 2.623, time/batch = 0.548\n",
            "714/18950 (epoch 1), train_loss = 2.666, time/batch = 0.560\n",
            "715/18950 (epoch 1), train_loss = 2.635, time/batch = 0.561\n",
            "716/18950 (epoch 1), train_loss = 2.636, time/batch = 0.574\n",
            "717/18950 (epoch 1), train_loss = 2.648, time/batch = 0.565\n",
            "718/18950 (epoch 1), train_loss = 2.607, time/batch = 0.564\n",
            "719/18950 (epoch 1), train_loss = 2.652, time/batch = 0.564\n",
            "720/18950 (epoch 1), train_loss = 2.660, time/batch = 0.553\n",
            "721/18950 (epoch 1), train_loss = 2.667, time/batch = 0.556\n",
            "722/18950 (epoch 1), train_loss = 2.628, time/batch = 0.566\n",
            "723/18950 (epoch 1), train_loss = 2.616, time/batch = 0.569\n",
            "724/18950 (epoch 1), train_loss = 2.628, time/batch = 0.686\n",
            "725/18950 (epoch 1), train_loss = 2.603, time/batch = 0.735\n",
            "726/18950 (epoch 1), train_loss = 2.629, time/batch = 0.727\n",
            "727/18950 (epoch 1), train_loss = 2.651, time/batch = 0.731\n",
            "728/18950 (epoch 1), train_loss = 2.631, time/batch = 0.755\n",
            "729/18950 (epoch 1), train_loss = 2.636, time/batch = 0.756\n",
            "730/18950 (epoch 1), train_loss = 2.622, time/batch = 0.758\n",
            "731/18950 (epoch 1), train_loss = 2.635, time/batch = 0.746\n",
            "732/18950 (epoch 1), train_loss = 2.596, time/batch = 0.741\n",
            "733/18950 (epoch 1), train_loss = 2.614, time/batch = 0.753\n",
            "734/18950 (epoch 1), train_loss = 2.628, time/batch = 0.718\n",
            "735/18950 (epoch 1), train_loss = 2.627, time/batch = 0.775\n",
            "736/18950 (epoch 1), train_loss = 2.654, time/batch = 0.733\n",
            "737/18950 (epoch 1), train_loss = 2.592, time/batch = 0.706\n",
            "738/18950 (epoch 1), train_loss = 2.627, time/batch = 0.737\n",
            "739/18950 (epoch 1), train_loss = 2.604, time/batch = 0.741\n",
            "740/18950 (epoch 1), train_loss = 2.610, time/batch = 0.731\n",
            "741/18950 (epoch 1), train_loss = 2.609, time/batch = 0.694\n",
            "742/18950 (epoch 1), train_loss = 2.628, time/batch = 0.561\n",
            "743/18950 (epoch 1), train_loss = 2.635, time/batch = 0.578\n",
            "744/18950 (epoch 1), train_loss = 2.639, time/batch = 0.564\n",
            "745/18950 (epoch 1), train_loss = 2.652, time/batch = 0.568\n",
            "746/18950 (epoch 1), train_loss = 2.631, time/batch = 0.559\n",
            "747/18950 (epoch 1), train_loss = 2.603, time/batch = 0.564\n",
            "748/18950 (epoch 1), train_loss = 2.621, time/batch = 0.557\n",
            "749/18950 (epoch 1), train_loss = 2.621, time/batch = 0.561\n",
            "750/18950 (epoch 1), train_loss = 2.614, time/batch = 0.566\n",
            "751/18950 (epoch 1), train_loss = 2.624, time/batch = 0.584\n",
            "752/18950 (epoch 1), train_loss = 2.621, time/batch = 0.556\n",
            "753/18950 (epoch 1), train_loss = 2.580, time/batch = 0.573\n",
            "754/18950 (epoch 1), train_loss = 2.609, time/batch = 0.574\n",
            "755/18950 (epoch 1), train_loss = 2.618, time/batch = 0.563\n",
            "756/18950 (epoch 1), train_loss = 2.620, time/batch = 0.569\n",
            "757/18950 (epoch 1), train_loss = 2.596, time/batch = 0.561\n",
            "758/18950 (epoch 2), train_loss = 2.628, time/batch = 0.749\n",
            "759/18950 (epoch 2), train_loss = 2.601, time/batch = 0.722\n",
            "760/18950 (epoch 2), train_loss = 2.597, time/batch = 0.723\n",
            "761/18950 (epoch 2), train_loss = 2.599, time/batch = 0.733\n",
            "762/18950 (epoch 2), train_loss = 2.624, time/batch = 0.725\n",
            "763/18950 (epoch 2), train_loss = 2.576, time/batch = 0.733\n",
            "764/18950 (epoch 2), train_loss = 2.562, time/batch = 0.732\n",
            "765/18950 (epoch 2), train_loss = 2.564, time/batch = 0.745\n",
            "766/18950 (epoch 2), train_loss = 2.569, time/batch = 0.736\n",
            "767/18950 (epoch 2), train_loss = 2.570, time/batch = 0.717\n",
            "768/18950 (epoch 2), train_loss = 2.599, time/batch = 0.755\n",
            "769/18950 (epoch 2), train_loss = 2.591, time/batch = 0.731\n",
            "770/18950 (epoch 2), train_loss = 2.588, time/batch = 0.730\n",
            "771/18950 (epoch 2), train_loss = 2.595, time/batch = 0.769\n",
            "772/18950 (epoch 2), train_loss = 2.608, time/batch = 0.748\n",
            "773/18950 (epoch 2), train_loss = 2.585, time/batch = 0.728\n",
            "774/18950 (epoch 2), train_loss = 2.599, time/batch = 0.662\n",
            "775/18950 (epoch 2), train_loss = 2.590, time/batch = 0.577\n",
            "776/18950 (epoch 2), train_loss = 2.590, time/batch = 0.568\n",
            "777/18950 (epoch 2), train_loss = 2.607, time/batch = 0.574\n",
            "778/18950 (epoch 2), train_loss = 2.557, time/batch = 0.563\n",
            "779/18950 (epoch 2), train_loss = 2.578, time/batch = 0.576\n",
            "780/18950 (epoch 2), train_loss = 2.585, time/batch = 0.558\n",
            "781/18950 (epoch 2), train_loss = 2.563, time/batch = 0.574\n",
            "782/18950 (epoch 2), train_loss = 2.563, time/batch = 0.578\n",
            "783/18950 (epoch 2), train_loss = 2.593, time/batch = 0.569\n",
            "784/18950 (epoch 2), train_loss = 2.579, time/batch = 0.574\n",
            "785/18950 (epoch 2), train_loss = 2.544, time/batch = 0.561\n",
            "786/18950 (epoch 2), train_loss = 2.567, time/batch = 0.572\n",
            "787/18950 (epoch 2), train_loss = 2.605, time/batch = 0.575\n",
            "788/18950 (epoch 2), train_loss = 2.560, time/batch = 0.562\n",
            "789/18950 (epoch 2), train_loss = 2.581, time/batch = 0.561\n",
            "790/18950 (epoch 2), train_loss = 2.595, time/batch = 0.564\n",
            "791/18950 (epoch 2), train_loss = 2.577, time/batch = 0.579\n",
            "792/18950 (epoch 2), train_loss = 2.579, time/batch = 0.705\n",
            "793/18950 (epoch 2), train_loss = 2.581, time/batch = 0.739\n",
            "794/18950 (epoch 2), train_loss = 2.576, time/batch = 0.670\n",
            "795/18950 (epoch 2), train_loss = 2.609, time/batch = 0.717\n",
            "796/18950 (epoch 2), train_loss = 2.552, time/batch = 0.719\n",
            "797/18950 (epoch 2), train_loss = 2.561, time/batch = 0.727\n",
            "798/18950 (epoch 2), train_loss = 2.557, time/batch = 0.727\n",
            "799/18950 (epoch 2), train_loss = 2.538, time/batch = 0.741\n",
            "800/18950 (epoch 2), train_loss = 2.590, time/batch = 0.748\n",
            "801/18950 (epoch 2), train_loss = 2.576, time/batch = 0.733\n",
            "802/18950 (epoch 2), train_loss = 2.561, time/batch = 0.722\n",
            "803/18950 (epoch 2), train_loss = 2.570, time/batch = 0.727\n",
            "804/18950 (epoch 2), train_loss = 2.628, time/batch = 0.742\n",
            "805/18950 (epoch 2), train_loss = 2.556, time/batch = 0.722\n",
            "806/18950 (epoch 2), train_loss = 2.550, time/batch = 0.716\n",
            "807/18950 (epoch 2), train_loss = 2.552, time/batch = 0.753\n",
            "808/18950 (epoch 2), train_loss = 2.528, time/batch = 0.740\n",
            "809/18950 (epoch 2), train_loss = 2.530, time/batch = 0.707\n",
            "810/18950 (epoch 2), train_loss = 2.561, time/batch = 0.560\n",
            "811/18950 (epoch 2), train_loss = 2.546, time/batch = 0.572\n",
            "812/18950 (epoch 2), train_loss = 2.575, time/batch = 0.590\n",
            "813/18950 (epoch 2), train_loss = 2.604, time/batch = 0.564\n",
            "814/18950 (epoch 2), train_loss = 2.551, time/batch = 0.609\n",
            "815/18950 (epoch 2), train_loss = 2.571, time/batch = 0.574\n",
            "816/18950 (epoch 2), train_loss = 2.547, time/batch = 0.565\n",
            "817/18950 (epoch 2), train_loss = 2.539, time/batch = 0.587\n",
            "818/18950 (epoch 2), train_loss = 2.542, time/batch = 0.570\n",
            "819/18950 (epoch 2), train_loss = 2.560, time/batch = 0.589\n",
            "820/18950 (epoch 2), train_loss = 2.547, time/batch = 0.574\n",
            "821/18950 (epoch 2), train_loss = 2.559, time/batch = 0.590\n",
            "822/18950 (epoch 2), train_loss = 2.577, time/batch = 0.576\n",
            "823/18950 (epoch 2), train_loss = 2.578, time/batch = 0.570\n",
            "824/18950 (epoch 2), train_loss = 2.556, time/batch = 0.572\n",
            "825/18950 (epoch 2), train_loss = 2.538, time/batch = 0.600\n",
            "826/18950 (epoch 2), train_loss = 2.535, time/batch = 0.600\n",
            "827/18950 (epoch 2), train_loss = 2.556, time/batch = 0.746\n",
            "828/18950 (epoch 2), train_loss = 2.570, time/batch = 0.726\n",
            "829/18950 (epoch 2), train_loss = 2.567, time/batch = 0.729\n",
            "830/18950 (epoch 2), train_loss = 2.556, time/batch = 0.731\n",
            "831/18950 (epoch 2), train_loss = 2.580, time/batch = 0.768\n",
            "832/18950 (epoch 2), train_loss = 2.544, time/batch = 0.741\n",
            "833/18950 (epoch 2), train_loss = 2.534, time/batch = 0.744\n",
            "834/18950 (epoch 2), train_loss = 2.519, time/batch = 0.747\n",
            "835/18950 (epoch 2), train_loss = 2.526, time/batch = 0.743\n",
            "836/18950 (epoch 2), train_loss = 2.511, time/batch = 0.742\n",
            "837/18950 (epoch 2), train_loss = 2.529, time/batch = 0.733\n",
            "838/18950 (epoch 2), train_loss = 2.530, time/batch = 0.746\n",
            "839/18950 (epoch 2), train_loss = 2.556, time/batch = 0.745\n",
            "840/18950 (epoch 2), train_loss = 2.563, time/batch = 0.730\n",
            "841/18950 (epoch 2), train_loss = 2.580, time/batch = 0.742\n",
            "842/18950 (epoch 2), train_loss = 2.563, time/batch = 0.729\n",
            "843/18950 (epoch 2), train_loss = 2.556, time/batch = 0.728\n",
            "844/18950 (epoch 2), train_loss = 2.592, time/batch = 0.555\n",
            "845/18950 (epoch 2), train_loss = 2.565, time/batch = 0.560\n",
            "846/18950 (epoch 2), train_loss = 2.547, time/batch = 0.559\n",
            "847/18950 (epoch 2), train_loss = 2.543, time/batch = 0.578\n",
            "848/18950 (epoch 2), train_loss = 2.572, time/batch = 0.560\n",
            "849/18950 (epoch 2), train_loss = 2.542, time/batch = 0.581\n",
            "850/18950 (epoch 2), train_loss = 2.530, time/batch = 0.580\n",
            "851/18950 (epoch 2), train_loss = 2.552, time/batch = 0.588\n",
            "852/18950 (epoch 2), train_loss = 2.562, time/batch = 0.582\n",
            "853/18950 (epoch 2), train_loss = 2.528, time/batch = 0.564\n",
            "854/18950 (epoch 2), train_loss = 2.549, time/batch = 0.585\n",
            "855/18950 (epoch 2), train_loss = 2.556, time/batch = 0.576\n",
            "856/18950 (epoch 2), train_loss = 2.541, time/batch = 0.565\n",
            "857/18950 (epoch 2), train_loss = 2.543, time/batch = 0.574\n",
            "858/18950 (epoch 2), train_loss = 2.544, time/batch = 0.588\n",
            "859/18950 (epoch 2), train_loss = 2.567, time/batch = 0.563\n",
            "860/18950 (epoch 2), train_loss = 2.530, time/batch = 0.561\n",
            "861/18950 (epoch 2), train_loss = 2.543, time/batch = 0.672\n",
            "862/18950 (epoch 2), train_loss = 2.545, time/batch = 0.725\n",
            "863/18950 (epoch 2), train_loss = 2.511, time/batch = 0.719\n",
            "864/18950 (epoch 2), train_loss = 2.534, time/batch = 0.734\n",
            "865/18950 (epoch 2), train_loss = 2.517, time/batch = 0.741\n",
            "866/18950 (epoch 2), train_loss = 2.530, time/batch = 0.769\n",
            "867/18950 (epoch 2), train_loss = 2.537, time/batch = 0.778\n",
            "868/18950 (epoch 2), train_loss = 2.549, time/batch = 0.754\n",
            "869/18950 (epoch 2), train_loss = 2.543, time/batch = 0.759\n",
            "870/18950 (epoch 2), train_loss = 2.523, time/batch = 0.744\n",
            "871/18950 (epoch 2), train_loss = 2.521, time/batch = 0.715\n",
            "872/18950 (epoch 2), train_loss = 2.514, time/batch = 0.763\n",
            "873/18950 (epoch 2), train_loss = 2.520, time/batch = 0.748\n",
            "874/18950 (epoch 2), train_loss = 2.541, time/batch = 0.792\n",
            "875/18950 (epoch 2), train_loss = 2.527, time/batch = 0.745\n",
            "876/18950 (epoch 2), train_loss = 2.546, time/batch = 0.744\n",
            "877/18950 (epoch 2), train_loss = 2.529, time/batch = 0.743\n",
            "878/18950 (epoch 2), train_loss = 2.516, time/batch = 0.741\n",
            "879/18950 (epoch 2), train_loss = 2.519, time/batch = 0.578\n",
            "880/18950 (epoch 2), train_loss = 2.567, time/batch = 0.578\n",
            "881/18950 (epoch 2), train_loss = 2.526, time/batch = 0.591\n",
            "882/18950 (epoch 2), train_loss = 2.519, time/batch = 0.567\n",
            "883/18950 (epoch 2), train_loss = 2.490, time/batch = 0.576\n",
            "884/18950 (epoch 2), train_loss = 2.507, time/batch = 0.580\n",
            "885/18950 (epoch 2), train_loss = 2.490, time/batch = 0.570\n",
            "886/18950 (epoch 2), train_loss = 2.485, time/batch = 0.580\n",
            "887/18950 (epoch 2), train_loss = 2.553, time/batch = 0.572\n",
            "888/18950 (epoch 2), train_loss = 2.512, time/batch = 0.577\n",
            "889/18950 (epoch 2), train_loss = 2.495, time/batch = 0.573\n",
            "890/18950 (epoch 2), train_loss = 2.519, time/batch = 0.570\n",
            "891/18950 (epoch 2), train_loss = 2.489, time/batch = 0.567\n",
            "892/18950 (epoch 2), train_loss = 2.476, time/batch = 0.581\n",
            "893/18950 (epoch 2), train_loss = 2.485, time/batch = 0.582\n",
            "894/18950 (epoch 2), train_loss = 2.484, time/batch = 0.571\n",
            "895/18950 (epoch 2), train_loss = 2.487, time/batch = 0.569\n",
            "896/18950 (epoch 2), train_loss = 2.501, time/batch = 0.705\n",
            "897/18950 (epoch 2), train_loss = 2.469, time/batch = 0.752\n",
            "898/18950 (epoch 2), train_loss = 2.501, time/batch = 0.740\n",
            "899/18950 (epoch 2), train_loss = 2.513, time/batch = 0.756\n",
            "900/18950 (epoch 2), train_loss = 2.508, time/batch = 0.729\n",
            "901/18950 (epoch 2), train_loss = 2.507, time/batch = 0.755\n",
            "902/18950 (epoch 2), train_loss = 2.522, time/batch = 0.751\n",
            "903/18950 (epoch 2), train_loss = 2.519, time/batch = 0.733\n",
            "904/18950 (epoch 2), train_loss = 2.521, time/batch = 0.753\n",
            "905/18950 (epoch 2), train_loss = 2.501, time/batch = 0.798\n",
            "906/18950 (epoch 2), train_loss = 2.480, time/batch = 0.755\n",
            "907/18950 (epoch 2), train_loss = 2.519, time/batch = 0.771\n",
            "908/18950 (epoch 2), train_loss = 2.511, time/batch = 0.754\n",
            "909/18950 (epoch 2), train_loss = 2.486, time/batch = 0.768\n",
            "910/18950 (epoch 2), train_loss = 2.483, time/batch = 0.770\n",
            "911/18950 (epoch 2), train_loss = 2.503, time/batch = 0.746\n",
            "912/18950 (epoch 2), train_loss = 2.513, time/batch = 0.801\n",
            "913/18950 (epoch 2), train_loss = 2.527, time/batch = 0.637\n",
            "914/18950 (epoch 2), train_loss = 2.512, time/batch = 0.574\n",
            "915/18950 (epoch 2), train_loss = 2.508, time/batch = 0.567\n",
            "916/18950 (epoch 2), train_loss = 2.506, time/batch = 0.571\n",
            "917/18950 (epoch 2), train_loss = 2.493, time/batch = 0.587\n",
            "918/18950 (epoch 2), train_loss = 2.507, time/batch = 0.564\n",
            "919/18950 (epoch 2), train_loss = 2.461, time/batch = 0.575\n",
            "920/18950 (epoch 2), train_loss = 2.520, time/batch = 0.589\n",
            "921/18950 (epoch 2), train_loss = 2.474, time/batch = 0.574\n",
            "922/18950 (epoch 2), train_loss = 2.477, time/batch = 0.582\n",
            "923/18950 (epoch 2), train_loss = 2.451, time/batch = 0.567\n",
            "924/18950 (epoch 2), train_loss = 2.476, time/batch = 0.579\n",
            "925/18950 (epoch 2), train_loss = 2.506, time/batch = 0.584\n",
            "926/18950 (epoch 2), train_loss = 2.549, time/batch = 0.575\n",
            "927/18950 (epoch 2), train_loss = 2.508, time/batch = 0.590\n",
            "928/18950 (epoch 2), train_loss = 2.473, time/batch = 0.586\n",
            "929/18950 (epoch 2), train_loss = 2.463, time/batch = 0.582\n",
            "930/18950 (epoch 2), train_loss = 2.492, time/batch = 0.623\n",
            "931/18950 (epoch 2), train_loss = 2.464, time/batch = 0.774\n",
            "932/18950 (epoch 2), train_loss = 2.478, time/batch = 0.756\n",
            "933/18950 (epoch 2), train_loss = 2.457, time/batch = 0.729\n",
            "934/18950 (epoch 2), train_loss = 2.479, time/batch = 0.715\n",
            "935/18950 (epoch 2), train_loss = 2.488, time/batch = 0.731\n",
            "936/18950 (epoch 2), train_loss = 2.475, time/batch = 0.730\n",
            "937/18950 (epoch 2), train_loss = 2.483, time/batch = 0.743\n",
            "938/18950 (epoch 2), train_loss = 2.517, time/batch = 0.718\n",
            "939/18950 (epoch 2), train_loss = 2.484, time/batch = 0.742\n",
            "940/18950 (epoch 2), train_loss = 2.502, time/batch = 0.744\n",
            "941/18950 (epoch 2), train_loss = 2.475, time/batch = 0.737\n",
            "942/18950 (epoch 2), train_loss = 2.484, time/batch = 0.716\n",
            "943/18950 (epoch 2), train_loss = 2.476, time/batch = 0.729\n",
            "944/18950 (epoch 2), train_loss = 2.473, time/batch = 0.743\n",
            "945/18950 (epoch 2), train_loss = 2.484, time/batch = 0.719\n",
            "946/18950 (epoch 2), train_loss = 2.463, time/batch = 0.771\n",
            "947/18950 (epoch 2), train_loss = 2.506, time/batch = 0.717\n",
            "948/18950 (epoch 2), train_loss = 2.455, time/batch = 0.588\n",
            "949/18950 (epoch 2), train_loss = 2.481, time/batch = 0.579\n",
            "950/18950 (epoch 2), train_loss = 2.454, time/batch = 0.576\n",
            "951/18950 (epoch 2), train_loss = 2.466, time/batch = 0.587\n",
            "952/18950 (epoch 2), train_loss = 2.444, time/batch = 0.579\n",
            "953/18950 (epoch 2), train_loss = 2.423, time/batch = 0.602\n",
            "954/18950 (epoch 2), train_loss = 2.435, time/batch = 0.572\n",
            "955/18950 (epoch 2), train_loss = 2.443, time/batch = 0.573\n",
            "956/18950 (epoch 2), train_loss = 2.496, time/batch = 0.598\n",
            "957/18950 (epoch 2), train_loss = 2.486, time/batch = 0.587\n",
            "958/18950 (epoch 2), train_loss = 2.470, time/batch = 0.591\n",
            "959/18950 (epoch 2), train_loss = 2.454, time/batch = 0.615\n",
            "960/18950 (epoch 2), train_loss = 2.438, time/batch = 0.570\n",
            "961/18950 (epoch 2), train_loss = 2.423, time/batch = 0.573\n",
            "962/18950 (epoch 2), train_loss = 2.450, time/batch = 0.572\n",
            "963/18950 (epoch 2), train_loss = 2.462, time/batch = 0.606\n",
            "964/18950 (epoch 2), train_loss = 2.445, time/batch = 0.575\n",
            "965/18950 (epoch 2), train_loss = 2.452, time/batch = 0.730\n",
            "966/18950 (epoch 2), train_loss = 2.457, time/batch = 0.749\n",
            "967/18950 (epoch 2), train_loss = 2.472, time/batch = 0.751\n",
            "968/18950 (epoch 2), train_loss = 2.474, time/batch = 0.781\n",
            "969/18950 (epoch 2), train_loss = 2.463, time/batch = 0.733\n",
            "970/18950 (epoch 2), train_loss = 2.455, time/batch = 0.728\n",
            "971/18950 (epoch 2), train_loss = 2.438, time/batch = 0.769\n",
            "972/18950 (epoch 2), train_loss = 2.441, time/batch = 0.746\n",
            "973/18950 (epoch 2), train_loss = 2.453, time/batch = 0.797\n",
            "974/18950 (epoch 2), train_loss = 2.470, time/batch = 0.763\n",
            "975/18950 (epoch 2), train_loss = 2.437, time/batch = 0.764\n",
            "976/18950 (epoch 2), train_loss = 2.435, time/batch = 0.754\n",
            "977/18950 (epoch 2), train_loss = 2.428, time/batch = 0.758\n",
            "978/18950 (epoch 2), train_loss = 2.424, time/batch = 0.730\n",
            "979/18950 (epoch 2), train_loss = 2.423, time/batch = 0.714\n",
            "980/18950 (epoch 2), train_loss = 2.429, time/batch = 0.731\n",
            "981/18950 (epoch 2), train_loss = 2.464, time/batch = 0.607\n",
            "982/18950 (epoch 2), train_loss = 2.437, time/batch = 0.574\n",
            "983/18950 (epoch 2), train_loss = 2.464, time/batch = 0.581\n",
            "984/18950 (epoch 2), train_loss = 2.457, time/batch = 0.570\n",
            "985/18950 (epoch 2), train_loss = 2.459, time/batch = 0.587\n",
            "986/18950 (epoch 2), train_loss = 2.447, time/batch = 0.565\n",
            "987/18950 (epoch 2), train_loss = 2.473, time/batch = 0.568\n",
            "988/18950 (epoch 2), train_loss = 2.459, time/batch = 0.564\n",
            "989/18950 (epoch 2), train_loss = 2.473, time/batch = 0.580\n",
            "990/18950 (epoch 2), train_loss = 2.449, time/batch = 0.583\n",
            "991/18950 (epoch 2), train_loss = 2.424, time/batch = 0.565\n",
            "992/18950 (epoch 2), train_loss = 2.440, time/batch = 0.575\n",
            "993/18950 (epoch 2), train_loss = 2.421, time/batch = 0.596\n",
            "994/18950 (epoch 2), train_loss = 2.426, time/batch = 0.587\n",
            "995/18950 (epoch 2), train_loss = 2.424, time/batch = 0.593\n",
            "996/18950 (epoch 2), train_loss = 2.432, time/batch = 0.595\n",
            "997/18950 (epoch 2), train_loss = 2.424, time/batch = 0.583\n",
            "998/18950 (epoch 2), train_loss = 2.425, time/batch = 0.656\n",
            "999/18950 (epoch 2), train_loss = 2.443, time/batch = 0.746\n",
            "1000/18950 (epoch 2), train_loss = 2.465, time/batch = 0.748\n",
            "model saved to /content/drive/Shareddrives/Materials UNI/UNIPI/ISPR/Midterm3 Assignment_4/save/save1/model.ckpt\n",
            "1001/18950 (epoch 2), train_loss = 2.443, time/batch = 0.823\n",
            "1002/18950 (epoch 2), train_loss = 2.442, time/batch = 0.798\n",
            "1003/18950 (epoch 2), train_loss = 2.450, time/batch = 0.758\n",
            "1004/18950 (epoch 2), train_loss = 2.441, time/batch = 0.732\n",
            "1005/18950 (epoch 2), train_loss = 2.449, time/batch = 0.748\n",
            "1006/18950 (epoch 2), train_loss = 2.427, time/batch = 0.756\n",
            "1007/18950 (epoch 2), train_loss = 2.452, time/batch = 0.760\n",
            "1008/18950 (epoch 2), train_loss = 2.411, time/batch = 0.762\n",
            "1009/18950 (epoch 2), train_loss = 2.422, time/batch = 0.715\n",
            "1010/18950 (epoch 2), train_loss = 2.434, time/batch = 0.754\n",
            "1011/18950 (epoch 2), train_loss = 2.451, time/batch = 0.604\n",
            "1012/18950 (epoch 2), train_loss = 2.431, time/batch = 0.587\n",
            "1013/18950 (epoch 2), train_loss = 2.403, time/batch = 0.600\n",
            "1014/18950 (epoch 2), train_loss = 2.406, time/batch = 0.640\n",
            "1015/18950 (epoch 2), train_loss = 2.412, time/batch = 0.608\n",
            "1016/18950 (epoch 2), train_loss = 2.396, time/batch = 0.583\n",
            "1017/18950 (epoch 2), train_loss = 2.397, time/batch = 0.577\n",
            "1018/18950 (epoch 2), train_loss = 2.406, time/batch = 0.576\n",
            "1019/18950 (epoch 2), train_loss = 2.388, time/batch = 0.579\n",
            "1020/18950 (epoch 2), train_loss = 2.384, time/batch = 0.606\n",
            "1021/18950 (epoch 2), train_loss = 2.413, time/batch = 0.593\n",
            "1022/18950 (epoch 2), train_loss = 2.404, time/batch = 0.578\n",
            "1023/18950 (epoch 2), train_loss = 2.403, time/batch = 0.596\n",
            "1024/18950 (epoch 2), train_loss = 2.405, time/batch = 0.588\n",
            "1025/18950 (epoch 2), train_loss = 2.395, time/batch = 0.583\n",
            "1026/18950 (epoch 2), train_loss = 2.392, time/batch = 0.585\n",
            "1027/18950 (epoch 2), train_loss = 2.370, time/batch = 0.592\n",
            "1028/18950 (epoch 2), train_loss = 2.372, time/batch = 0.750\n",
            "1029/18950 (epoch 2), train_loss = 2.390, time/batch = 0.771\n",
            "1030/18950 (epoch 2), train_loss = 2.372, time/batch = 0.761\n",
            "1031/18950 (epoch 2), train_loss = 2.404, time/batch = 0.760\n",
            "1032/18950 (epoch 2), train_loss = 2.365, time/batch = 0.769\n",
            "1033/18950 (epoch 2), train_loss = 2.402, time/batch = 0.766\n",
            "1034/18950 (epoch 2), train_loss = 2.387, time/batch = 0.777\n",
            "1035/18950 (epoch 2), train_loss = 2.370, time/batch = 0.762\n",
            "1036/18950 (epoch 2), train_loss = 2.387, time/batch = 0.776\n",
            "1037/18950 (epoch 2), train_loss = 2.394, time/batch = 0.732\n",
            "1038/18950 (epoch 2), train_loss = 2.377, time/batch = 0.758\n",
            "1039/18950 (epoch 2), train_loss = 2.386, time/batch = 0.778\n",
            "1040/18950 (epoch 2), train_loss = 2.379, time/batch = 0.767\n",
            "1041/18950 (epoch 2), train_loss = 2.378, time/batch = 0.757\n",
            "1042/18950 (epoch 2), train_loss = 2.370, time/batch = 0.784\n",
            "1043/18950 (epoch 2), train_loss = 2.338, time/batch = 0.706\n",
            "1044/18950 (epoch 2), train_loss = 2.361, time/batch = 0.616\n",
            "1045/18950 (epoch 2), train_loss = 2.357, time/batch = 0.577\n",
            "1046/18950 (epoch 2), train_loss = 2.375, time/batch = 0.584\n",
            "1047/18950 (epoch 2), train_loss = 2.378, time/batch = 0.591\n",
            "1048/18950 (epoch 2), train_loss = 2.374, time/batch = 0.581\n",
            "1049/18950 (epoch 2), train_loss = 2.353, time/batch = 0.579\n",
            "1050/18950 (epoch 2), train_loss = 2.352, time/batch = 0.597\n",
            "1051/18950 (epoch 2), train_loss = 2.386, time/batch = 0.605\n",
            "1052/18950 (epoch 2), train_loss = 2.417, time/batch = 0.575\n",
            "1053/18950 (epoch 2), train_loss = 2.386, time/batch = 0.580\n",
            "1054/18950 (epoch 2), train_loss = 2.366, time/batch = 0.585\n",
            "1055/18950 (epoch 2), train_loss = 2.403, time/batch = 0.576\n",
            "1056/18950 (epoch 2), train_loss = 2.363, time/batch = 0.602\n",
            "1057/18950 (epoch 2), train_loss = 2.352, time/batch = 0.580\n",
            "1058/18950 (epoch 2), train_loss = 2.356, time/batch = 0.597\n",
            "1059/18950 (epoch 2), train_loss = 2.344, time/batch = 0.591\n",
            "1060/18950 (epoch 2), train_loss = 2.327, time/batch = 0.677\n",
            "1061/18950 (epoch 2), train_loss = 2.336, time/batch = 0.741\n",
            "1062/18950 (epoch 2), train_loss = 2.340, time/batch = 0.763\n",
            "1063/18950 (epoch 2), train_loss = 2.384, time/batch = 0.764\n",
            "1064/18950 (epoch 2), train_loss = 2.370, time/batch = 0.727\n",
            "1065/18950 (epoch 2), train_loss = 2.330, time/batch = 0.757\n",
            "1066/18950 (epoch 2), train_loss = 2.345, time/batch = 0.754\n",
            "1067/18950 (epoch 2), train_loss = 2.345, time/batch = 0.724\n",
            "1068/18950 (epoch 2), train_loss = 2.318, time/batch = 0.759\n",
            "1069/18950 (epoch 2), train_loss = 2.314, time/batch = 0.731\n",
            "1070/18950 (epoch 2), train_loss = 2.318, time/batch = 0.723\n",
            "1071/18950 (epoch 2), train_loss = 2.347, time/batch = 0.746\n",
            "1072/18950 (epoch 2), train_loss = 2.374, time/batch = 0.760\n",
            "1073/18950 (epoch 2), train_loss = 2.347, time/batch = 0.752\n",
            "1074/18950 (epoch 2), train_loss = 2.367, time/batch = 0.774\n",
            "1075/18950 (epoch 2), train_loss = 2.334, time/batch = 0.778\n",
            "1076/18950 (epoch 2), train_loss = 2.323, time/batch = 0.678\n",
            "1077/18950 (epoch 2), train_loss = 2.317, time/batch = 0.587\n",
            "1078/18950 (epoch 2), train_loss = 2.317, time/batch = 0.587\n",
            "1079/18950 (epoch 2), train_loss = 2.336, time/batch = 0.603\n",
            "1080/18950 (epoch 2), train_loss = 2.335, time/batch = 0.578\n",
            "1081/18950 (epoch 2), train_loss = 2.341, time/batch = 0.604\n",
            "1082/18950 (epoch 2), train_loss = 2.333, time/batch = 0.579\n",
            "1083/18950 (epoch 2), train_loss = 2.326, time/batch = 0.594\n",
            "1084/18950 (epoch 2), train_loss = 2.326, time/batch = 0.596\n",
            "1085/18950 (epoch 2), train_loss = 2.337, time/batch = 0.583\n",
            "1086/18950 (epoch 2), train_loss = 2.320, time/batch = 0.618\n",
            "1087/18950 (epoch 2), train_loss = 2.316, time/batch = 0.584\n",
            "1088/18950 (epoch 2), train_loss = 2.314, time/batch = 0.593\n",
            "1089/18950 (epoch 2), train_loss = 2.325, time/batch = 0.602\n",
            "1090/18950 (epoch 2), train_loss = 2.310, time/batch = 0.601\n",
            "1091/18950 (epoch 2), train_loss = 2.313, time/batch = 0.586\n",
            "1092/18950 (epoch 2), train_loss = 2.314, time/batch = 0.594\n",
            "1093/18950 (epoch 2), train_loss = 2.321, time/batch = 0.678\n",
            "1094/18950 (epoch 2), train_loss = 2.310, time/batch = 0.761\n",
            "1095/18950 (epoch 2), train_loss = 2.306, time/batch = 0.769\n",
            "1096/18950 (epoch 2), train_loss = 2.327, time/batch = 0.743\n",
            "1097/18950 (epoch 2), train_loss = 2.284, time/batch = 0.831\n",
            "1098/18950 (epoch 2), train_loss = 2.321, time/batch = 0.773\n",
            "1099/18950 (epoch 2), train_loss = 2.318, time/batch = 0.784\n",
            "1100/18950 (epoch 2), train_loss = 2.328, time/batch = 0.783\n",
            "1101/18950 (epoch 2), train_loss = 2.314, time/batch = 0.772\n",
            "1102/18950 (epoch 2), train_loss = 2.302, time/batch = 0.775\n",
            "1103/18950 (epoch 2), train_loss = 2.302, time/batch = 0.748\n",
            "1104/18950 (epoch 2), train_loss = 2.284, time/batch = 0.765\n",
            "1105/18950 (epoch 2), train_loss = 2.306, time/batch = 0.735\n",
            "1106/18950 (epoch 2), train_loss = 2.316, time/batch = 0.752\n",
            "1107/18950 (epoch 2), train_loss = 2.303, time/batch = 0.726\n",
            "1108/18950 (epoch 2), train_loss = 2.310, time/batch = 0.742\n",
            "1109/18950 (epoch 2), train_loss = 2.301, time/batch = 0.613\n",
            "1110/18950 (epoch 2), train_loss = 2.311, time/batch = 0.582\n",
            "1111/18950 (epoch 2), train_loss = 2.276, time/batch = 0.565\n",
            "1112/18950 (epoch 2), train_loss = 2.294, time/batch = 0.576\n",
            "1113/18950 (epoch 2), train_loss = 2.300, time/batch = 0.584\n",
            "1114/18950 (epoch 2), train_loss = 2.293, time/batch = 0.576\n",
            "1115/18950 (epoch 2), train_loss = 2.321, time/batch = 0.571\n",
            "1116/18950 (epoch 2), train_loss = 2.278, time/batch = 0.568\n",
            "1117/18950 (epoch 2), train_loss = 2.292, time/batch = 0.574\n",
            "1118/18950 (epoch 2), train_loss = 2.280, time/batch = 0.619\n",
            "1119/18950 (epoch 2), train_loss = 2.284, time/batch = 0.603\n",
            "1120/18950 (epoch 2), train_loss = 2.283, time/batch = 0.577\n",
            "1121/18950 (epoch 2), train_loss = 2.285, time/batch = 0.593\n",
            "1122/18950 (epoch 2), train_loss = 2.288, time/batch = 0.570\n",
            "1123/18950 (epoch 2), train_loss = 2.302, time/batch = 0.625\n",
            "1124/18950 (epoch 2), train_loss = 2.315, time/batch = 0.608\n",
            "1125/18950 (epoch 2), train_loss = 2.295, time/batch = 0.575\n",
            "1126/18950 (epoch 2), train_loss = 2.264, time/batch = 0.688\n",
            "1127/18950 (epoch 2), train_loss = 2.276, time/batch = 0.757\n",
            "1128/18950 (epoch 2), train_loss = 2.274, time/batch = 0.740\n",
            "1129/18950 (epoch 2), train_loss = 2.280, time/batch = 0.739\n",
            "1130/18950 (epoch 2), train_loss = 2.276, time/batch = 0.746\n",
            "1131/18950 (epoch 2), train_loss = 2.286, time/batch = 0.752\n",
            "1132/18950 (epoch 2), train_loss = 2.251, time/batch = 0.779\n",
            "1133/18950 (epoch 2), train_loss = 2.261, time/batch = 0.767\n",
            "1134/18950 (epoch 2), train_loss = 2.281, time/batch = 0.740\n",
            "1135/18950 (epoch 2), train_loss = 2.272, time/batch = 0.761\n",
            "1136/18950 (epoch 2), train_loss = 2.266, time/batch = 0.762\n",
            "1137/18950 (epoch 3), train_loss = 2.300, time/batch = 0.816\n",
            "1138/18950 (epoch 3), train_loss = 2.268, time/batch = 0.778\n",
            "1139/18950 (epoch 3), train_loss = 2.279, time/batch = 0.678\n",
            "1140/18950 (epoch 3), train_loss = 2.268, time/batch = 0.580\n",
            "1141/18950 (epoch 3), train_loss = 2.275, time/batch = 0.589\n",
            "1142/18950 (epoch 3), train_loss = 2.249, time/batch = 0.586\n",
            "1143/18950 (epoch 3), train_loss = 2.229, time/batch = 0.591\n",
            "1144/18950 (epoch 3), train_loss = 2.238, time/batch = 0.610\n",
            "1145/18950 (epoch 3), train_loss = 2.237, time/batch = 0.579\n",
            "1146/18950 (epoch 3), train_loss = 2.240, time/batch = 0.589\n",
            "1147/18950 (epoch 3), train_loss = 2.252, time/batch = 0.578\n",
            "1148/18950 (epoch 3), train_loss = 2.245, time/batch = 0.578\n",
            "1149/18950 (epoch 3), train_loss = 2.250, time/batch = 0.579\n",
            "1150/18950 (epoch 3), train_loss = 2.249, time/batch = 0.564\n",
            "1151/18950 (epoch 3), train_loss = 2.252, time/batch = 0.564\n",
            "1152/18950 (epoch 3), train_loss = 2.245, time/batch = 0.578\n",
            "1153/18950 (epoch 3), train_loss = 2.260, time/batch = 0.585\n",
            "1154/18950 (epoch 3), train_loss = 2.251, time/batch = 0.575\n",
            "1155/18950 (epoch 3), train_loss = 2.237, time/batch = 0.578\n",
            "1156/18950 (epoch 3), train_loss = 2.247, time/batch = 0.653\n",
            "1157/18950 (epoch 3), train_loss = 2.229, time/batch = 0.730\n",
            "1158/18950 (epoch 3), train_loss = 2.229, time/batch = 0.745\n",
            "1159/18950 (epoch 3), train_loss = 2.248, time/batch = 0.760\n",
            "1160/18950 (epoch 3), train_loss = 2.235, time/batch = 0.744\n",
            "1161/18950 (epoch 3), train_loss = 2.238, time/batch = 0.731\n",
            "1162/18950 (epoch 3), train_loss = 2.235, time/batch = 0.731\n",
            "1163/18950 (epoch 3), train_loss = 2.240, time/batch = 0.754\n",
            "1164/18950 (epoch 3), train_loss = 2.228, time/batch = 0.742\n",
            "1165/18950 (epoch 3), train_loss = 2.219, time/batch = 0.762\n",
            "1166/18950 (epoch 3), train_loss = 2.251, time/batch = 0.746\n",
            "1167/18950 (epoch 3), train_loss = 2.225, time/batch = 0.753\n",
            "1168/18950 (epoch 3), train_loss = 2.237, time/batch = 0.768\n",
            "1169/18950 (epoch 3), train_loss = 2.247, time/batch = 0.792\n",
            "1170/18950 (epoch 3), train_loss = 2.226, time/batch = 0.774\n",
            "1171/18950 (epoch 3), train_loss = 2.229, time/batch = 0.765\n",
            "1172/18950 (epoch 3), train_loss = 2.228, time/batch = 0.579\n",
            "1173/18950 (epoch 3), train_loss = 2.220, time/batch = 0.579\n",
            "1174/18950 (epoch 3), train_loss = 2.251, time/batch = 0.587\n",
            "1175/18950 (epoch 3), train_loss = 2.208, time/batch = 0.579\n",
            "1176/18950 (epoch 3), train_loss = 2.218, time/batch = 0.573\n",
            "1177/18950 (epoch 3), train_loss = 2.234, time/batch = 0.592\n",
            "1178/18950 (epoch 3), train_loss = 2.215, time/batch = 0.586\n",
            "1179/18950 (epoch 3), train_loss = 2.239, time/batch = 0.579\n",
            "1180/18950 (epoch 3), train_loss = 2.232, time/batch = 0.592\n",
            "1181/18950 (epoch 3), train_loss = 2.217, time/batch = 0.588\n",
            "1182/18950 (epoch 3), train_loss = 2.232, time/batch = 0.593\n",
            "1183/18950 (epoch 3), train_loss = 2.263, time/batch = 0.584\n",
            "1184/18950 (epoch 3), train_loss = 2.216, time/batch = 0.618\n",
            "1185/18950 (epoch 3), train_loss = 2.214, time/batch = 0.580\n",
            "1186/18950 (epoch 3), train_loss = 2.226, time/batch = 0.595\n",
            "1187/18950 (epoch 3), train_loss = 2.196, time/batch = 0.586\n",
            "1188/18950 (epoch 3), train_loss = 2.206, time/batch = 0.579\n",
            "1189/18950 (epoch 3), train_loss = 2.214, time/batch = 0.730\n",
            "1190/18950 (epoch 3), train_loss = 2.215, time/batch = 0.769\n",
            "1191/18950 (epoch 3), train_loss = 2.236, time/batch = 0.800\n",
            "1192/18950 (epoch 3), train_loss = 2.248, time/batch = 0.751\n",
            "1193/18950 (epoch 3), train_loss = 2.224, time/batch = 0.741\n",
            "1194/18950 (epoch 3), train_loss = 2.218, time/batch = 0.748\n",
            "1195/18950 (epoch 3), train_loss = 2.208, time/batch = 0.765\n",
            "1196/18950 (epoch 3), train_loss = 2.196, time/batch = 0.750\n",
            "1197/18950 (epoch 3), train_loss = 2.204, time/batch = 0.733\n",
            "1198/18950 (epoch 3), train_loss = 2.207, time/batch = 0.778\n",
            "1199/18950 (epoch 3), train_loss = 2.200, time/batch = 0.744\n",
            "1200/18950 (epoch 3), train_loss = 2.219, time/batch = 0.748\n",
            "1201/18950 (epoch 3), train_loss = 2.230, time/batch = 0.766\n",
            "1202/18950 (epoch 3), train_loss = 2.233, time/batch = 0.746\n",
            "1203/18950 (epoch 3), train_loss = 2.221, time/batch = 0.762\n",
            "1204/18950 (epoch 3), train_loss = 2.204, time/batch = 0.691\n",
            "1205/18950 (epoch 3), train_loss = 2.204, time/batch = 0.575\n",
            "1206/18950 (epoch 3), train_loss = 2.219, time/batch = 0.583\n",
            "1207/18950 (epoch 3), train_loss = 2.238, time/batch = 0.577\n",
            "1208/18950 (epoch 3), train_loss = 2.217, time/batch = 0.573\n",
            "1209/18950 (epoch 3), train_loss = 2.233, time/batch = 0.574\n",
            "1210/18950 (epoch 3), train_loss = 2.229, time/batch = 0.572\n",
            "1211/18950 (epoch 3), train_loss = 2.198, time/batch = 0.593\n",
            "1212/18950 (epoch 3), train_loss = 2.201, time/batch = 0.578\n",
            "1213/18950 (epoch 3), train_loss = 2.189, time/batch = 0.585\n",
            "1214/18950 (epoch 3), train_loss = 2.192, time/batch = 0.615\n",
            "1215/18950 (epoch 3), train_loss = 2.192, time/batch = 0.576\n",
            "1216/18950 (epoch 3), train_loss = 2.206, time/batch = 0.586\n",
            "1217/18950 (epoch 3), train_loss = 2.194, time/batch = 0.573\n",
            "1218/18950 (epoch 3), train_loss = 2.217, time/batch = 0.581\n",
            "1219/18950 (epoch 3), train_loss = 2.215, time/batch = 0.591\n",
            "1220/18950 (epoch 3), train_loss = 2.228, time/batch = 0.589\n",
            "1221/18950 (epoch 3), train_loss = 2.217, time/batch = 0.598\n",
            "1222/18950 (epoch 3), train_loss = 2.209, time/batch = 0.705\n",
            "1223/18950 (epoch 3), train_loss = 2.241, time/batch = 0.755\n",
            "1224/18950 (epoch 3), train_loss = 2.220, time/batch = 0.776\n",
            "1225/18950 (epoch 3), train_loss = 2.195, time/batch = 0.719\n",
            "1226/18950 (epoch 3), train_loss = 2.187, time/batch = 0.715\n",
            "1227/18950 (epoch 3), train_loss = 2.223, time/batch = 0.771\n",
            "1228/18950 (epoch 3), train_loss = 2.210, time/batch = 0.748\n",
            "1229/18950 (epoch 3), train_loss = 2.192, time/batch = 0.751\n",
            "1230/18950 (epoch 3), train_loss = 2.213, time/batch = 0.813\n",
            "1231/18950 (epoch 3), train_loss = 2.217, time/batch = 0.786\n",
            "1232/18950 (epoch 3), train_loss = 2.197, time/batch = 0.757\n",
            "1233/18950 (epoch 3), train_loss = 2.201, time/batch = 0.742\n",
            "1234/18950 (epoch 3), train_loss = 2.219, time/batch = 0.739\n",
            "1235/18950 (epoch 3), train_loss = 2.205, time/batch = 0.765\n",
            "1236/18950 (epoch 3), train_loss = 2.195, time/batch = 0.771\n",
            "1237/18950 (epoch 3), train_loss = 2.213, time/batch = 0.744\n",
            "1238/18950 (epoch 3), train_loss = 2.219, time/batch = 0.581\n",
            "1239/18950 (epoch 3), train_loss = 2.202, time/batch = 0.576\n",
            "1240/18950 (epoch 3), train_loss = 2.197, time/batch = 0.588\n",
            "1241/18950 (epoch 3), train_loss = 2.207, time/batch = 0.572\n",
            "1242/18950 (epoch 3), train_loss = 2.188, time/batch = 0.595\n",
            "1243/18950 (epoch 3), train_loss = 2.191, time/batch = 0.568\n",
            "1244/18950 (epoch 3), train_loss = 2.186, time/batch = 0.598\n",
            "1245/18950 (epoch 3), train_loss = 2.194, time/batch = 0.592\n",
            "1246/18950 (epoch 3), train_loss = 2.192, time/batch = 0.583\n",
            "1247/18950 (epoch 3), train_loss = 2.215, time/batch = 0.578\n",
            "1248/18950 (epoch 3), train_loss = 2.216, time/batch = 0.578\n",
            "1249/18950 (epoch 3), train_loss = 2.182, time/batch = 0.587\n",
            "1250/18950 (epoch 3), train_loss = 2.182, time/batch = 0.576\n",
            "1251/18950 (epoch 3), train_loss = 2.185, time/batch = 0.574\n",
            "1252/18950 (epoch 3), train_loss = 2.190, time/batch = 0.577\n",
            "1253/18950 (epoch 3), train_loss = 2.203, time/batch = 0.588\n",
            "1254/18950 (epoch 3), train_loss = 2.192, time/batch = 0.601\n",
            "1255/18950 (epoch 3), train_loss = 2.198, time/batch = 0.713\n",
            "1256/18950 (epoch 3), train_loss = 2.194, time/batch = 0.761\n",
            "1257/18950 (epoch 3), train_loss = 2.171, time/batch = 0.757\n",
            "1258/18950 (epoch 3), train_loss = 2.172, time/batch = 0.763\n",
            "1259/18950 (epoch 3), train_loss = 2.215, time/batch = 0.780\n",
            "1260/18950 (epoch 3), train_loss = 2.188, time/batch = 0.748\n",
            "1261/18950 (epoch 3), train_loss = 2.186, time/batch = 0.762\n",
            "1262/18950 (epoch 3), train_loss = 2.170, time/batch = 0.776\n",
            "1263/18950 (epoch 3), train_loss = 2.182, time/batch = 0.748\n",
            "1264/18950 (epoch 3), train_loss = 2.166, time/batch = 0.783\n",
            "1265/18950 (epoch 3), train_loss = 2.156, time/batch = 0.794\n",
            "1266/18950 (epoch 3), train_loss = 2.194, time/batch = 0.777\n",
            "1267/18950 (epoch 3), train_loss = 2.173, time/batch = 0.760\n",
            "1268/18950 (epoch 3), train_loss = 2.164, time/batch = 0.775\n",
            "1269/18950 (epoch 3), train_loss = 2.171, time/batch = 0.785\n",
            "1270/18950 (epoch 3), train_loss = 2.157, time/batch = 0.696\n",
            "1271/18950 (epoch 3), train_loss = 2.137, time/batch = 0.577\n",
            "1272/18950 (epoch 3), train_loss = 2.155, time/batch = 0.589\n",
            "1273/18950 (epoch 3), train_loss = 2.152, time/batch = 0.599\n",
            "1274/18950 (epoch 3), train_loss = 2.158, time/batch = 0.585\n",
            "1275/18950 (epoch 3), train_loss = 2.157, time/batch = 0.587\n",
            "1276/18950 (epoch 3), train_loss = 2.146, time/batch = 0.637\n",
            "1277/18950 (epoch 3), train_loss = 2.173, time/batch = 0.589\n",
            "1278/18950 (epoch 3), train_loss = 2.179, time/batch = 0.596\n",
            "1279/18950 (epoch 3), train_loss = 2.167, time/batch = 0.580\n",
            "1280/18950 (epoch 3), train_loss = 2.178, time/batch = 0.581\n",
            "1281/18950 (epoch 3), train_loss = 2.188, time/batch = 0.598\n",
            "1282/18950 (epoch 3), train_loss = 2.168, time/batch = 0.575\n",
            "1283/18950 (epoch 3), train_loss = 2.188, time/batch = 0.584\n",
            "1284/18950 (epoch 3), train_loss = 2.177, time/batch = 0.580\n",
            "1285/18950 (epoch 3), train_loss = 2.152, time/batch = 0.579\n",
            "1286/18950 (epoch 3), train_loss = 2.178, time/batch = 0.586\n",
            "1287/18950 (epoch 3), train_loss = 2.188, time/batch = 0.613\n",
            "1288/18950 (epoch 3), train_loss = 2.154, time/batch = 0.765\n",
            "1289/18950 (epoch 3), train_loss = 2.157, time/batch = 0.754\n",
            "1290/18950 (epoch 3), train_loss = 2.173, time/batch = 0.761\n",
            "1291/18950 (epoch 3), train_loss = 2.173, time/batch = 0.756\n",
            "1292/18950 (epoch 3), train_loss = 2.190, time/batch = 0.739\n",
            "1293/18950 (epoch 3), train_loss = 2.169, time/batch = 0.748\n",
            "1294/18950 (epoch 3), train_loss = 2.166, time/batch = 0.740\n",
            "1295/18950 (epoch 3), train_loss = 2.156, time/batch = 0.781\n",
            "1296/18950 (epoch 3), train_loss = 2.157, time/batch = 0.731\n",
            "1297/18950 (epoch 3), train_loss = 2.168, time/batch = 0.747\n",
            "1298/18950 (epoch 3), train_loss = 2.140, time/batch = 0.747\n",
            "1299/18950 (epoch 3), train_loss = 2.174, time/batch = 0.724\n",
            "1300/18950 (epoch 3), train_loss = 2.144, time/batch = 0.743\n",
            "1301/18950 (epoch 3), train_loss = 2.146, time/batch = 0.768\n",
            "1302/18950 (epoch 3), train_loss = 2.119, time/batch = 0.747\n",
            "1303/18950 (epoch 3), train_loss = 2.153, time/batch = 0.693\n",
            "1304/18950 (epoch 3), train_loss = 2.168, time/batch = 0.568\n",
            "1305/18950 (epoch 3), train_loss = 2.191, time/batch = 0.589\n",
            "1306/18950 (epoch 3), train_loss = 2.165, time/batch = 0.574\n",
            "1307/18950 (epoch 3), train_loss = 2.157, time/batch = 0.599\n",
            "1308/18950 (epoch 3), train_loss = 2.137, time/batch = 0.595\n",
            "1309/18950 (epoch 3), train_loss = 2.165, time/batch = 0.585\n",
            "1310/18950 (epoch 3), train_loss = 2.142, time/batch = 0.614\n",
            "1311/18950 (epoch 3), train_loss = 2.140, time/batch = 0.581\n",
            "1312/18950 (epoch 3), train_loss = 2.122, time/batch = 0.572\n",
            "1313/18950 (epoch 3), train_loss = 2.148, time/batch = 0.578\n",
            "1314/18950 (epoch 3), train_loss = 2.177, time/batch = 0.580\n",
            "1315/18950 (epoch 3), train_loss = 2.144, time/batch = 0.580\n",
            "1316/18950 (epoch 3), train_loss = 2.155, time/batch = 0.581\n",
            "1317/18950 (epoch 3), train_loss = 2.172, time/batch = 0.585\n",
            "1318/18950 (epoch 3), train_loss = 2.159, time/batch = 0.606\n",
            "1319/18950 (epoch 3), train_loss = 2.165, time/batch = 0.625\n",
            "1320/18950 (epoch 3), train_loss = 2.143, time/batch = 0.644\n",
            "1321/18950 (epoch 3), train_loss = 2.159, time/batch = 0.777\n",
            "1322/18950 (epoch 3), train_loss = 2.150, time/batch = 0.785\n",
            "1323/18950 (epoch 3), train_loss = 2.144, time/batch = 0.770\n",
            "1324/18950 (epoch 3), train_loss = 2.157, time/batch = 0.755\n",
            "1325/18950 (epoch 3), train_loss = 2.140, time/batch = 0.753\n",
            "1326/18950 (epoch 3), train_loss = 2.169, time/batch = 0.737\n",
            "1327/18950 (epoch 3), train_loss = 2.138, time/batch = 0.768\n",
            "1328/18950 (epoch 3), train_loss = 2.165, time/batch = 0.761\n",
            "1329/18950 (epoch 3), train_loss = 2.142, time/batch = 0.763\n",
            "1330/18950 (epoch 3), train_loss = 2.149, time/batch = 0.756\n",
            "1331/18950 (epoch 3), train_loss = 2.120, time/batch = 0.749\n",
            "1332/18950 (epoch 3), train_loss = 2.114, time/batch = 0.740\n",
            "1333/18950 (epoch 3), train_loss = 2.124, time/batch = 0.755\n",
            "1334/18950 (epoch 3), train_loss = 2.134, time/batch = 0.759\n",
            "1335/18950 (epoch 3), train_loss = 2.150, time/batch = 0.751\n",
            "1336/18950 (epoch 3), train_loss = 2.163, time/batch = 0.664\n",
            "1337/18950 (epoch 3), train_loss = 2.159, time/batch = 0.583\n",
            "1338/18950 (epoch 3), train_loss = 2.151, time/batch = 0.604\n",
            "1339/18950 (epoch 3), train_loss = 2.126, time/batch = 0.591\n",
            "1340/18950 (epoch 3), train_loss = 2.110, time/batch = 0.582\n",
            "1341/18950 (epoch 3), train_loss = 2.135, time/batch = 0.577\n",
            "1342/18950 (epoch 3), train_loss = 2.139, time/batch = 0.591\n",
            "1343/18950 (epoch 3), train_loss = 2.136, time/batch = 0.575\n",
            "1344/18950 (epoch 3), train_loss = 2.142, time/batch = 0.576\n",
            "1345/18950 (epoch 3), train_loss = 2.148, time/batch = 0.590\n",
            "1346/18950 (epoch 3), train_loss = 2.163, time/batch = 0.572\n",
            "1347/18950 (epoch 3), train_loss = 2.157, time/batch = 0.576\n",
            "1348/18950 (epoch 3), train_loss = 2.150, time/batch = 0.571\n",
            "1349/18950 (epoch 3), train_loss = 2.139, time/batch = 0.598\n",
            "1350/18950 (epoch 3), train_loss = 2.143, time/batch = 0.573\n",
            "1351/18950 (epoch 3), train_loss = 2.132, time/batch = 0.588\n",
            "1352/18950 (epoch 3), train_loss = 2.144, time/batch = 0.595\n",
            "1353/18950 (epoch 3), train_loss = 2.156, time/batch = 0.629\n",
            "1354/18950 (epoch 3), train_loss = 2.134, time/batch = 0.767\n",
            "1355/18950 (epoch 3), train_loss = 2.128, time/batch = 0.800\n",
            "1356/18950 (epoch 3), train_loss = 2.121, time/batch = 0.728\n",
            "1357/18950 (epoch 3), train_loss = 2.126, time/batch = 0.786\n",
            "1358/18950 (epoch 3), train_loss = 2.133, time/batch = 0.805\n",
            "1359/18950 (epoch 3), train_loss = 2.124, time/batch = 0.748\n",
            "1360/18950 (epoch 3), train_loss = 2.141, time/batch = 0.777\n",
            "1361/18950 (epoch 3), train_loss = 2.122, time/batch = 0.755\n",
            "1362/18950 (epoch 3), train_loss = 2.138, time/batch = 0.774\n",
            "1363/18950 (epoch 3), train_loss = 2.141, time/batch = 0.811\n",
            "1364/18950 (epoch 3), train_loss = 2.153, time/batch = 0.752\n",
            "1365/18950 (epoch 3), train_loss = 2.136, time/batch = 0.786\n",
            "1366/18950 (epoch 3), train_loss = 2.145, time/batch = 0.775\n",
            "1367/18950 (epoch 3), train_loss = 2.144, time/batch = 0.769\n",
            "1368/18950 (epoch 3), train_loss = 2.153, time/batch = 0.773\n",
            "1369/18950 (epoch 3), train_loss = 2.131, time/batch = 0.670\n",
            "1370/18950 (epoch 3), train_loss = 2.132, time/batch = 0.605\n",
            "1371/18950 (epoch 3), train_loss = 2.141, time/batch = 0.586\n",
            "1372/18950 (epoch 3), train_loss = 2.125, time/batch = 0.603\n",
            "1373/18950 (epoch 3), train_loss = 2.130, time/batch = 0.582\n",
            "1374/18950 (epoch 3), train_loss = 2.123, time/batch = 0.581\n",
            "1375/18950 (epoch 3), train_loss = 2.127, time/batch = 0.593\n",
            "1376/18950 (epoch 3), train_loss = 2.117, time/batch = 0.593\n",
            "1377/18950 (epoch 3), train_loss = 2.111, time/batch = 0.599\n",
            "1378/18950 (epoch 3), train_loss = 2.137, time/batch = 0.582\n",
            "1379/18950 (epoch 3), train_loss = 2.149, time/batch = 0.589\n",
            "1380/18950 (epoch 3), train_loss = 2.135, time/batch = 0.584\n",
            "1381/18950 (epoch 3), train_loss = 2.138, time/batch = 0.585\n",
            "1382/18950 (epoch 3), train_loss = 2.144, time/batch = 0.578\n",
            "1383/18950 (epoch 3), train_loss = 2.127, time/batch = 0.580\n",
            "1384/18950 (epoch 3), train_loss = 2.139, time/batch = 0.585\n",
            "1385/18950 (epoch 3), train_loss = 2.128, time/batch = 0.598\n",
            "1386/18950 (epoch 3), train_loss = 2.160, time/batch = 0.686\n",
            "1387/18950 (epoch 3), train_loss = 2.103, time/batch = 0.765\n",
            "1388/18950 (epoch 3), train_loss = 2.126, time/batch = 0.807\n",
            "1389/18950 (epoch 3), train_loss = 2.124, time/batch = 0.737\n",
            "1390/18950 (epoch 3), train_loss = 2.140, time/batch = 0.754\n",
            "1391/18950 (epoch 3), train_loss = 2.118, time/batch = 0.753\n",
            "1392/18950 (epoch 3), train_loss = 2.113, time/batch = 0.778\n",
            "1393/18950 (epoch 3), train_loss = 2.129, time/batch = 0.760\n",
            "1394/18950 (epoch 3), train_loss = 2.135, time/batch = 0.755\n",
            "1395/18950 (epoch 3), train_loss = 2.108, time/batch = 0.744\n",
            "1396/18950 (epoch 3), train_loss = 2.094, time/batch = 0.750\n",
            "1397/18950 (epoch 3), train_loss = 2.119, time/batch = 0.739\n",
            "1398/18950 (epoch 3), train_loss = 2.110, time/batch = 0.765\n",
            "1399/18950 (epoch 3), train_loss = 2.093, time/batch = 0.765\n",
            "1400/18950 (epoch 3), train_loss = 2.108, time/batch = 0.777\n",
            "1401/18950 (epoch 3), train_loss = 2.113, time/batch = 0.734\n",
            "1402/18950 (epoch 3), train_loss = 2.108, time/batch = 0.588\n",
            "1403/18950 (epoch 3), train_loss = 2.114, time/batch = 0.584\n",
            "1404/18950 (epoch 3), train_loss = 2.110, time/batch = 0.594\n",
            "1405/18950 (epoch 3), train_loss = 2.117, time/batch = 0.595\n",
            "1406/18950 (epoch 3), train_loss = 2.092, time/batch = 0.610\n",
            "1407/18950 (epoch 3), train_loss = 2.081, time/batch = 0.595\n",
            "1408/18950 (epoch 3), train_loss = 2.109, time/batch = 0.591\n",
            "1409/18950 (epoch 3), train_loss = 2.090, time/batch = 0.582\n",
            "1410/18950 (epoch 3), train_loss = 2.100, time/batch = 0.604\n",
            "1411/18950 (epoch 3), train_loss = 2.094, time/batch = 0.615\n",
            "1412/18950 (epoch 3), train_loss = 2.117, time/batch = 0.621\n",
            "1413/18950 (epoch 3), train_loss = 2.103, time/batch = 0.631\n",
            "1414/18950 (epoch 3), train_loss = 2.100, time/batch = 0.606\n",
            "1415/18950 (epoch 3), train_loss = 2.129, time/batch = 0.603\n",
            "1416/18950 (epoch 3), train_loss = 2.130, time/batch = 0.613\n",
            "1417/18950 (epoch 3), train_loss = 2.121, time/batch = 0.585\n",
            "1418/18950 (epoch 3), train_loss = 2.108, time/batch = 0.624\n",
            "1419/18950 (epoch 3), train_loss = 2.099, time/batch = 0.764\n",
            "1420/18950 (epoch 3), train_loss = 2.115, time/batch = 0.826\n",
            "1421/18950 (epoch 3), train_loss = 2.093, time/batch = 0.780\n",
            "1422/18950 (epoch 3), train_loss = 2.067, time/batch = 0.721\n",
            "1423/18950 (epoch 3), train_loss = 2.097, time/batch = 0.755\n",
            "1424/18950 (epoch 3), train_loss = 2.102, time/batch = 0.777\n",
            "1425/18950 (epoch 3), train_loss = 2.107, time/batch = 0.770\n",
            "1426/18950 (epoch 3), train_loss = 2.116, time/batch = 0.737\n",
            "1427/18950 (epoch 3), train_loss = 2.100, time/batch = 0.781\n",
            "1428/18950 (epoch 3), train_loss = 2.081, time/batch = 0.760\n",
            "1429/18950 (epoch 3), train_loss = 2.087, time/batch = 0.775\n",
            "1430/18950 (epoch 3), train_loss = 2.128, time/batch = 0.751\n",
            "1431/18950 (epoch 3), train_loss = 2.154, time/batch = 0.776\n",
            "1432/18950 (epoch 3), train_loss = 2.122, time/batch = 0.755\n",
            "1433/18950 (epoch 3), train_loss = 2.105, time/batch = 0.774\n",
            "1434/18950 (epoch 3), train_loss = 2.112, time/batch = 0.669\n",
            "1435/18950 (epoch 3), train_loss = 2.090, time/batch = 0.583\n",
            "1436/18950 (epoch 3), train_loss = 2.086, time/batch = 0.586\n",
            "1437/18950 (epoch 3), train_loss = 2.091, time/batch = 0.594\n",
            "1438/18950 (epoch 3), train_loss = 2.088, time/batch = 0.582\n",
            "1439/18950 (epoch 3), train_loss = 2.073, time/batch = 0.586\n",
            "1440/18950 (epoch 3), train_loss = 2.074, time/batch = 0.577\n",
            "1441/18950 (epoch 3), train_loss = 2.092, time/batch = 0.577\n",
            "1442/18950 (epoch 3), train_loss = 2.133, time/batch = 0.572\n",
            "1443/18950 (epoch 3), train_loss = 2.109, time/batch = 0.617\n",
            "1444/18950 (epoch 3), train_loss = 2.075, time/batch = 0.587\n",
            "1445/18950 (epoch 3), train_loss = 2.097, time/batch = 0.580\n",
            "1446/18950 (epoch 3), train_loss = 2.102, time/batch = 0.587\n",
            "1447/18950 (epoch 3), train_loss = 2.076, time/batch = 0.581\n",
            "1448/18950 (epoch 3), train_loss = 2.066, time/batch = 0.586\n",
            "1449/18950 (epoch 3), train_loss = 2.068, time/batch = 0.577\n",
            "1450/18950 (epoch 3), train_loss = 2.088, time/batch = 0.596\n",
            "1451/18950 (epoch 3), train_loss = 2.120, time/batch = 0.672\n",
            "1452/18950 (epoch 3), train_loss = 2.080, time/batch = 0.755\n",
            "1453/18950 (epoch 3), train_loss = 2.121, time/batch = 0.771\n",
            "1454/18950 (epoch 3), train_loss = 2.085, time/batch = 0.763\n",
            "1455/18950 (epoch 3), train_loss = 2.074, time/batch = 0.768\n",
            "1456/18950 (epoch 3), train_loss = 2.074, time/batch = 0.765\n",
            "1457/18950 (epoch 3), train_loss = 2.066, time/batch = 0.754\n",
            "1458/18950 (epoch 3), train_loss = 2.077, time/batch = 0.807\n",
            "1459/18950 (epoch 3), train_loss = 2.079, time/batch = 0.781\n",
            "1460/18950 (epoch 3), train_loss = 2.100, time/batch = 0.758\n",
            "1461/18950 (epoch 3), train_loss = 2.089, time/batch = 0.736\n",
            "1462/18950 (epoch 3), train_loss = 2.074, time/batch = 0.760\n",
            "1463/18950 (epoch 3), train_loss = 2.078, time/batch = 0.766\n",
            "1464/18950 (epoch 3), train_loss = 2.092, time/batch = 0.746\n",
            "1465/18950 (epoch 3), train_loss = 2.074, time/batch = 0.751\n",
            "1466/18950 (epoch 3), train_loss = 2.074, time/batch = 0.750\n",
            "1467/18950 (epoch 3), train_loss = 2.064, time/batch = 0.584\n",
            "1468/18950 (epoch 3), train_loss = 2.066, time/batch = 0.591\n",
            "1469/18950 (epoch 3), train_loss = 2.065, time/batch = 0.587\n",
            "1470/18950 (epoch 3), train_loss = 2.071, time/batch = 0.577\n",
            "1471/18950 (epoch 3), train_loss = 2.071, time/batch = 0.582\n",
            "1472/18950 (epoch 3), train_loss = 2.080, time/batch = 0.598\n",
            "1473/18950 (epoch 3), train_loss = 2.071, time/batch = 0.577\n",
            "1474/18950 (epoch 3), train_loss = 2.069, time/batch = 0.591\n",
            "1475/18950 (epoch 3), train_loss = 2.095, time/batch = 0.581\n",
            "1476/18950 (epoch 3), train_loss = 2.064, time/batch = 0.580\n",
            "1477/18950 (epoch 3), train_loss = 2.084, time/batch = 0.603\n",
            "1478/18950 (epoch 3), train_loss = 2.066, time/batch = 0.622\n",
            "1479/18950 (epoch 3), train_loss = 2.083, time/batch = 0.577\n",
            "1480/18950 (epoch 3), train_loss = 2.081, time/batch = 0.585\n",
            "1481/18950 (epoch 3), train_loss = 2.073, time/batch = 0.583\n",
            "1482/18950 (epoch 3), train_loss = 2.072, time/batch = 0.577\n",
            "1483/18950 (epoch 3), train_loss = 2.050, time/batch = 0.599\n",
            "1484/18950 (epoch 3), train_loss = 2.072, time/batch = 0.736\n",
            "1485/18950 (epoch 3), train_loss = 2.076, time/batch = 0.757\n",
            "1486/18950 (epoch 3), train_loss = 2.065, time/batch = 0.764\n",
            "1487/18950 (epoch 3), train_loss = 2.068, time/batch = 0.752\n",
            "1488/18950 (epoch 3), train_loss = 2.076, time/batch = 0.788\n",
            "1489/18950 (epoch 3), train_loss = 2.083, time/batch = 0.770\n",
            "1490/18950 (epoch 3), train_loss = 2.050, time/batch = 0.757\n",
            "1491/18950 (epoch 3), train_loss = 2.068, time/batch = 0.756\n",
            "1492/18950 (epoch 3), train_loss = 2.065, time/batch = 0.766\n",
            "1493/18950 (epoch 3), train_loss = 2.072, time/batch = 0.764\n",
            "1494/18950 (epoch 3), train_loss = 2.102, time/batch = 0.750\n",
            "1495/18950 (epoch 3), train_loss = 2.058, time/batch = 0.750\n",
            "1496/18950 (epoch 3), train_loss = 2.075, time/batch = 0.784\n",
            "1497/18950 (epoch 3), train_loss = 2.061, time/batch = 0.762\n",
            "1498/18950 (epoch 3), train_loss = 2.070, time/batch = 0.766\n",
            "1499/18950 (epoch 3), train_loss = 2.071, time/batch = 0.708\n",
            "1500/18950 (epoch 3), train_loss = 2.078, time/batch = 0.605\n",
            "1501/18950 (epoch 3), train_loss = 2.064, time/batch = 0.575\n",
            "1502/18950 (epoch 3), train_loss = 2.087, time/batch = 0.605\n",
            "1503/18950 (epoch 3), train_loss = 2.103, time/batch = 0.576\n",
            "1504/18950 (epoch 3), train_loss = 2.083, time/batch = 0.588\n",
            "1505/18950 (epoch 3), train_loss = 2.063, time/batch = 0.580\n",
            "1506/18950 (epoch 3), train_loss = 2.061, time/batch = 0.577\n",
            "1507/18950 (epoch 3), train_loss = 2.052, time/batch = 0.586\n",
            "1508/18950 (epoch 3), train_loss = 2.060, time/batch = 0.612\n",
            "1509/18950 (epoch 3), train_loss = 2.058, time/batch = 0.588\n",
            "1510/18950 (epoch 3), train_loss = 2.079, time/batch = 0.608\n",
            "1511/18950 (epoch 3), train_loss = 2.034, time/batch = 0.595\n",
            "1512/18950 (epoch 3), train_loss = 2.060, time/batch = 0.602\n",
            "1513/18950 (epoch 3), train_loss = 2.068, time/batch = 0.581\n",
            "1514/18950 (epoch 3), train_loss = 2.064, time/batch = 0.581\n",
            "1515/18950 (epoch 3), train_loss = 2.072, time/batch = 0.590\n",
            "1516/18950 (epoch 4), train_loss = 2.098, time/batch = 0.838\n",
            "1517/18950 (epoch 4), train_loss = 2.065, time/batch = 0.768\n",
            "1518/18950 (epoch 4), train_loss = 2.076, time/batch = 0.754\n",
            "1519/18950 (epoch 4), train_loss = 2.069, time/batch = 0.770\n",
            "1520/18950 (epoch 4), train_loss = 2.079, time/batch = 0.752\n",
            "1521/18950 (epoch 4), train_loss = 2.041, time/batch = 0.758\n",
            "1522/18950 (epoch 4), train_loss = 2.031, time/batch = 0.752\n",
            "1523/18950 (epoch 4), train_loss = 2.042, time/batch = 0.728\n",
            "1524/18950 (epoch 4), train_loss = 2.038, time/batch = 0.783\n",
            "1525/18950 (epoch 4), train_loss = 2.042, time/batch = 0.754\n",
            "1526/18950 (epoch 4), train_loss = 2.045, time/batch = 0.744\n",
            "1527/18950 (epoch 4), train_loss = 2.048, time/batch = 0.760\n",
            "1528/18950 (epoch 4), train_loss = 2.062, time/batch = 0.741\n",
            "1529/18950 (epoch 4), train_loss = 2.041, time/batch = 0.694\n",
            "1530/18950 (epoch 4), train_loss = 2.050, time/batch = 0.593\n",
            "1531/18950 (epoch 4), train_loss = 2.050, time/batch = 0.589\n",
            "1532/18950 (epoch 4), train_loss = 2.058, time/batch = 0.590\n",
            "1533/18950 (epoch 4), train_loss = 2.053, time/batch = 0.588\n",
            "1534/18950 (epoch 4), train_loss = 2.038, time/batch = 0.610\n",
            "1535/18950 (epoch 4), train_loss = 2.048, time/batch = 0.588\n",
            "1536/18950 (epoch 4), train_loss = 2.038, time/batch = 0.597\n",
            "1537/18950 (epoch 4), train_loss = 2.037, time/batch = 0.603\n",
            "1538/18950 (epoch 4), train_loss = 2.054, time/batch = 0.582\n",
            "1539/18950 (epoch 4), train_loss = 2.047, time/batch = 0.626\n",
            "1540/18950 (epoch 4), train_loss = 2.040, time/batch = 0.613\n",
            "1541/18950 (epoch 4), train_loss = 2.037, time/batch = 0.582\n",
            "1542/18950 (epoch 4), train_loss = 2.050, time/batch = 0.598\n",
            "1543/18950 (epoch 4), train_loss = 2.040, time/batch = 0.579\n",
            "1544/18950 (epoch 4), train_loss = 2.018, time/batch = 0.594\n",
            "1545/18950 (epoch 4), train_loss = 2.051, time/batch = 0.572\n",
            "1546/18950 (epoch 4), train_loss = 2.035, time/batch = 0.688\n",
            "1547/18950 (epoch 4), train_loss = 2.039, time/batch = 0.759\n",
            "1548/18950 (epoch 4), train_loss = 2.058, time/batch = 0.767\n",
            "1549/18950 (epoch 4), train_loss = 2.028, time/batch = 0.760\n",
            "1550/18950 (epoch 4), train_loss = 2.045, time/batch = 0.739\n",
            "1551/18950 (epoch 4), train_loss = 2.037, time/batch = 0.777\n",
            "1552/18950 (epoch 4), train_loss = 2.031, time/batch = 0.764\n",
            "1553/18950 (epoch 4), train_loss = 2.054, time/batch = 0.742\n",
            "1554/18950 (epoch 4), train_loss = 2.017, time/batch = 0.737\n",
            "1555/18950 (epoch 4), train_loss = 2.021, time/batch = 0.781\n",
            "1556/18950 (epoch 4), train_loss = 2.044, time/batch = 0.772\n",
            "1557/18950 (epoch 4), train_loss = 2.028, time/batch = 0.739\n",
            "1558/18950 (epoch 4), train_loss = 2.051, time/batch = 0.780\n",
            "1559/18950 (epoch 4), train_loss = 2.050, time/batch = 0.759\n",
            "1560/18950 (epoch 4), train_loss = 2.031, time/batch = 0.754\n",
            "1561/18950 (epoch 4), train_loss = 2.037, time/batch = 0.742\n",
            "1562/18950 (epoch 4), train_loss = 2.067, time/batch = 0.658\n",
            "1563/18950 (epoch 4), train_loss = 2.033, time/batch = 0.605\n",
            "1564/18950 (epoch 4), train_loss = 2.021, time/batch = 0.595\n",
            "1565/18950 (epoch 4), train_loss = 2.034, time/batch = 0.594\n",
            "1566/18950 (epoch 4), train_loss = 2.020, time/batch = 0.612\n",
            "1567/18950 (epoch 4), train_loss = 2.015, time/batch = 0.606\n",
            "1568/18950 (epoch 4), train_loss = 2.032, time/batch = 0.603\n",
            "1569/18950 (epoch 4), train_loss = 2.037, time/batch = 0.586\n",
            "1570/18950 (epoch 4), train_loss = 2.048, time/batch = 0.574\n",
            "1571/18950 (epoch 4), train_loss = 2.063, time/batch = 0.583\n",
            "1572/18950 (epoch 4), train_loss = 2.050, time/batch = 0.587\n",
            "1573/18950 (epoch 4), train_loss = 2.041, time/batch = 0.616\n",
            "1574/18950 (epoch 4), train_loss = 2.021, time/batch = 0.579\n",
            "1575/18950 (epoch 4), train_loss = 2.010, time/batch = 0.616\n",
            "1576/18950 (epoch 4), train_loss = 2.020, time/batch = 0.607\n",
            "1577/18950 (epoch 4), train_loss = 2.024, time/batch = 0.583\n",
            "1578/18950 (epoch 4), train_loss = 2.021, time/batch = 0.578\n",
            "1579/18950 (epoch 4), train_loss = 2.032, time/batch = 0.723\n",
            "1580/18950 (epoch 4), train_loss = 2.048, time/batch = 0.767\n",
            "1581/18950 (epoch 4), train_loss = 2.053, time/batch = 0.755\n",
            "1582/18950 (epoch 4), train_loss = 2.035, time/batch = 0.731\n",
            "1583/18950 (epoch 4), train_loss = 2.027, time/batch = 0.789\n",
            "1584/18950 (epoch 4), train_loss = 2.031, time/batch = 0.774\n",
            "1585/18950 (epoch 4), train_loss = 2.050, time/batch = 0.762\n",
            "1586/18950 (epoch 4), train_loss = 2.064, time/batch = 0.768\n",
            "1587/18950 (epoch 4), train_loss = 2.039, time/batch = 0.722\n",
            "1588/18950 (epoch 4), train_loss = 2.033, time/batch = 0.723\n",
            "1589/18950 (epoch 4), train_loss = 2.049, time/batch = 0.761\n",
            "1590/18950 (epoch 4), train_loss = 2.011, time/batch = 0.790\n",
            "1591/18950 (epoch 4), train_loss = 2.027, time/batch = 0.707\n",
            "1592/18950 (epoch 4), train_loss = 2.009, time/batch = 0.774\n",
            "1593/18950 (epoch 4), train_loss = 2.014, time/batch = 0.760\n",
            "1594/18950 (epoch 4), train_loss = 2.013, time/batch = 0.811\n",
            "1595/18950 (epoch 4), train_loss = 2.028, time/batch = 0.646\n",
            "1596/18950 (epoch 4), train_loss = 2.019, time/batch = 0.574\n",
            "1597/18950 (epoch 4), train_loss = 2.034, time/batch = 0.585\n",
            "1598/18950 (epoch 4), train_loss = 2.032, time/batch = 0.577\n",
            "1599/18950 (epoch 4), train_loss = 2.049, time/batch = 0.587\n",
            "1600/18950 (epoch 4), train_loss = 2.032, time/batch = 0.573\n",
            "1601/18950 (epoch 4), train_loss = 2.028, time/batch = 0.580\n",
            "1602/18950 (epoch 4), train_loss = 2.060, time/batch = 0.570\n",
            "1603/18950 (epoch 4), train_loss = 2.039, time/batch = 0.576\n",
            "1604/18950 (epoch 4), train_loss = 2.019, time/batch = 0.587\n",
            "1605/18950 (epoch 4), train_loss = 2.019, time/batch = 0.581\n",
            "1606/18950 (epoch 4), train_loss = 2.045, time/batch = 0.580\n",
            "1607/18950 (epoch 4), train_loss = 2.026, time/batch = 0.574\n",
            "1608/18950 (epoch 4), train_loss = 2.019, time/batch = 0.606\n",
            "1609/18950 (epoch 4), train_loss = 2.046, time/batch = 0.586\n",
            "1610/18950 (epoch 4), train_loss = 2.039, time/batch = 0.574\n",
            "1611/18950 (epoch 4), train_loss = 2.028, time/batch = 0.575\n",
            "1612/18950 (epoch 4), train_loss = 2.035, time/batch = 0.648\n",
            "1613/18950 (epoch 4), train_loss = 2.056, time/batch = 0.769\n",
            "1614/18950 (epoch 4), train_loss = 2.030, time/batch = 0.762\n",
            "1615/18950 (epoch 4), train_loss = 2.025, time/batch = 0.742\n",
            "1616/18950 (epoch 4), train_loss = 2.043, time/batch = 0.754\n",
            "1617/18950 (epoch 4), train_loss = 2.044, time/batch = 0.754\n",
            "1618/18950 (epoch 4), train_loss = 2.030, time/batch = 0.767\n",
            "1619/18950 (epoch 4), train_loss = 2.031, time/batch = 0.715\n",
            "1620/18950 (epoch 4), train_loss = 2.030, time/batch = 0.769\n",
            "1621/18950 (epoch 4), train_loss = 2.013, time/batch = 0.775\n",
            "1622/18950 (epoch 4), train_loss = 2.016, time/batch = 0.757\n",
            "1623/18950 (epoch 4), train_loss = 2.015, time/batch = 0.755\n",
            "1624/18950 (epoch 4), train_loss = 2.015, time/batch = 0.767\n",
            "1625/18950 (epoch 4), train_loss = 2.023, time/batch = 0.749\n",
            "1626/18950 (epoch 4), train_loss = 2.041, time/batch = 0.740\n",
            "1627/18950 (epoch 4), train_loss = 2.036, time/batch = 0.731\n",
            "1628/18950 (epoch 4), train_loss = 2.009, time/batch = 0.725\n",
            "1629/18950 (epoch 4), train_loss = 2.013, time/batch = 0.592\n",
            "1630/18950 (epoch 4), train_loss = 2.011, time/batch = 0.594\n",
            "1631/18950 (epoch 4), train_loss = 2.016, time/batch = 0.596\n",
            "1632/18950 (epoch 4), train_loss = 2.037, time/batch = 0.600\n",
            "1633/18950 (epoch 4), train_loss = 2.027, time/batch = 0.610\n",
            "1634/18950 (epoch 4), train_loss = 2.034, time/batch = 0.597\n",
            "1635/18950 (epoch 4), train_loss = 2.028, time/batch = 0.589\n",
            "1636/18950 (epoch 4), train_loss = 2.007, time/batch = 0.604\n",
            "1637/18950 (epoch 4), train_loss = 2.006, time/batch = 0.581\n",
            "1638/18950 (epoch 4), train_loss = 2.042, time/batch = 0.595\n",
            "1639/18950 (epoch 4), train_loss = 2.016, time/batch = 0.603\n",
            "1640/18950 (epoch 4), train_loss = 2.017, time/batch = 0.579\n",
            "1641/18950 (epoch 4), train_loss = 2.002, time/batch = 0.583\n",
            "1642/18950 (epoch 4), train_loss = 2.015, time/batch = 0.594\n",
            "1643/18950 (epoch 4), train_loss = 2.008, time/batch = 0.588\n",
            "1644/18950 (epoch 4), train_loss = 1.990, time/batch = 0.612\n",
            "1645/18950 (epoch 4), train_loss = 2.029, time/batch = 0.660\n",
            "1646/18950 (epoch 4), train_loss = 2.000, time/batch = 0.759\n",
            "1647/18950 (epoch 4), train_loss = 2.000, time/batch = 0.770\n",
            "1648/18950 (epoch 4), train_loss = 2.017, time/batch = 0.741\n",
            "1649/18950 (epoch 4), train_loss = 1.988, time/batch = 0.754\n",
            "1650/18950 (epoch 4), train_loss = 1.964, time/batch = 0.743\n",
            "1651/18950 (epoch 4), train_loss = 1.992, time/batch = 0.768\n",
            "1652/18950 (epoch 4), train_loss = 1.995, time/batch = 0.777\n",
            "1653/18950 (epoch 4), train_loss = 1.994, time/batch = 0.748\n",
            "1654/18950 (epoch 4), train_loss = 1.998, time/batch = 0.744\n",
            "1655/18950 (epoch 4), train_loss = 1.984, time/batch = 0.754\n",
            "1656/18950 (epoch 4), train_loss = 2.007, time/batch = 0.761\n",
            "1657/18950 (epoch 4), train_loss = 2.013, time/batch = 0.767\n",
            "1658/18950 (epoch 4), train_loss = 2.004, time/batch = 0.751\n",
            "1659/18950 (epoch 4), train_loss = 2.013, time/batch = 0.781\n",
            "1660/18950 (epoch 4), train_loss = 2.023, time/batch = 0.779\n",
            "1661/18950 (epoch 4), train_loss = 2.009, time/batch = 0.587\n",
            "1662/18950 (epoch 4), train_loss = 2.029, time/batch = 0.577\n",
            "1663/18950 (epoch 4), train_loss = 2.010, time/batch = 0.596\n",
            "1664/18950 (epoch 4), train_loss = 2.004, time/batch = 0.575\n",
            "1665/18950 (epoch 4), train_loss = 2.019, time/batch = 0.603\n",
            "1666/18950 (epoch 4), train_loss = 2.025, time/batch = 0.574\n",
            "1667/18950 (epoch 4), train_loss = 1.987, time/batch = 0.582\n",
            "1668/18950 (epoch 4), train_loss = 1.993, time/batch = 0.593\n",
            "1669/18950 (epoch 4), train_loss = 2.007, time/batch = 0.583\n",
            "1670/18950 (epoch 4), train_loss = 2.013, time/batch = 0.595\n",
            "1671/18950 (epoch 4), train_loss = 2.032, time/batch = 0.585\n",
            "1672/18950 (epoch 4), train_loss = 2.018, time/batch = 0.580\n",
            "1673/18950 (epoch 4), train_loss = 1.999, time/batch = 0.595\n",
            "1674/18950 (epoch 4), train_loss = 1.996, time/batch = 0.590\n",
            "1675/18950 (epoch 4), train_loss = 1.998, time/batch = 0.570\n",
            "1676/18950 (epoch 4), train_loss = 2.011, time/batch = 0.594\n",
            "1677/18950 (epoch 4), train_loss = 1.977, time/batch = 0.582\n",
            "1678/18950 (epoch 4), train_loss = 2.016, time/batch = 0.715\n",
            "1679/18950 (epoch 4), train_loss = 1.993, time/batch = 0.755\n",
            "1680/18950 (epoch 4), train_loss = 1.988, time/batch = 0.748\n",
            "1681/18950 (epoch 4), train_loss = 1.969, time/batch = 0.755\n",
            "1682/18950 (epoch 4), train_loss = 1.990, time/batch = 0.787\n",
            "1683/18950 (epoch 4), train_loss = 1.999, time/batch = 0.750\n",
            "1684/18950 (epoch 4), train_loss = 2.030, time/batch = 0.743\n",
            "1685/18950 (epoch 4), train_loss = 2.001, time/batch = 0.750\n",
            "1686/18950 (epoch 4), train_loss = 1.997, time/batch = 0.754\n",
            "1687/18950 (epoch 4), train_loss = 1.977, time/batch = 0.765\n",
            "1688/18950 (epoch 4), train_loss = 1.999, time/batch = 0.753\n",
            "1689/18950 (epoch 4), train_loss = 1.978, time/batch = 0.757\n",
            "1690/18950 (epoch 4), train_loss = 1.967, time/batch = 0.759\n",
            "1691/18950 (epoch 4), train_loss = 1.967, time/batch = 0.764\n",
            "1692/18950 (epoch 4), train_loss = 1.992, time/batch = 0.732\n",
            "1693/18950 (epoch 4), train_loss = 2.019, time/batch = 0.749\n",
            "1694/18950 (epoch 4), train_loss = 1.991, time/batch = 0.603\n",
            "1695/18950 (epoch 4), train_loss = 2.002, time/batch = 0.572\n",
            "1696/18950 (epoch 4), train_loss = 2.009, time/batch = 0.583\n",
            "1697/18950 (epoch 4), train_loss = 1.998, time/batch = 0.598\n",
            "1698/18950 (epoch 4), train_loss = 2.007, time/batch = 0.582\n",
            "1699/18950 (epoch 4), train_loss = 1.987, time/batch = 0.575\n",
            "1700/18950 (epoch 4), train_loss = 2.007, time/batch = 0.584\n",
            "1701/18950 (epoch 4), train_loss = 1.991, time/batch = 0.585\n",
            "1702/18950 (epoch 4), train_loss = 1.986, time/batch = 0.578\n",
            "1703/18950 (epoch 4), train_loss = 1.997, time/batch = 0.600\n",
            "1704/18950 (epoch 4), train_loss = 1.984, time/batch = 0.589\n",
            "1705/18950 (epoch 4), train_loss = 2.020, time/batch = 0.590\n",
            "1706/18950 (epoch 4), train_loss = 1.983, time/batch = 0.582\n",
            "1707/18950 (epoch 4), train_loss = 2.015, time/batch = 0.592\n",
            "1708/18950 (epoch 4), train_loss = 1.989, time/batch = 0.604\n",
            "1709/18950 (epoch 4), train_loss = 1.996, time/batch = 0.602\n",
            "1710/18950 (epoch 4), train_loss = 1.958, time/batch = 0.587\n",
            "1711/18950 (epoch 4), train_loss = 1.962, time/batch = 0.721\n",
            "1712/18950 (epoch 4), train_loss = 1.976, time/batch = 0.767\n",
            "1713/18950 (epoch 4), train_loss = 1.981, time/batch = 0.766\n",
            "1714/18950 (epoch 4), train_loss = 1.995, time/batch = 0.790\n",
            "1715/18950 (epoch 4), train_loss = 2.015, time/batch = 0.754\n",
            "1716/18950 (epoch 4), train_loss = 1.996, time/batch = 0.785\n",
            "1717/18950 (epoch 4), train_loss = 1.995, time/batch = 0.814\n",
            "1718/18950 (epoch 4), train_loss = 1.960, time/batch = 0.783\n",
            "1719/18950 (epoch 4), train_loss = 1.961, time/batch = 0.764\n",
            "1720/18950 (epoch 4), train_loss = 1.980, time/batch = 0.716\n",
            "1721/18950 (epoch 4), train_loss = 1.983, time/batch = 0.740\n",
            "1722/18950 (epoch 4), train_loss = 1.985, time/batch = 0.785\n",
            "1723/18950 (epoch 4), train_loss = 1.997, time/batch = 0.730\n",
            "1724/18950 (epoch 4), train_loss = 1.992, time/batch = 0.751\n",
            "1725/18950 (epoch 4), train_loss = 2.011, time/batch = 0.763\n",
            "1726/18950 (epoch 4), train_loss = 2.007, time/batch = 0.757\n",
            "1727/18950 (epoch 4), train_loss = 1.993, time/batch = 0.597\n",
            "1728/18950 (epoch 4), train_loss = 1.981, time/batch = 0.583\n",
            "1729/18950 (epoch 4), train_loss = 1.992, time/batch = 0.578\n",
            "1730/18950 (epoch 4), train_loss = 1.984, time/batch = 0.581\n",
            "1731/18950 (epoch 4), train_loss = 1.982, time/batch = 0.580\n",
            "1732/18950 (epoch 4), train_loss = 2.001, time/batch = 0.585\n",
            "1733/18950 (epoch 4), train_loss = 1.983, time/batch = 0.580\n",
            "1734/18950 (epoch 4), train_loss = 1.978, time/batch = 0.581\n",
            "1735/18950 (epoch 4), train_loss = 1.970, time/batch = 0.599\n",
            "1736/18950 (epoch 4), train_loss = 1.976, time/batch = 0.588\n",
            "1737/18950 (epoch 4), train_loss = 1.982, time/batch = 0.595\n",
            "1738/18950 (epoch 4), train_loss = 1.974, time/batch = 0.588\n",
            "1739/18950 (epoch 4), train_loss = 1.986, time/batch = 0.580\n",
            "1740/18950 (epoch 4), train_loss = 1.982, time/batch = 0.579\n",
            "1741/18950 (epoch 4), train_loss = 1.984, time/batch = 0.586\n",
            "1742/18950 (epoch 4), train_loss = 1.993, time/batch = 0.587\n",
            "1743/18950 (epoch 4), train_loss = 1.996, time/batch = 0.589\n",
            "1744/18950 (epoch 4), train_loss = 1.991, time/batch = 0.750\n",
            "1745/18950 (epoch 4), train_loss = 2.001, time/batch = 0.759\n",
            "1746/18950 (epoch 4), train_loss = 2.003, time/batch = 0.750\n",
            "1747/18950 (epoch 4), train_loss = 1.999, time/batch = 0.765\n",
            "1748/18950 (epoch 4), train_loss = 1.991, time/batch = 0.738\n",
            "1749/18950 (epoch 4), train_loss = 1.991, time/batch = 0.749\n",
            "1750/18950 (epoch 4), train_loss = 1.997, time/batch = 0.722\n",
            "1751/18950 (epoch 4), train_loss = 1.979, time/batch = 0.764\n",
            "1752/18950 (epoch 4), train_loss = 1.979, time/batch = 0.770\n",
            "1753/18950 (epoch 4), train_loss = 1.978, time/batch = 0.772\n",
            "1754/18950 (epoch 4), train_loss = 1.983, time/batch = 0.745\n",
            "1755/18950 (epoch 4), train_loss = 1.978, time/batch = 0.762\n",
            "1756/18950 (epoch 4), train_loss = 1.967, time/batch = 0.770\n",
            "1757/18950 (epoch 4), train_loss = 1.990, time/batch = 0.750\n",
            "1758/18950 (epoch 4), train_loss = 2.008, time/batch = 0.749\n",
            "1759/18950 (epoch 4), train_loss = 1.988, time/batch = 0.701\n",
            "1760/18950 (epoch 4), train_loss = 2.002, time/batch = 0.571\n",
            "1761/18950 (epoch 4), train_loss = 1.996, time/batch = 0.604\n",
            "1762/18950 (epoch 4), train_loss = 1.993, time/batch = 0.587\n",
            "1763/18950 (epoch 4), train_loss = 1.999, time/batch = 0.585\n",
            "1764/18950 (epoch 4), train_loss = 1.985, time/batch = 0.588\n",
            "1765/18950 (epoch 4), train_loss = 2.019, time/batch = 0.574\n",
            "1766/18950 (epoch 4), train_loss = 1.966, time/batch = 0.582\n",
            "1767/18950 (epoch 4), train_loss = 1.993, time/batch = 0.571\n",
            "1768/18950 (epoch 4), train_loss = 1.993, time/batch = 0.584\n",
            "1769/18950 (epoch 4), train_loss = 2.003, time/batch = 0.594\n",
            "1770/18950 (epoch 4), train_loss = 1.982, time/batch = 0.577\n",
            "1771/18950 (epoch 4), train_loss = 1.974, time/batch = 0.586\n",
            "1772/18950 (epoch 4), train_loss = 1.992, time/batch = 0.573\n",
            "1773/18950 (epoch 4), train_loss = 2.000, time/batch = 0.617\n",
            "1774/18950 (epoch 4), train_loss = 1.968, time/batch = 0.590\n",
            "1775/18950 (epoch 4), train_loss = 1.957, time/batch = 0.587\n",
            "1776/18950 (epoch 4), train_loss = 1.975, time/batch = 0.612\n",
            "1777/18950 (epoch 4), train_loss = 1.971, time/batch = 0.741\n",
            "1778/18950 (epoch 4), train_loss = 1.955, time/batch = 0.749\n",
            "1779/18950 (epoch 4), train_loss = 1.972, time/batch = 0.769\n",
            "1780/18950 (epoch 4), train_loss = 1.973, time/batch = 0.734\n",
            "1781/18950 (epoch 4), train_loss = 1.972, time/batch = 0.771\n",
            "1782/18950 (epoch 4), train_loss = 1.992, time/batch = 0.792\n",
            "1783/18950 (epoch 4), train_loss = 1.967, time/batch = 0.765\n",
            "1784/18950 (epoch 4), train_loss = 1.980, time/batch = 0.759\n",
            "1785/18950 (epoch 4), train_loss = 1.953, time/batch = 0.757\n",
            "1786/18950 (epoch 4), train_loss = 1.947, time/batch = 0.758\n",
            "1787/18950 (epoch 4), train_loss = 1.976, time/batch = 0.752\n",
            "1788/18950 (epoch 4), train_loss = 1.948, time/batch = 0.751\n",
            "1789/18950 (epoch 4), train_loss = 1.972, time/batch = 0.775\n",
            "1790/18950 (epoch 4), train_loss = 1.958, time/batch = 0.762\n",
            "1791/18950 (epoch 4), train_loss = 1.970, time/batch = 0.766\n",
            "1792/18950 (epoch 4), train_loss = 1.953, time/batch = 0.681\n",
            "1793/18950 (epoch 4), train_loss = 1.962, time/batch = 0.596\n",
            "1794/18950 (epoch 4), train_loss = 1.992, time/batch = 0.595\n",
            "1795/18950 (epoch 4), train_loss = 1.995, time/batch = 0.604\n",
            "1796/18950 (epoch 4), train_loss = 1.991, time/batch = 0.581\n",
            "1797/18950 (epoch 4), train_loss = 1.977, time/batch = 0.581\n",
            "1798/18950 (epoch 4), train_loss = 1.965, time/batch = 0.601\n",
            "1799/18950 (epoch 4), train_loss = 1.981, time/batch = 0.607\n",
            "1800/18950 (epoch 4), train_loss = 1.956, time/batch = 0.602\n",
            "1801/18950 (epoch 4), train_loss = 1.938, time/batch = 0.603\n",
            "1802/18950 (epoch 4), train_loss = 1.959, time/batch = 0.605\n",
            "1803/18950 (epoch 4), train_loss = 1.967, time/batch = 0.595\n",
            "1804/18950 (epoch 4), train_loss = 1.979, time/batch = 0.595\n",
            "1805/18950 (epoch 4), train_loss = 1.979, time/batch = 0.610\n",
            "1806/18950 (epoch 4), train_loss = 1.973, time/batch = 0.584\n",
            "1807/18950 (epoch 4), train_loss = 1.951, time/batch = 0.588\n",
            "1808/18950 (epoch 4), train_loss = 1.948, time/batch = 0.623\n",
            "1809/18950 (epoch 4), train_loss = 2.007, time/batch = 0.701\n",
            "1810/18950 (epoch 4), train_loss = 2.021, time/batch = 0.761\n",
            "1811/18950 (epoch 4), train_loss = 1.988, time/batch = 0.773\n",
            "1812/18950 (epoch 4), train_loss = 1.974, time/batch = 0.727\n",
            "1813/18950 (epoch 4), train_loss = 1.983, time/batch = 0.733\n",
            "1814/18950 (epoch 4), train_loss = 1.961, time/batch = 0.772\n",
            "1815/18950 (epoch 4), train_loss = 1.954, time/batch = 0.744\n",
            "1816/18950 (epoch 4), train_loss = 1.964, time/batch = 0.734\n",
            "1817/18950 (epoch 4), train_loss = 1.962, time/batch = 0.723\n",
            "1818/18950 (epoch 4), train_loss = 1.947, time/batch = 0.775\n",
            "1819/18950 (epoch 4), train_loss = 1.947, time/batch = 0.759\n",
            "1820/18950 (epoch 4), train_loss = 1.963, time/batch = 0.755\n",
            "1821/18950 (epoch 4), train_loss = 1.997, time/batch = 0.740\n",
            "1822/18950 (epoch 4), train_loss = 1.978, time/batch = 0.723\n",
            "1823/18950 (epoch 4), train_loss = 1.948, time/batch = 0.746\n",
            "1824/18950 (epoch 4), train_loss = 1.963, time/batch = 0.762\n",
            "1825/18950 (epoch 4), train_loss = 1.974, time/batch = 0.655\n",
            "1826/18950 (epoch 4), train_loss = 1.946, time/batch = 0.590\n",
            "1827/18950 (epoch 4), train_loss = 1.933, time/batch = 0.606\n",
            "1828/18950 (epoch 4), train_loss = 1.940, time/batch = 0.582\n",
            "1829/18950 (epoch 4), train_loss = 1.957, time/batch = 0.589\n",
            "1830/18950 (epoch 4), train_loss = 1.990, time/batch = 0.571\n",
            "1831/18950 (epoch 4), train_loss = 1.952, time/batch = 0.586\n",
            "1832/18950 (epoch 4), train_loss = 1.991, time/batch = 0.582\n",
            "1833/18950 (epoch 4), train_loss = 1.956, time/batch = 0.573\n",
            "1834/18950 (epoch 4), train_loss = 1.940, time/batch = 0.594\n",
            "1835/18950 (epoch 4), train_loss = 1.937, time/batch = 0.589\n",
            "1836/18950 (epoch 4), train_loss = 1.935, time/batch = 0.582\n",
            "1837/18950 (epoch 4), train_loss = 1.951, time/batch = 0.613\n",
            "1838/18950 (epoch 4), train_loss = 1.953, time/batch = 0.614\n",
            "1839/18950 (epoch 4), train_loss = 1.970, time/batch = 0.585\n",
            "1840/18950 (epoch 4), train_loss = 1.962, time/batch = 0.583\n",
            "1841/18950 (epoch 4), train_loss = 1.939, time/batch = 0.579\n",
            "1842/18950 (epoch 4), train_loss = 1.947, time/batch = 0.667\n",
            "1843/18950 (epoch 4), train_loss = 1.966, time/batch = 0.755\n",
            "1844/18950 (epoch 4), train_loss = 1.956, time/batch = 0.764\n",
            "1845/18950 (epoch 4), train_loss = 1.952, time/batch = 0.751\n",
            "1846/18950 (epoch 4), train_loss = 1.934, time/batch = 0.770\n",
            "1847/18950 (epoch 4), train_loss = 1.942, time/batch = 0.750\n",
            "1848/18950 (epoch 4), train_loss = 1.940, time/batch = 0.742\n",
            "1849/18950 (epoch 4), train_loss = 1.945, time/batch = 0.754\n",
            "1850/18950 (epoch 4), train_loss = 1.952, time/batch = 0.758\n",
            "1851/18950 (epoch 4), train_loss = 1.954, time/batch = 0.758\n",
            "1852/18950 (epoch 4), train_loss = 1.949, time/batch = 0.743\n",
            "1853/18950 (epoch 4), train_loss = 1.941, time/batch = 0.755\n",
            "1854/18950 (epoch 4), train_loss = 1.971, time/batch = 0.757\n",
            "1855/18950 (epoch 4), train_loss = 1.938, time/batch = 0.768\n",
            "1856/18950 (epoch 4), train_loss = 1.953, time/batch = 0.767\n",
            "1857/18950 (epoch 4), train_loss = 1.938, time/batch = 0.761\n",
            "1858/18950 (epoch 4), train_loss = 1.960, time/batch = 0.593\n",
            "1859/18950 (epoch 4), train_loss = 1.953, time/batch = 0.598\n",
            "1860/18950 (epoch 4), train_loss = 1.944, time/batch = 0.594\n",
            "1861/18950 (epoch 4), train_loss = 1.945, time/batch = 0.597\n",
            "1862/18950 (epoch 4), train_loss = 1.924, time/batch = 0.587\n",
            "1863/18950 (epoch 4), train_loss = 1.949, time/batch = 0.586\n",
            "1864/18950 (epoch 4), train_loss = 1.951, time/batch = 0.585\n",
            "1865/18950 (epoch 4), train_loss = 1.949, time/batch = 0.592\n",
            "1866/18950 (epoch 4), train_loss = 1.946, time/batch = 0.592\n",
            "1867/18950 (epoch 4), train_loss = 1.956, time/batch = 0.586\n",
            "1868/18950 (epoch 4), train_loss = 1.960, time/batch = 0.616\n",
            "1869/18950 (epoch 4), train_loss = 1.933, time/batch = 0.593\n",
            "1870/18950 (epoch 4), train_loss = 1.950, time/batch = 0.587\n",
            "1871/18950 (epoch 4), train_loss = 1.951, time/batch = 0.589\n",
            "1872/18950 (epoch 4), train_loss = 1.946, time/batch = 0.581\n",
            "1873/18950 (epoch 4), train_loss = 1.978, time/batch = 0.572\n",
            "1874/18950 (epoch 4), train_loss = 1.934, time/batch = 0.588\n",
            "1875/18950 (epoch 4), train_loss = 1.954, time/batch = 0.756\n",
            "1876/18950 (epoch 4), train_loss = 1.932, time/batch = 0.719\n",
            "1877/18950 (epoch 4), train_loss = 1.948, time/batch = 0.732\n",
            "1878/18950 (epoch 4), train_loss = 1.953, time/batch = 0.736\n",
            "1879/18950 (epoch 4), train_loss = 1.952, time/batch = 0.733\n",
            "1880/18950 (epoch 4), train_loss = 1.944, time/batch = 0.772\n",
            "1881/18950 (epoch 4), train_loss = 1.960, time/batch = 0.763\n",
            "1882/18950 (epoch 4), train_loss = 1.985, time/batch = 0.771\n",
            "1883/18950 (epoch 4), train_loss = 1.960, time/batch = 0.753\n",
            "1884/18950 (epoch 4), train_loss = 1.941, time/batch = 0.741\n",
            "1885/18950 (epoch 4), train_loss = 1.932, time/batch = 0.753\n",
            "1886/18950 (epoch 4), train_loss = 1.930, time/batch = 0.759\n",
            "1887/18950 (epoch 4), train_loss = 1.934, time/batch = 0.770\n",
            "1888/18950 (epoch 4), train_loss = 1.939, time/batch = 0.760\n",
            "1889/18950 (epoch 4), train_loss = 1.952, time/batch = 0.767\n",
            "1890/18950 (epoch 4), train_loss = 1.906, time/batch = 0.743\n",
            "1891/18950 (epoch 4), train_loss = 1.938, time/batch = 0.606\n",
            "1892/18950 (epoch 4), train_loss = 1.952, time/batch = 0.609\n",
            "1893/18950 (epoch 4), train_loss = 1.944, time/batch = 0.579\n",
            "1894/18950 (epoch 4), train_loss = 1.946, time/batch = 0.596\n",
            "1895/18950 (epoch 5), train_loss = 1.981, time/batch = 0.589\n",
            "1896/18950 (epoch 5), train_loss = 1.946, time/batch = 0.583\n",
            "1897/18950 (epoch 5), train_loss = 1.955, time/batch = 0.586\n",
            "1898/18950 (epoch 5), train_loss = 1.947, time/batch = 0.592\n",
            "1899/18950 (epoch 5), train_loss = 1.959, time/batch = 0.592\n",
            "1900/18950 (epoch 5), train_loss = 1.920, time/batch = 0.591\n",
            "1901/18950 (epoch 5), train_loss = 1.903, time/batch = 0.582\n",
            "1902/18950 (epoch 5), train_loss = 1.920, time/batch = 0.577\n",
            "1903/18950 (epoch 5), train_loss = 1.920, time/batch = 0.586\n",
            "1904/18950 (epoch 5), train_loss = 1.928, time/batch = 0.577\n",
            "1905/18950 (epoch 5), train_loss = 1.926, time/batch = 0.571\n",
            "1906/18950 (epoch 5), train_loss = 1.922, time/batch = 0.743\n",
            "1907/18950 (epoch 5), train_loss = 1.937, time/batch = 0.733\n",
            "1908/18950 (epoch 5), train_loss = 1.917, time/batch = 0.704\n",
            "1909/18950 (epoch 5), train_loss = 1.930, time/batch = 0.750\n",
            "1910/18950 (epoch 5), train_loss = 1.928, time/batch = 0.731\n",
            "1911/18950 (epoch 5), train_loss = 1.934, time/batch = 0.787\n",
            "1912/18950 (epoch 5), train_loss = 1.927, time/batch = 0.776\n",
            "1913/18950 (epoch 5), train_loss = 1.912, time/batch = 0.756\n",
            "1914/18950 (epoch 5), train_loss = 1.924, time/batch = 0.734\n",
            "1915/18950 (epoch 5), train_loss = 1.918, time/batch = 0.742\n",
            "1916/18950 (epoch 5), train_loss = 1.920, time/batch = 0.750\n",
            "1917/18950 (epoch 5), train_loss = 1.934, time/batch = 0.733\n",
            "1918/18950 (epoch 5), train_loss = 1.926, time/batch = 0.748\n",
            "1919/18950 (epoch 5), train_loss = 1.920, time/batch = 0.730\n",
            "1920/18950 (epoch 5), train_loss = 1.919, time/batch = 0.750\n",
            "1921/18950 (epoch 5), train_loss = 1.920, time/batch = 0.747\n",
            "1922/18950 (epoch 5), train_loss = 1.923, time/batch = 0.657\n",
            "1923/18950 (epoch 5), train_loss = 1.899, time/batch = 0.574\n",
            "1924/18950 (epoch 5), train_loss = 1.935, time/batch = 0.571\n",
            "1925/18950 (epoch 5), train_loss = 1.919, time/batch = 0.584\n",
            "1926/18950 (epoch 5), train_loss = 1.929, time/batch = 0.574\n",
            "1927/18950 (epoch 5), train_loss = 1.936, time/batch = 0.574\n",
            "1928/18950 (epoch 5), train_loss = 1.913, time/batch = 0.579\n",
            "1929/18950 (epoch 5), train_loss = 1.919, time/batch = 0.575\n",
            "1930/18950 (epoch 5), train_loss = 1.920, time/batch = 0.592\n",
            "1931/18950 (epoch 5), train_loss = 1.910, time/batch = 0.599\n",
            "1932/18950 (epoch 5), train_loss = 1.943, time/batch = 0.577\n",
            "1933/18950 (epoch 5), train_loss = 1.904, time/batch = 0.584\n",
            "1934/18950 (epoch 5), train_loss = 1.910, time/batch = 0.575\n",
            "1935/18950 (epoch 5), train_loss = 1.926, time/batch = 0.574\n",
            "1936/18950 (epoch 5), train_loss = 1.912, time/batch = 0.573\n",
            "1937/18950 (epoch 5), train_loss = 1.943, time/batch = 0.571\n",
            "1938/18950 (epoch 5), train_loss = 1.936, time/batch = 0.576\n",
            "1939/18950 (epoch 5), train_loss = 1.919, time/batch = 0.604\n",
            "1940/18950 (epoch 5), train_loss = 1.922, time/batch = 0.738\n",
            "1941/18950 (epoch 5), train_loss = 1.954, time/batch = 0.754\n",
            "1942/18950 (epoch 5), train_loss = 1.920, time/batch = 0.703\n",
            "1943/18950 (epoch 5), train_loss = 1.913, time/batch = 0.741\n",
            "1944/18950 (epoch 5), train_loss = 1.927, time/batch = 0.736\n",
            "1945/18950 (epoch 5), train_loss = 1.904, time/batch = 0.752\n",
            "1946/18950 (epoch 5), train_loss = 1.908, time/batch = 0.740\n",
            "1947/18950 (epoch 5), train_loss = 1.922, time/batch = 0.754\n",
            "1948/18950 (epoch 5), train_loss = 1.925, time/batch = 0.748\n",
            "1949/18950 (epoch 5), train_loss = 1.931, time/batch = 0.750\n",
            "1950/18950 (epoch 5), train_loss = 1.947, time/batch = 0.753\n",
            "1951/18950 (epoch 5), train_loss = 1.939, time/batch = 0.761\n",
            "1952/18950 (epoch 5), train_loss = 1.928, time/batch = 0.747\n",
            "1953/18950 (epoch 5), train_loss = 1.900, time/batch = 0.757\n",
            "1954/18950 (epoch 5), train_loss = 1.893, time/batch = 0.753\n",
            "1955/18950 (epoch 5), train_loss = 1.904, time/batch = 0.718\n",
            "1956/18950 (epoch 5), train_loss = 1.911, time/batch = 0.636\n",
            "1957/18950 (epoch 5), train_loss = 1.908, time/batch = 0.582\n",
            "1958/18950 (epoch 5), train_loss = 1.918, time/batch = 0.575\n",
            "1959/18950 (epoch 5), train_loss = 1.935, time/batch = 0.574\n",
            "1960/18950 (epoch 5), train_loss = 1.946, time/batch = 0.566\n",
            "1961/18950 (epoch 5), train_loss = 1.921, time/batch = 0.577\n",
            "1962/18950 (epoch 5), train_loss = 1.915, time/batch = 0.574\n",
            "1963/18950 (epoch 5), train_loss = 1.920, time/batch = 0.590\n",
            "1964/18950 (epoch 5), train_loss = 1.937, time/batch = 0.576\n",
            "1965/18950 (epoch 5), train_loss = 1.953, time/batch = 0.579\n",
            "1966/18950 (epoch 5), train_loss = 1.929, time/batch = 0.566\n",
            "1967/18950 (epoch 5), train_loss = 1.912, time/batch = 0.575\n",
            "1968/18950 (epoch 5), train_loss = 1.935, time/batch = 0.581\n",
            "1969/18950 (epoch 5), train_loss = 1.901, time/batch = 0.575\n",
            "1970/18950 (epoch 5), train_loss = 1.906, time/batch = 0.586\n",
            "1971/18950 (epoch 5), train_loss = 1.893, time/batch = 0.573\n",
            "1972/18950 (epoch 5), train_loss = 1.892, time/batch = 0.598\n",
            "1973/18950 (epoch 5), train_loss = 1.903, time/batch = 0.751\n",
            "1974/18950 (epoch 5), train_loss = 1.910, time/batch = 0.736\n",
            "1975/18950 (epoch 5), train_loss = 1.904, time/batch = 0.701\n",
            "1976/18950 (epoch 5), train_loss = 1.930, time/batch = 0.770\n",
            "1977/18950 (epoch 5), train_loss = 1.926, time/batch = 0.800\n",
            "1978/18950 (epoch 5), train_loss = 1.939, time/batch = 0.741\n",
            "1979/18950 (epoch 5), train_loss = 1.922, time/batch = 0.740\n",
            "1980/18950 (epoch 5), train_loss = 1.917, time/batch = 0.773\n",
            "1981/18950 (epoch 5), train_loss = 1.956, time/batch = 0.746\n",
            "1982/18950 (epoch 5), train_loss = 1.930, time/batch = 0.746\n",
            "1983/18950 (epoch 5), train_loss = 1.917, time/batch = 0.763\n",
            "1984/18950 (epoch 5), train_loss = 1.907, time/batch = 0.755\n",
            "1985/18950 (epoch 5), train_loss = 1.934, time/batch = 0.806\n",
            "1986/18950 (epoch 5), train_loss = 1.921, time/batch = 0.724\n",
            "1987/18950 (epoch 5), train_loss = 1.907, time/batch = 0.737\n",
            "1988/18950 (epoch 5), train_loss = 1.940, time/batch = 0.762\n",
            "1989/18950 (epoch 5), train_loss = 1.935, time/batch = 0.586\n",
            "1990/18950 (epoch 5), train_loss = 1.919, time/batch = 0.580\n",
            "1991/18950 (epoch 5), train_loss = 1.919, time/batch = 0.585\n",
            "1992/18950 (epoch 5), train_loss = 1.940, time/batch = 0.581\n",
            "1993/18950 (epoch 5), train_loss = 1.920, time/batch = 0.579\n",
            "1994/18950 (epoch 5), train_loss = 1.913, time/batch = 0.595\n",
            "1995/18950 (epoch 5), train_loss = 1.944, time/batch = 0.587\n",
            "1996/18950 (epoch 5), train_loss = 1.928, time/batch = 0.598\n",
            "1997/18950 (epoch 5), train_loss = 1.911, time/batch = 0.580\n",
            "1998/18950 (epoch 5), train_loss = 1.915, time/batch = 0.589\n",
            "1999/18950 (epoch 5), train_loss = 1.922, time/batch = 0.584\n",
            "2000/18950 (epoch 5), train_loss = 1.902, time/batch = 0.591\n",
            "model saved to /content/drive/Shareddrives/Materials UNI/UNIPI/ISPR/Midterm3 Assignment_4/save/save1/model.ckpt\n",
            "2001/18950 (epoch 5), train_loss = 1.908, time/batch = 0.687\n",
            "2002/18950 (epoch 5), train_loss = 1.905, time/batch = 0.606\n",
            "2003/18950 (epoch 5), train_loss = 1.905, time/batch = 0.763\n",
            "2004/18950 (epoch 5), train_loss = 1.914, time/batch = 0.785\n",
            "2005/18950 (epoch 5), train_loss = 1.934, time/batch = 0.763\n",
            "2006/18950 (epoch 5), train_loss = 1.925, time/batch = 0.765\n",
            "2007/18950 (epoch 5), train_loss = 1.901, time/batch = 0.773\n",
            "2008/18950 (epoch 5), train_loss = 1.901, time/batch = 0.838\n",
            "2009/18950 (epoch 5), train_loss = 1.899, time/batch = 0.824\n",
            "2010/18950 (epoch 5), train_loss = 1.907, time/batch = 0.752\n",
            "2011/18950 (epoch 5), train_loss = 1.939, time/batch = 0.788\n",
            "2012/18950 (epoch 5), train_loss = 1.924, time/batch = 0.759\n",
            "2013/18950 (epoch 5), train_loss = 1.924, time/batch = 0.767\n",
            "2014/18950 (epoch 5), train_loss = 1.920, time/batch = 0.765\n",
            "2015/18950 (epoch 5), train_loss = 1.889, time/batch = 0.782\n",
            "2016/18950 (epoch 5), train_loss = 1.898, time/batch = 0.767\n",
            "2017/18950 (epoch 5), train_loss = 1.937, time/batch = 0.811\n",
            "2018/18950 (epoch 5), train_loss = 1.911, time/batch = 0.765\n",
            "2019/18950 (epoch 5), train_loss = 1.912, time/batch = 0.621\n",
            "2020/18950 (epoch 5), train_loss = 1.897, time/batch = 0.589\n",
            "2021/18950 (epoch 5), train_loss = 1.913, time/batch = 0.601\n",
            "2022/18950 (epoch 5), train_loss = 1.908, time/batch = 0.603\n",
            "2023/18950 (epoch 5), train_loss = 1.888, time/batch = 0.593\n",
            "2024/18950 (epoch 5), train_loss = 1.916, time/batch = 0.581\n",
            "2025/18950 (epoch 5), train_loss = 1.894, time/batch = 0.577\n",
            "2026/18950 (epoch 5), train_loss = 1.898, time/batch = 0.590\n",
            "2027/18950 (epoch 5), train_loss = 1.905, time/batch = 0.590\n",
            "2028/18950 (epoch 5), train_loss = 1.884, time/batch = 0.577\n",
            "2029/18950 (epoch 5), train_loss = 1.861, time/batch = 0.603\n",
            "2030/18950 (epoch 5), train_loss = 1.887, time/batch = 0.585\n",
            "2031/18950 (epoch 5), train_loss = 1.886, time/batch = 0.605\n",
            "2032/18950 (epoch 5), train_loss = 1.889, time/batch = 0.605\n",
            "2033/18950 (epoch 5), train_loss = 1.894, time/batch = 0.591\n",
            "2034/18950 (epoch 5), train_loss = 1.883, time/batch = 0.595\n",
            "2035/18950 (epoch 5), train_loss = 1.906, time/batch = 0.614\n",
            "2036/18950 (epoch 5), train_loss = 1.914, time/batch = 0.756\n",
            "2037/18950 (epoch 5), train_loss = 1.902, time/batch = 0.761\n",
            "2038/18950 (epoch 5), train_loss = 1.909, time/batch = 0.760\n",
            "2039/18950 (epoch 5), train_loss = 1.911, time/batch = 0.739\n",
            "2040/18950 (epoch 5), train_loss = 1.892, time/batch = 0.774\n",
            "2041/18950 (epoch 5), train_loss = 1.929, time/batch = 0.770\n",
            "2042/18950 (epoch 5), train_loss = 1.907, time/batch = 0.784\n",
            "2043/18950 (epoch 5), train_loss = 1.889, time/batch = 0.754\n",
            "2044/18950 (epoch 5), train_loss = 1.917, time/batch = 0.803\n",
            "2045/18950 (epoch 5), train_loss = 1.927, time/batch = 0.764\n",
            "2046/18950 (epoch 5), train_loss = 1.878, time/batch = 0.752\n",
            "2047/18950 (epoch 5), train_loss = 1.893, time/batch = 0.760\n",
            "2048/18950 (epoch 5), train_loss = 1.910, time/batch = 0.775\n",
            "2049/18950 (epoch 5), train_loss = 1.906, time/batch = 0.722\n",
            "2050/18950 (epoch 5), train_loss = 1.930, time/batch = 0.757\n",
            "2051/18950 (epoch 5), train_loss = 1.915, time/batch = 0.626\n",
            "2052/18950 (epoch 5), train_loss = 1.908, time/batch = 0.592\n",
            "2053/18950 (epoch 5), train_loss = 1.888, time/batch = 0.578\n",
            "2054/18950 (epoch 5), train_loss = 1.900, time/batch = 0.586\n",
            "2055/18950 (epoch 5), train_loss = 1.901, time/batch = 0.576\n",
            "2056/18950 (epoch 5), train_loss = 1.878, time/batch = 0.585\n",
            "2057/18950 (epoch 5), train_loss = 1.910, time/batch = 0.575\n",
            "2058/18950 (epoch 5), train_loss = 1.889, time/batch = 0.586\n",
            "2059/18950 (epoch 5), train_loss = 1.884, time/batch = 0.579\n",
            "2060/18950 (epoch 5), train_loss = 1.870, time/batch = 0.576\n",
            "2061/18950 (epoch 5), train_loss = 1.886, time/batch = 0.573\n",
            "2062/18950 (epoch 5), train_loss = 1.897, time/batch = 0.576\n",
            "2063/18950 (epoch 5), train_loss = 1.926, time/batch = 0.589\n",
            "2064/18950 (epoch 5), train_loss = 1.904, time/batch = 0.588\n",
            "2065/18950 (epoch 5), train_loss = 1.894, time/batch = 0.580\n",
            "2066/18950 (epoch 5), train_loss = 1.873, time/batch = 0.586\n",
            "2067/18950 (epoch 5), train_loss = 1.904, time/batch = 0.574\n",
            "2068/18950 (epoch 5), train_loss = 1.876, time/batch = 0.640\n",
            "2069/18950 (epoch 5), train_loss = 1.859, time/batch = 0.751\n",
            "2070/18950 (epoch 5), train_loss = 1.864, time/batch = 0.757\n",
            "2071/18950 (epoch 5), train_loss = 1.885, time/batch = 0.757\n",
            "2072/18950 (epoch 5), train_loss = 1.921, time/batch = 0.772\n",
            "2073/18950 (epoch 5), train_loss = 1.880, time/batch = 0.769\n",
            "2074/18950 (epoch 5), train_loss = 1.895, time/batch = 0.806\n",
            "2075/18950 (epoch 5), train_loss = 1.905, time/batch = 0.773\n",
            "2076/18950 (epoch 5), train_loss = 1.899, time/batch = 0.755\n",
            "2077/18950 (epoch 5), train_loss = 1.907, time/batch = 0.758\n",
            "2078/18950 (epoch 5), train_loss = 1.885, time/batch = 0.773\n",
            "2079/18950 (epoch 5), train_loss = 1.900, time/batch = 0.760\n",
            "2080/18950 (epoch 5), train_loss = 1.892, time/batch = 0.756\n",
            "2081/18950 (epoch 5), train_loss = 1.883, time/batch = 0.764\n",
            "2082/18950 (epoch 5), train_loss = 1.897, time/batch = 0.756\n",
            "2083/18950 (epoch 5), train_loss = 1.885, time/batch = 0.762\n",
            "2084/18950 (epoch 5), train_loss = 1.913, time/batch = 0.616\n",
            "2085/18950 (epoch 5), train_loss = 1.887, time/batch = 0.574\n",
            "2086/18950 (epoch 5), train_loss = 1.913, time/batch = 0.600\n",
            "2087/18950 (epoch 5), train_loss = 1.893, time/batch = 0.593\n",
            "2088/18950 (epoch 5), train_loss = 1.892, time/batch = 0.576\n",
            "2089/18950 (epoch 5), train_loss = 1.861, time/batch = 0.586\n",
            "2090/18950 (epoch 5), train_loss = 1.852, time/batch = 0.590\n",
            "2091/18950 (epoch 5), train_loss = 1.880, time/batch = 0.589\n",
            "2092/18950 (epoch 5), train_loss = 1.881, time/batch = 0.584\n",
            "2093/18950 (epoch 5), train_loss = 1.897, time/batch = 0.579\n",
            "2094/18950 (epoch 5), train_loss = 1.911, time/batch = 0.601\n",
            "2095/18950 (epoch 5), train_loss = 1.894, time/batch = 0.571\n",
            "2096/18950 (epoch 5), train_loss = 1.895, time/batch = 0.575\n",
            "2097/18950 (epoch 5), train_loss = 1.861, time/batch = 0.584\n",
            "2098/18950 (epoch 5), train_loss = 1.858, time/batch = 0.618\n",
            "2099/18950 (epoch 5), train_loss = 1.878, time/batch = 0.586\n",
            "2100/18950 (epoch 5), train_loss = 1.882, time/batch = 0.590\n",
            "2101/18950 (epoch 5), train_loss = 1.885, time/batch = 0.674\n",
            "2102/18950 (epoch 5), train_loss = 1.902, time/batch = 0.784\n",
            "2103/18950 (epoch 5), train_loss = 1.887, time/batch = 0.746\n",
            "2104/18950 (epoch 5), train_loss = 1.911, time/batch = 0.756\n",
            "2105/18950 (epoch 5), train_loss = 1.903, time/batch = 0.797\n",
            "2106/18950 (epoch 5), train_loss = 1.897, time/batch = 0.749\n",
            "2107/18950 (epoch 5), train_loss = 1.886, time/batch = 0.755\n",
            "2108/18950 (epoch 5), train_loss = 1.902, time/batch = 0.731\n",
            "2109/18950 (epoch 5), train_loss = 1.889, time/batch = 0.736\n",
            "2110/18950 (epoch 5), train_loss = 1.883, time/batch = 0.777\n",
            "2111/18950 (epoch 5), train_loss = 1.901, time/batch = 0.755\n",
            "2112/18950 (epoch 5), train_loss = 1.885, time/batch = 0.745\n",
            "2113/18950 (epoch 5), train_loss = 1.880, time/batch = 0.758\n",
            "2114/18950 (epoch 5), train_loss = 1.870, time/batch = 0.758\n",
            "2115/18950 (epoch 5), train_loss = 1.878, time/batch = 0.746\n",
            "2116/18950 (epoch 5), train_loss = 1.879, time/batch = 0.784\n",
            "2117/18950 (epoch 5), train_loss = 1.872, time/batch = 0.584\n",
            "2118/18950 (epoch 5), train_loss = 1.888, time/batch = 0.581\n",
            "2119/18950 (epoch 5), train_loss = 1.885, time/batch = 0.573\n",
            "2120/18950 (epoch 5), train_loss = 1.887, time/batch = 0.576\n",
            "2121/18950 (epoch 5), train_loss = 1.895, time/batch = 0.586\n",
            "2122/18950 (epoch 5), train_loss = 1.894, time/batch = 0.591\n",
            "2123/18950 (epoch 5), train_loss = 1.888, time/batch = 0.573\n",
            "2124/18950 (epoch 5), train_loss = 1.898, time/batch = 0.600\n",
            "2125/18950 (epoch 5), train_loss = 1.905, time/batch = 0.582\n",
            "2126/18950 (epoch 5), train_loss = 1.903, time/batch = 0.582\n",
            "2127/18950 (epoch 5), train_loss = 1.896, time/batch = 0.608\n",
            "2128/18950 (epoch 5), train_loss = 1.898, time/batch = 0.621\n",
            "2129/18950 (epoch 5), train_loss = 1.893, time/batch = 0.580\n",
            "2130/18950 (epoch 5), train_loss = 1.871, time/batch = 0.590\n",
            "2131/18950 (epoch 5), train_loss = 1.891, time/batch = 0.578\n",
            "2132/18950 (epoch 5), train_loss = 1.879, time/batch = 0.583\n",
            "2133/18950 (epoch 5), train_loss = 1.881, time/batch = 0.590\n",
            "2134/18950 (epoch 5), train_loss = 1.882, time/batch = 0.715\n",
            "2135/18950 (epoch 5), train_loss = 1.867, time/batch = 0.773\n",
            "2136/18950 (epoch 5), train_loss = 1.894, time/batch = 0.787\n",
            "2137/18950 (epoch 5), train_loss = 1.912, time/batch = 0.757\n",
            "2138/18950 (epoch 5), train_loss = 1.890, time/batch = 0.769\n",
            "2139/18950 (epoch 5), train_loss = 1.904, time/batch = 0.725\n",
            "2140/18950 (epoch 5), train_loss = 1.899, time/batch = 0.767\n",
            "2141/18950 (epoch 5), train_loss = 1.893, time/batch = 0.778\n",
            "2142/18950 (epoch 5), train_loss = 1.895, time/batch = 0.764\n",
            "2143/18950 (epoch 5), train_loss = 1.883, time/batch = 0.770\n",
            "2144/18950 (epoch 5), train_loss = 1.918, time/batch = 0.753\n",
            "2145/18950 (epoch 5), train_loss = 1.863, time/batch = 0.757\n",
            "2146/18950 (epoch 5), train_loss = 1.896, time/batch = 0.779\n",
            "2147/18950 (epoch 5), train_loss = 1.893, time/batch = 0.746\n",
            "2148/18950 (epoch 5), train_loss = 1.905, time/batch = 0.756\n",
            "2149/18950 (epoch 5), train_loss = 1.882, time/batch = 0.734\n",
            "2150/18950 (epoch 5), train_loss = 1.867, time/batch = 0.589\n",
            "2151/18950 (epoch 5), train_loss = 1.896, time/batch = 0.607\n",
            "2152/18950 (epoch 5), train_loss = 1.900, time/batch = 0.588\n",
            "2153/18950 (epoch 5), train_loss = 1.870, time/batch = 0.591\n",
            "2154/18950 (epoch 5), train_loss = 1.857, time/batch = 0.582\n",
            "2155/18950 (epoch 5), train_loss = 1.876, time/batch = 0.591\n",
            "2156/18950 (epoch 5), train_loss = 1.875, time/batch = 0.578\n",
            "2157/18950 (epoch 5), train_loss = 1.859, time/batch = 0.601\n",
            "2158/18950 (epoch 5), train_loss = 1.878, time/batch = 0.586\n",
            "2159/18950 (epoch 5), train_loss = 1.874, time/batch = 0.589\n",
            "2160/18950 (epoch 5), train_loss = 1.875, time/batch = 0.612\n",
            "2161/18950 (epoch 5), train_loss = 1.890, time/batch = 0.595\n",
            "2162/18950 (epoch 5), train_loss = 1.878, time/batch = 0.609\n",
            "2163/18950 (epoch 5), train_loss = 1.886, time/batch = 0.603\n",
            "2164/18950 (epoch 5), train_loss = 1.863, time/batch = 0.605\n",
            "2165/18950 (epoch 5), train_loss = 1.861, time/batch = 0.590\n",
            "2166/18950 (epoch 5), train_loss = 1.879, time/batch = 0.658\n",
            "2167/18950 (epoch 5), train_loss = 1.846, time/batch = 0.757\n",
            "2168/18950 (epoch 5), train_loss = 1.875, time/batch = 0.780\n",
            "2169/18950 (epoch 5), train_loss = 1.865, time/batch = 0.762\n",
            "2170/18950 (epoch 5), train_loss = 1.873, time/batch = 0.769\n",
            "2171/18950 (epoch 5), train_loss = 1.855, time/batch = 0.782\n",
            "2172/18950 (epoch 5), train_loss = 1.871, time/batch = 0.775\n",
            "2173/18950 (epoch 5), train_loss = 1.893, time/batch = 0.749\n",
            "2174/18950 (epoch 5), train_loss = 1.899, time/batch = 0.764\n",
            "2175/18950 (epoch 5), train_loss = 1.898, time/batch = 0.765\n",
            "2176/18950 (epoch 5), train_loss = 1.887, time/batch = 0.768\n",
            "2177/18950 (epoch 5), train_loss = 1.871, time/batch = 0.766\n",
            "2178/18950 (epoch 5), train_loss = 1.886, time/batch = 0.758\n",
            "2179/18950 (epoch 5), train_loss = 1.863, time/batch = 0.758\n",
            "2180/18950 (epoch 5), train_loss = 1.834, time/batch = 0.830\n",
            "2181/18950 (epoch 5), train_loss = 1.864, time/batch = 0.766\n",
            "2182/18950 (epoch 5), train_loss = 1.880, time/batch = 0.590\n",
            "2183/18950 (epoch 5), train_loss = 1.887, time/batch = 0.587\n",
            "2184/18950 (epoch 5), train_loss = 1.891, time/batch = 0.590\n",
            "2185/18950 (epoch 5), train_loss = 1.882, time/batch = 0.584\n",
            "2186/18950 (epoch 5), train_loss = 1.856, time/batch = 0.585\n",
            "2187/18950 (epoch 5), train_loss = 1.857, time/batch = 0.614\n",
            "2188/18950 (epoch 5), train_loss = 1.912, time/batch = 0.591\n",
            "2189/18950 (epoch 5), train_loss = 1.927, time/batch = 0.596\n",
            "2190/18950 (epoch 5), train_loss = 1.889, time/batch = 0.599\n",
            "2191/18950 (epoch 5), train_loss = 1.884, time/batch = 0.596\n",
            "2192/18950 (epoch 5), train_loss = 1.898, time/batch = 0.581\n",
            "2193/18950 (epoch 5), train_loss = 1.871, time/batch = 0.580\n",
            "2194/18950 (epoch 5), train_loss = 1.870, time/batch = 0.584\n",
            "2195/18950 (epoch 5), train_loss = 1.870, time/batch = 0.620\n",
            "2196/18950 (epoch 5), train_loss = 1.872, time/batch = 0.609\n",
            "2197/18950 (epoch 5), train_loss = 1.852, time/batch = 0.609\n",
            "2198/18950 (epoch 5), train_loss = 1.845, time/batch = 0.593\n",
            "2199/18950 (epoch 5), train_loss = 1.862, time/batch = 0.749\n",
            "2200/18950 (epoch 5), train_loss = 1.902, time/batch = 0.754\n",
            "2201/18950 (epoch 5), train_loss = 1.878, time/batch = 0.757\n",
            "2202/18950 (epoch 5), train_loss = 1.854, time/batch = 0.755\n",
            "2203/18950 (epoch 5), train_loss = 1.873, time/batch = 0.750\n",
            "2204/18950 (epoch 5), train_loss = 1.881, time/batch = 0.753\n",
            "2205/18950 (epoch 5), train_loss = 1.854, time/batch = 0.753\n",
            "2206/18950 (epoch 5), train_loss = 1.841, time/batch = 0.734\n",
            "2207/18950 (epoch 5), train_loss = 1.860, time/batch = 0.751\n",
            "2208/18950 (epoch 5), train_loss = 1.862, time/batch = 0.725\n",
            "2209/18950 (epoch 5), train_loss = 1.899, time/batch = 0.737\n",
            "2210/18950 (epoch 5), train_loss = 1.855, time/batch = 0.769\n",
            "2211/18950 (epoch 5), train_loss = 1.903, time/batch = 0.754\n",
            "2212/18950 (epoch 5), train_loss = 1.868, time/batch = 0.759\n",
            "2213/18950 (epoch 5), train_loss = 1.847, time/batch = 0.747\n",
            "2214/18950 (epoch 5), train_loss = 1.848, time/batch = 0.701\n",
            "2215/18950 (epoch 5), train_loss = 1.836, time/batch = 0.580\n",
            "2216/18950 (epoch 5), train_loss = 1.858, time/batch = 0.584\n",
            "2217/18950 (epoch 5), train_loss = 1.859, time/batch = 0.591\n",
            "2218/18950 (epoch 5), train_loss = 1.874, time/batch = 0.586\n",
            "2219/18950 (epoch 5), train_loss = 1.870, time/batch = 0.591\n",
            "2220/18950 (epoch 5), train_loss = 1.844, time/batch = 0.578\n",
            "2221/18950 (epoch 5), train_loss = 1.850, time/batch = 0.589\n",
            "2222/18950 (epoch 5), train_loss = 1.874, time/batch = 0.581\n",
            "2223/18950 (epoch 5), train_loss = 1.860, time/batch = 0.590\n",
            "2224/18950 (epoch 5), train_loss = 1.857, time/batch = 0.600\n",
            "2225/18950 (epoch 5), train_loss = 1.833, time/batch = 0.587\n",
            "2226/18950 (epoch 5), train_loss = 1.836, time/batch = 0.607\n",
            "2227/18950 (epoch 5), train_loss = 1.856, time/batch = 0.588\n",
            "2228/18950 (epoch 5), train_loss = 1.855, time/batch = 0.589\n",
            "2229/18950 (epoch 5), train_loss = 1.861, time/batch = 0.591\n",
            "2230/18950 (epoch 5), train_loss = 1.857, time/batch = 0.589\n",
            "2231/18950 (epoch 5), train_loss = 1.857, time/batch = 0.673\n",
            "2232/18950 (epoch 5), train_loss = 1.844, time/batch = 0.767\n",
            "2233/18950 (epoch 5), train_loss = 1.877, time/batch = 0.758\n",
            "2234/18950 (epoch 5), train_loss = 1.841, time/batch = 0.741\n",
            "2235/18950 (epoch 5), train_loss = 1.856, time/batch = 0.754\n",
            "2236/18950 (epoch 5), train_loss = 1.847, time/batch = 0.788\n",
            "2237/18950 (epoch 5), train_loss = 1.865, time/batch = 0.737\n",
            "2238/18950 (epoch 5), train_loss = 1.857, time/batch = 0.773\n",
            "2239/18950 (epoch 5), train_loss = 1.854, time/batch = 0.748\n",
            "2240/18950 (epoch 5), train_loss = 1.857, time/batch = 0.784\n",
            "2241/18950 (epoch 5), train_loss = 1.841, time/batch = 0.744\n",
            "2242/18950 (epoch 5), train_loss = 1.857, time/batch = 0.787\n",
            "2243/18950 (epoch 5), train_loss = 1.855, time/batch = 0.760\n",
            "2244/18950 (epoch 5), train_loss = 1.847, time/batch = 0.754\n",
            "2245/18950 (epoch 5), train_loss = 1.852, time/batch = 0.769\n",
            "2246/18950 (epoch 5), train_loss = 1.858, time/batch = 0.774\n",
            "2247/18950 (epoch 5), train_loss = 1.867, time/batch = 0.610\n",
            "2248/18950 (epoch 5), train_loss = 1.842, time/batch = 0.596\n",
            "2249/18950 (epoch 5), train_loss = 1.859, time/batch = 0.600\n",
            "2250/18950 (epoch 5), train_loss = 1.857, time/batch = 0.628\n",
            "2251/18950 (epoch 5), train_loss = 1.853, time/batch = 0.601\n",
            "2252/18950 (epoch 5), train_loss = 1.895, time/batch = 0.593\n",
            "2253/18950 (epoch 5), train_loss = 1.845, time/batch = 0.583\n",
            "2254/18950 (epoch 5), train_loss = 1.862, time/batch = 0.577\n",
            "2255/18950 (epoch 5), train_loss = 1.844, time/batch = 0.597\n",
            "2256/18950 (epoch 5), train_loss = 1.867, time/batch = 0.590\n",
            "2257/18950 (epoch 5), train_loss = 1.867, time/batch = 0.574\n",
            "2258/18950 (epoch 5), train_loss = 1.863, time/batch = 0.603\n",
            "2259/18950 (epoch 5), train_loss = 1.853, time/batch = 0.588\n",
            "2260/18950 (epoch 5), train_loss = 1.874, time/batch = 0.599\n",
            "2261/18950 (epoch 5), train_loss = 1.892, time/batch = 0.584\n",
            "2262/18950 (epoch 5), train_loss = 1.869, time/batch = 0.592\n",
            "2263/18950 (epoch 5), train_loss = 1.847, time/batch = 0.595\n",
            "2264/18950 (epoch 5), train_loss = 1.838, time/batch = 0.764\n",
            "2265/18950 (epoch 5), train_loss = 1.838, time/batch = 0.758\n",
            "2266/18950 (epoch 5), train_loss = 1.841, time/batch = 0.762\n",
            "2267/18950 (epoch 5), train_loss = 1.847, time/batch = 0.772\n",
            "2268/18950 (epoch 5), train_loss = 1.860, time/batch = 0.744\n",
            "2269/18950 (epoch 5), train_loss = 1.824, time/batch = 0.753\n",
            "2270/18950 (epoch 5), train_loss = 1.851, time/batch = 0.760\n",
            "2271/18950 (epoch 5), train_loss = 1.861, time/batch = 0.792\n",
            "2272/18950 (epoch 5), train_loss = 1.857, time/batch = 0.747\n",
            "2273/18950 (epoch 5), train_loss = 1.866, time/batch = 0.758\n",
            "2274/18950 (epoch 6), train_loss = 1.900, time/batch = 0.832\n",
            "2275/18950 (epoch 6), train_loss = 1.857, time/batch = 0.754\n",
            "2276/18950 (epoch 6), train_loss = 1.870, time/batch = 0.736\n",
            "2277/18950 (epoch 6), train_loss = 1.854, time/batch = 0.576\n",
            "2278/18950 (epoch 6), train_loss = 1.879, time/batch = 0.596\n",
            "2279/18950 (epoch 6), train_loss = 1.839, time/batch = 0.604\n",
            "2280/18950 (epoch 6), train_loss = 1.824, time/batch = 0.579\n",
            "2281/18950 (epoch 6), train_loss = 1.831, time/batch = 0.577\n",
            "2282/18950 (epoch 6), train_loss = 1.835, time/batch = 0.579\n",
            "2283/18950 (epoch 6), train_loss = 1.839, time/batch = 0.589\n",
            "2284/18950 (epoch 6), train_loss = 1.848, time/batch = 0.591\n",
            "2285/18950 (epoch 6), train_loss = 1.834, time/batch = 0.577\n",
            "2286/18950 (epoch 6), train_loss = 1.855, time/batch = 0.599\n",
            "2287/18950 (epoch 6), train_loss = 1.828, time/batch = 0.576\n",
            "2288/18950 (epoch 6), train_loss = 1.838, time/batch = 0.586\n",
            "2289/18950 (epoch 6), train_loss = 1.838, time/batch = 0.591\n",
            "2290/18950 (epoch 6), train_loss = 1.851, time/batch = 0.580\n",
            "2291/18950 (epoch 6), train_loss = 1.839, time/batch = 0.590\n",
            "2292/18950 (epoch 6), train_loss = 1.826, time/batch = 0.585\n",
            "2293/18950 (epoch 6), train_loss = 1.835, time/batch = 0.642\n",
            "2294/18950 (epoch 6), train_loss = 1.825, time/batch = 0.759\n",
            "2295/18950 (epoch 6), train_loss = 1.826, time/batch = 0.743\n",
            "2296/18950 (epoch 6), train_loss = 1.840, time/batch = 0.730\n",
            "2297/18950 (epoch 6), train_loss = 1.837, time/batch = 0.783\n",
            "2298/18950 (epoch 6), train_loss = 1.834, time/batch = 0.754\n",
            "2299/18950 (epoch 6), train_loss = 1.837, time/batch = 0.763\n",
            "2300/18950 (epoch 6), train_loss = 1.839, time/batch = 0.765\n",
            "2301/18950 (epoch 6), train_loss = 1.840, time/batch = 0.791\n",
            "2302/18950 (epoch 6), train_loss = 1.802, time/batch = 0.761\n",
            "2303/18950 (epoch 6), train_loss = 1.849, time/batch = 0.754\n",
            "2304/18950 (epoch 6), train_loss = 1.833, time/batch = 0.754\n",
            "2305/18950 (epoch 6), train_loss = 1.848, time/batch = 0.755\n",
            "2306/18950 (epoch 6), train_loss = 1.851, time/batch = 0.747\n",
            "2307/18950 (epoch 6), train_loss = 1.831, time/batch = 0.757\n",
            "2308/18950 (epoch 6), train_loss = 1.834, time/batch = 0.744\n",
            "2309/18950 (epoch 6), train_loss = 1.841, time/batch = 0.679\n",
            "2310/18950 (epoch 6), train_loss = 1.825, time/batch = 0.599\n",
            "2311/18950 (epoch 6), train_loss = 1.857, time/batch = 0.594\n",
            "2312/18950 (epoch 6), train_loss = 1.824, time/batch = 0.594\n",
            "2313/18950 (epoch 6), train_loss = 1.827, time/batch = 0.610\n",
            "2314/18950 (epoch 6), train_loss = 1.840, time/batch = 0.591\n",
            "2315/18950 (epoch 6), train_loss = 1.825, time/batch = 0.564\n",
            "2316/18950 (epoch 6), train_loss = 1.855, time/batch = 0.577\n",
            "2317/18950 (epoch 6), train_loss = 1.854, time/batch = 0.584\n",
            "2318/18950 (epoch 6), train_loss = 1.837, time/batch = 0.582\n",
            "2319/18950 (epoch 6), train_loss = 1.844, time/batch = 0.573\n",
            "2320/18950 (epoch 6), train_loss = 1.866, time/batch = 0.586\n",
            "2321/18950 (epoch 6), train_loss = 1.836, time/batch = 0.606\n",
            "2322/18950 (epoch 6), train_loss = 1.823, time/batch = 0.594\n",
            "2323/18950 (epoch 6), train_loss = 1.846, time/batch = 0.604\n",
            "2324/18950 (epoch 6), train_loss = 1.827, time/batch = 0.588\n",
            "2325/18950 (epoch 6), train_loss = 1.822, time/batch = 0.588\n",
            "2326/18950 (epoch 6), train_loss = 1.836, time/batch = 0.664\n",
            "2327/18950 (epoch 6), train_loss = 1.842, time/batch = 0.753\n",
            "2328/18950 (epoch 6), train_loss = 1.848, time/batch = 0.768\n",
            "2329/18950 (epoch 6), train_loss = 1.859, time/batch = 0.762\n",
            "2330/18950 (epoch 6), train_loss = 1.852, time/batch = 0.759\n",
            "2331/18950 (epoch 6), train_loss = 1.843, time/batch = 0.745\n",
            "2332/18950 (epoch 6), train_loss = 1.814, time/batch = 0.778\n",
            "2333/18950 (epoch 6), train_loss = 1.814, time/batch = 0.760\n",
            "2334/18950 (epoch 6), train_loss = 1.821, time/batch = 0.736\n",
            "2335/18950 (epoch 6), train_loss = 1.839, time/batch = 0.802\n",
            "2336/18950 (epoch 6), train_loss = 1.823, time/batch = 0.766\n",
            "2337/18950 (epoch 6), train_loss = 1.839, time/batch = 0.778\n",
            "2338/18950 (epoch 6), train_loss = 1.853, time/batch = 0.754\n",
            "2339/18950 (epoch 6), train_loss = 1.863, time/batch = 0.757\n",
            "2340/18950 (epoch 6), train_loss = 1.848, time/batch = 0.748\n",
            "2341/18950 (epoch 6), train_loss = 1.831, time/batch = 0.750\n",
            "2342/18950 (epoch 6), train_loss = 1.832, time/batch = 0.631\n",
            "2343/18950 (epoch 6), train_loss = 1.855, time/batch = 0.600\n",
            "2344/18950 (epoch 6), train_loss = 1.872, time/batch = 0.577\n",
            "2345/18950 (epoch 6), train_loss = 1.849, time/batch = 0.580\n",
            "2346/18950 (epoch 6), train_loss = 1.832, time/batch = 0.580\n",
            "2347/18950 (epoch 6), train_loss = 1.851, time/batch = 0.576\n",
            "2348/18950 (epoch 6), train_loss = 1.817, time/batch = 0.578\n",
            "2349/18950 (epoch 6), train_loss = 1.823, time/batch = 0.592\n",
            "2350/18950 (epoch 6), train_loss = 1.815, time/batch = 0.575\n",
            "2351/18950 (epoch 6), train_loss = 1.810, time/batch = 0.568\n",
            "2352/18950 (epoch 6), train_loss = 1.818, time/batch = 0.590\n",
            "2353/18950 (epoch 6), train_loss = 1.830, time/batch = 0.571\n",
            "2354/18950 (epoch 6), train_loss = 1.824, time/batch = 0.588\n",
            "2355/18950 (epoch 6), train_loss = 1.847, time/batch = 0.578\n",
            "2356/18950 (epoch 6), train_loss = 1.838, time/batch = 0.572\n",
            "2357/18950 (epoch 6), train_loss = 1.859, time/batch = 0.568\n",
            "2358/18950 (epoch 6), train_loss = 1.835, time/batch = 0.561\n",
            "2359/18950 (epoch 6), train_loss = 1.828, time/batch = 0.734\n",
            "2360/18950 (epoch 6), train_loss = 1.873, time/batch = 0.780\n",
            "2361/18950 (epoch 6), train_loss = 1.847, time/batch = 0.747\n",
            "2362/18950 (epoch 6), train_loss = 1.833, time/batch = 0.754\n",
            "2363/18950 (epoch 6), train_loss = 1.823, time/batch = 0.748\n",
            "2364/18950 (epoch 6), train_loss = 1.855, time/batch = 0.749\n",
            "2365/18950 (epoch 6), train_loss = 1.841, time/batch = 0.751\n",
            "2366/18950 (epoch 6), train_loss = 1.826, time/batch = 0.748\n",
            "2367/18950 (epoch 6), train_loss = 1.859, time/batch = 0.764\n",
            "2368/18950 (epoch 6), train_loss = 1.851, time/batch = 0.706\n",
            "2369/18950 (epoch 6), train_loss = 1.843, time/batch = 0.749\n",
            "2370/18950 (epoch 6), train_loss = 1.839, time/batch = 0.742\n",
            "2371/18950 (epoch 6), train_loss = 1.853, time/batch = 0.754\n",
            "2372/18950 (epoch 6), train_loss = 1.835, time/batch = 0.748\n",
            "2373/18950 (epoch 6), train_loss = 1.827, time/batch = 0.750\n",
            "2374/18950 (epoch 6), train_loss = 1.862, time/batch = 0.746\n",
            "2375/18950 (epoch 6), train_loss = 1.843, time/batch = 0.594\n",
            "2376/18950 (epoch 6), train_loss = 1.833, time/batch = 0.567\n",
            "2377/18950 (epoch 6), train_loss = 1.839, time/batch = 0.584\n",
            "2378/18950 (epoch 6), train_loss = 1.840, time/batch = 0.571\n",
            "2379/18950 (epoch 6), train_loss = 1.822, time/batch = 0.589\n",
            "2380/18950 (epoch 6), train_loss = 1.834, time/batch = 0.569\n",
            "2381/18950 (epoch 6), train_loss = 1.823, time/batch = 0.577\n",
            "2382/18950 (epoch 6), train_loss = 1.822, time/batch = 0.584\n",
            "2383/18950 (epoch 6), train_loss = 1.833, time/batch = 0.588\n",
            "2384/18950 (epoch 6), train_loss = 1.852, time/batch = 0.572\n",
            "2385/18950 (epoch 6), train_loss = 1.845, time/batch = 0.568\n",
            "2386/18950 (epoch 6), train_loss = 1.818, time/batch = 0.567\n",
            "2387/18950 (epoch 6), train_loss = 1.819, time/batch = 0.581\n",
            "2388/18950 (epoch 6), train_loss = 1.823, time/batch = 0.584\n",
            "2389/18950 (epoch 6), train_loss = 1.821, time/batch = 0.583\n",
            "2390/18950 (epoch 6), train_loss = 1.855, time/batch = 0.596\n",
            "2391/18950 (epoch 6), train_loss = 1.848, time/batch = 0.571\n",
            "2392/18950 (epoch 6), train_loss = 1.843, time/batch = 0.670\n",
            "2393/18950 (epoch 6), train_loss = 1.839, time/batch = 0.736\n",
            "2394/18950 (epoch 6), train_loss = 1.810, time/batch = 0.751\n",
            "2395/18950 (epoch 6), train_loss = 1.820, time/batch = 0.747\n",
            "2396/18950 (epoch 6), train_loss = 1.854, time/batch = 0.744\n",
            "2397/18950 (epoch 6), train_loss = 1.832, time/batch = 0.738\n",
            "2398/18950 (epoch 6), train_loss = 1.834, time/batch = 0.747\n",
            "2399/18950 (epoch 6), train_loss = 1.817, time/batch = 0.760\n",
            "2400/18950 (epoch 6), train_loss = 1.830, time/batch = 0.749\n",
            "2401/18950 (epoch 6), train_loss = 1.832, time/batch = 0.740\n",
            "2402/18950 (epoch 6), train_loss = 1.811, time/batch = 0.729\n",
            "2403/18950 (epoch 6), train_loss = 1.835, time/batch = 0.767\n",
            "2404/18950 (epoch 6), train_loss = 1.810, time/batch = 0.745\n",
            "2405/18950 (epoch 6), train_loss = 1.809, time/batch = 0.812\n",
            "2406/18950 (epoch 6), train_loss = 1.823, time/batch = 0.761\n",
            "2407/18950 (epoch 6), train_loss = 1.802, time/batch = 0.739\n",
            "2408/18950 (epoch 6), train_loss = 1.781, time/batch = 0.580\n",
            "2409/18950 (epoch 6), train_loss = 1.814, time/batch = 0.602\n",
            "2410/18950 (epoch 6), train_loss = 1.804, time/batch = 0.581\n",
            "2411/18950 (epoch 6), train_loss = 1.810, time/batch = 0.568\n",
            "2412/18950 (epoch 6), train_loss = 1.816, time/batch = 0.610\n",
            "2413/18950 (epoch 6), train_loss = 1.800, time/batch = 0.584\n",
            "2414/18950 (epoch 6), train_loss = 1.831, time/batch = 0.580\n",
            "2415/18950 (epoch 6), train_loss = 1.832, time/batch = 0.592\n",
            "2416/18950 (epoch 6), train_loss = 1.821, time/batch = 0.581\n",
            "2417/18950 (epoch 6), train_loss = 1.827, time/batch = 0.580\n",
            "2418/18950 (epoch 6), train_loss = 1.837, time/batch = 0.580\n",
            "2419/18950 (epoch 6), train_loss = 1.821, time/batch = 0.595\n",
            "2420/18950 (epoch 6), train_loss = 1.850, time/batch = 0.568\n",
            "2421/18950 (epoch 6), train_loss = 1.827, time/batch = 0.569\n",
            "2422/18950 (epoch 6), train_loss = 1.805, time/batch = 0.589\n",
            "2423/18950 (epoch 6), train_loss = 1.836, time/batch = 0.573\n",
            "2424/18950 (epoch 6), train_loss = 1.840, time/batch = 0.579\n",
            "2425/18950 (epoch 6), train_loss = 1.803, time/batch = 0.692\n",
            "2426/18950 (epoch 6), train_loss = 1.806, time/batch = 0.741\n",
            "2427/18950 (epoch 6), train_loss = 1.833, time/batch = 0.753\n",
            "2428/18950 (epoch 6), train_loss = 1.826, time/batch = 0.773\n",
            "2429/18950 (epoch 6), train_loss = 1.844, time/batch = 0.770\n",
            "2430/18950 (epoch 6), train_loss = 1.836, time/batch = 0.766\n",
            "2431/18950 (epoch 6), train_loss = 1.834, time/batch = 0.769\n",
            "2432/18950 (epoch 6), train_loss = 1.815, time/batch = 0.767\n",
            "2433/18950 (epoch 6), train_loss = 1.825, time/batch = 0.809\n",
            "2434/18950 (epoch 6), train_loss = 1.823, time/batch = 0.754\n",
            "2435/18950 (epoch 6), train_loss = 1.797, time/batch = 0.739\n",
            "2436/18950 (epoch 6), train_loss = 1.833, time/batch = 0.768\n",
            "2437/18950 (epoch 6), train_loss = 1.817, time/batch = 0.754\n",
            "2438/18950 (epoch 6), train_loss = 1.806, time/batch = 0.758\n",
            "2439/18950 (epoch 6), train_loss = 1.784, time/batch = 0.745\n",
            "2440/18950 (epoch 6), train_loss = 1.803, time/batch = 0.746\n",
            "2441/18950 (epoch 6), train_loss = 1.809, time/batch = 0.593\n",
            "2442/18950 (epoch 6), train_loss = 1.851, time/batch = 0.570\n",
            "2443/18950 (epoch 6), train_loss = 1.822, time/batch = 0.576\n",
            "2444/18950 (epoch 6), train_loss = 1.809, time/batch = 0.574\n",
            "2445/18950 (epoch 6), train_loss = 1.792, time/batch = 0.574\n",
            "2446/18950 (epoch 6), train_loss = 1.819, time/batch = 0.580\n",
            "2447/18950 (epoch 6), train_loss = 1.801, time/batch = 0.591\n",
            "2448/18950 (epoch 6), train_loss = 1.794, time/batch = 0.577\n",
            "2449/18950 (epoch 6), train_loss = 1.790, time/batch = 0.588\n",
            "2450/18950 (epoch 6), train_loss = 1.806, time/batch = 0.589\n",
            "2451/18950 (epoch 6), train_loss = 1.845, time/batch = 0.604\n",
            "2452/18950 (epoch 6), train_loss = 1.803, time/batch = 0.596\n",
            "2453/18950 (epoch 6), train_loss = 1.823, time/batch = 0.577\n",
            "2454/18950 (epoch 6), train_loss = 1.825, time/batch = 0.608\n",
            "2455/18950 (epoch 6), train_loss = 1.813, time/batch = 0.585\n",
            "2456/18950 (epoch 6), train_loss = 1.824, time/batch = 0.592\n",
            "2457/18950 (epoch 6), train_loss = 1.804, time/batch = 0.600\n",
            "2458/18950 (epoch 6), train_loss = 1.822, time/batch = 0.719\n",
            "2459/18950 (epoch 6), train_loss = 1.815, time/batch = 0.743\n",
            "2460/18950 (epoch 6), train_loss = 1.800, time/batch = 0.759\n",
            "2461/18950 (epoch 6), train_loss = 1.818, time/batch = 0.771\n",
            "2462/18950 (epoch 6), train_loss = 1.806, time/batch = 0.752\n",
            "2463/18950 (epoch 6), train_loss = 1.836, time/batch = 0.769\n",
            "2464/18950 (epoch 6), train_loss = 1.809, time/batch = 0.768\n",
            "2465/18950 (epoch 6), train_loss = 1.837, time/batch = 0.767\n",
            "2466/18950 (epoch 6), train_loss = 1.811, time/batch = 0.771\n",
            "2467/18950 (epoch 6), train_loss = 1.818, time/batch = 0.732\n",
            "2468/18950 (epoch 6), train_loss = 1.787, time/batch = 0.775\n",
            "2469/18950 (epoch 6), train_loss = 1.778, time/batch = 0.752\n",
            "2470/18950 (epoch 6), train_loss = 1.807, time/batch = 0.757\n",
            "2471/18950 (epoch 6), train_loss = 1.794, time/batch = 0.705\n",
            "2472/18950 (epoch 6), train_loss = 1.820, time/batch = 0.741\n",
            "2473/18950 (epoch 6), train_loss = 1.839, time/batch = 0.756\n",
            "2474/18950 (epoch 6), train_loss = 1.821, time/batch = 0.609\n",
            "2475/18950 (epoch 6), train_loss = 1.821, time/batch = 0.578\n",
            "2476/18950 (epoch 6), train_loss = 1.786, time/batch = 0.584\n",
            "2477/18950 (epoch 6), train_loss = 1.788, time/batch = 0.573\n",
            "2478/18950 (epoch 6), train_loss = 1.796, time/batch = 0.594\n",
            "2479/18950 (epoch 6), train_loss = 1.806, time/batch = 0.585\n",
            "2480/18950 (epoch 6), train_loss = 1.813, time/batch = 0.577\n",
            "2481/18950 (epoch 6), train_loss = 1.826, time/batch = 0.592\n",
            "2482/18950 (epoch 6), train_loss = 1.818, time/batch = 0.580\n",
            "2483/18950 (epoch 6), train_loss = 1.845, time/batch = 0.571\n",
            "2484/18950 (epoch 6), train_loss = 1.829, time/batch = 0.573\n",
            "2485/18950 (epoch 6), train_loss = 1.818, time/batch = 0.573\n",
            "2486/18950 (epoch 6), train_loss = 1.809, time/batch = 0.582\n",
            "2487/18950 (epoch 6), train_loss = 1.818, time/batch = 0.573\n",
            "2488/18950 (epoch 6), train_loss = 1.816, time/batch = 0.572\n",
            "2489/18950 (epoch 6), train_loss = 1.806, time/batch = 0.583\n",
            "2490/18950 (epoch 6), train_loss = 1.826, time/batch = 0.575\n",
            "2491/18950 (epoch 6), train_loss = 1.808, time/batch = 0.677\n",
            "2492/18950 (epoch 6), train_loss = 1.795, time/batch = 0.741\n",
            "2493/18950 (epoch 6), train_loss = 1.795, time/batch = 0.756\n",
            "2494/18950 (epoch 6), train_loss = 1.807, time/batch = 0.743\n",
            "2495/18950 (epoch 6), train_loss = 1.804, time/batch = 0.771\n",
            "2496/18950 (epoch 6), train_loss = 1.800, time/batch = 0.734\n",
            "2497/18950 (epoch 6), train_loss = 1.808, time/batch = 0.767\n",
            "2498/18950 (epoch 6), train_loss = 1.811, time/batch = 0.750\n",
            "2499/18950 (epoch 6), train_loss = 1.805, time/batch = 0.774\n",
            "2500/18950 (epoch 6), train_loss = 1.822, time/batch = 0.739\n",
            "2501/18950 (epoch 6), train_loss = 1.824, time/batch = 0.756\n",
            "2502/18950 (epoch 6), train_loss = 1.815, time/batch = 0.771\n",
            "2503/18950 (epoch 6), train_loss = 1.827, time/batch = 0.740\n",
            "2504/18950 (epoch 6), train_loss = 1.823, time/batch = 0.740\n",
            "2505/18950 (epoch 6), train_loss = 1.833, time/batch = 0.754\n",
            "2506/18950 (epoch 6), train_loss = 1.820, time/batch = 0.719\n",
            "2507/18950 (epoch 6), train_loss = 1.818, time/batch = 0.592\n",
            "2508/18950 (epoch 6), train_loss = 1.817, time/batch = 0.583\n",
            "2509/18950 (epoch 6), train_loss = 1.791, time/batch = 0.572\n",
            "2510/18950 (epoch 6), train_loss = 1.817, time/batch = 0.584\n",
            "2511/18950 (epoch 6), train_loss = 1.799, time/batch = 0.579\n",
            "2512/18950 (epoch 6), train_loss = 1.804, time/batch = 0.603\n",
            "2513/18950 (epoch 6), train_loss = 1.804, time/batch = 0.576\n",
            "2514/18950 (epoch 6), train_loss = 1.794, time/batch = 0.564\n",
            "2515/18950 (epoch 6), train_loss = 1.814, time/batch = 0.579\n",
            "2516/18950 (epoch 6), train_loss = 1.833, time/batch = 0.593\n",
            "2517/18950 (epoch 6), train_loss = 1.819, time/batch = 0.589\n",
            "2518/18950 (epoch 6), train_loss = 1.824, time/batch = 0.599\n",
            "2519/18950 (epoch 6), train_loss = 1.821, time/batch = 0.579\n",
            "2520/18950 (epoch 6), train_loss = 1.822, time/batch = 0.593\n",
            "2521/18950 (epoch 6), train_loss = 1.820, time/batch = 0.585\n",
            "2522/18950 (epoch 6), train_loss = 1.805, time/batch = 0.603\n",
            "2523/18950 (epoch 6), train_loss = 1.836, time/batch = 0.580\n",
            "2524/18950 (epoch 6), train_loss = 1.784, time/batch = 0.710\n",
            "2525/18950 (epoch 6), train_loss = 1.826, time/batch = 0.781\n",
            "2526/18950 (epoch 6), train_loss = 1.817, time/batch = 0.755\n",
            "2527/18950 (epoch 6), train_loss = 1.837, time/batch = 0.758\n",
            "2528/18950 (epoch 6), train_loss = 1.805, time/batch = 0.768\n",
            "2529/18950 (epoch 6), train_loss = 1.796, time/batch = 0.748\n",
            "2530/18950 (epoch 6), train_loss = 1.816, time/batch = 0.743\n",
            "2531/18950 (epoch 6), train_loss = 1.827, time/batch = 0.756\n",
            "2532/18950 (epoch 6), train_loss = 1.796, time/batch = 0.747\n",
            "2533/18950 (epoch 6), train_loss = 1.794, time/batch = 0.756\n",
            "2534/18950 (epoch 6), train_loss = 1.801, time/batch = 0.741\n",
            "2535/18950 (epoch 6), train_loss = 1.798, time/batch = 0.742\n",
            "2536/18950 (epoch 6), train_loss = 1.791, time/batch = 0.763\n",
            "2537/18950 (epoch 6), train_loss = 1.800, time/batch = 0.757\n",
            "2538/18950 (epoch 6), train_loss = 1.799, time/batch = 0.748\n",
            "2539/18950 (epoch 6), train_loss = 1.793, time/batch = 0.743\n",
            "2540/18950 (epoch 6), train_loss = 1.806, time/batch = 0.584\n",
            "2541/18950 (epoch 6), train_loss = 1.797, time/batch = 0.595\n",
            "2542/18950 (epoch 6), train_loss = 1.809, time/batch = 0.602\n",
            "2543/18950 (epoch 6), train_loss = 1.783, time/batch = 0.578\n",
            "2544/18950 (epoch 6), train_loss = 1.785, time/batch = 0.581\n",
            "2545/18950 (epoch 6), train_loss = 1.805, time/batch = 0.583\n",
            "2546/18950 (epoch 6), train_loss = 1.767, time/batch = 0.585\n",
            "2547/18950 (epoch 6), train_loss = 1.797, time/batch = 0.581\n",
            "2548/18950 (epoch 6), train_loss = 1.788, time/batch = 0.606\n",
            "2549/18950 (epoch 6), train_loss = 1.795, time/batch = 0.578\n",
            "2550/18950 (epoch 6), train_loss = 1.778, time/batch = 0.584\n",
            "2551/18950 (epoch 6), train_loss = 1.784, time/batch = 0.583\n",
            "2552/18950 (epoch 6), train_loss = 1.825, time/batch = 0.596\n",
            "2553/18950 (epoch 6), train_loss = 1.825, time/batch = 0.612\n",
            "2554/18950 (epoch 6), train_loss = 1.826, time/batch = 0.604\n",
            "2555/18950 (epoch 6), train_loss = 1.811, time/batch = 0.583\n",
            "2556/18950 (epoch 6), train_loss = 1.796, time/batch = 0.638\n",
            "2557/18950 (epoch 6), train_loss = 1.812, time/batch = 0.793\n",
            "2558/18950 (epoch 6), train_loss = 1.789, time/batch = 0.755\n",
            "2559/18950 (epoch 6), train_loss = 1.762, time/batch = 0.748\n",
            "2560/18950 (epoch 6), train_loss = 1.784, time/batch = 0.746\n",
            "2561/18950 (epoch 6), train_loss = 1.806, time/batch = 0.770\n",
            "2562/18950 (epoch 6), train_loss = 1.814, time/batch = 0.713\n",
            "2563/18950 (epoch 6), train_loss = 1.817, time/batch = 0.789\n",
            "2564/18950 (epoch 6), train_loss = 1.807, time/batch = 0.761\n",
            "2565/18950 (epoch 6), train_loss = 1.777, time/batch = 0.763\n",
            "2566/18950 (epoch 6), train_loss = 1.790, time/batch = 0.755\n",
            "2567/18950 (epoch 6), train_loss = 1.831, time/batch = 0.728\n",
            "2568/18950 (epoch 6), train_loss = 1.852, time/batch = 0.760\n",
            "2569/18950 (epoch 6), train_loss = 1.811, time/batch = 0.757\n",
            "2570/18950 (epoch 6), train_loss = 1.810, time/batch = 0.772\n",
            "2571/18950 (epoch 6), train_loss = 1.818, time/batch = 0.757\n",
            "2572/18950 (epoch 6), train_loss = 1.799, time/batch = 0.654\n",
            "2573/18950 (epoch 6), train_loss = 1.796, time/batch = 0.598\n",
            "2574/18950 (epoch 6), train_loss = 1.796, time/batch = 0.599\n",
            "2575/18950 (epoch 6), train_loss = 1.799, time/batch = 0.573\n",
            "2576/18950 (epoch 6), train_loss = 1.784, time/batch = 0.584\n",
            "2577/18950 (epoch 6), train_loss = 1.776, time/batch = 0.577\n",
            "2578/18950 (epoch 6), train_loss = 1.793, time/batch = 0.608\n",
            "2579/18950 (epoch 6), train_loss = 1.822, time/batch = 0.584\n",
            "2580/18950 (epoch 6), train_loss = 1.806, time/batch = 0.576\n",
            "2581/18950 (epoch 6), train_loss = 1.781, time/batch = 0.583\n",
            "2582/18950 (epoch 6), train_loss = 1.804, time/batch = 0.586\n",
            "2583/18950 (epoch 6), train_loss = 1.812, time/batch = 0.585\n",
            "2584/18950 (epoch 6), train_loss = 1.779, time/batch = 0.587\n",
            "2585/18950 (epoch 6), train_loss = 1.772, time/batch = 0.594\n",
            "2586/18950 (epoch 6), train_loss = 1.783, time/batch = 0.586\n",
            "2587/18950 (epoch 6), train_loss = 1.793, time/batch = 0.591\n",
            "2588/18950 (epoch 6), train_loss = 1.822, time/batch = 0.591\n",
            "2589/18950 (epoch 6), train_loss = 1.786, time/batch = 0.713\n",
            "2590/18950 (epoch 6), train_loss = 1.828, time/batch = 0.763\n",
            "2591/18950 (epoch 6), train_loss = 1.793, time/batch = 0.759\n",
            "2592/18950 (epoch 6), train_loss = 1.780, time/batch = 0.707\n",
            "2593/18950 (epoch 6), train_loss = 1.775, time/batch = 0.739\n",
            "2594/18950 (epoch 6), train_loss = 1.766, time/batch = 0.752\n",
            "2595/18950 (epoch 6), train_loss = 1.774, time/batch = 0.768\n",
            "2596/18950 (epoch 6), train_loss = 1.789, time/batch = 0.764\n",
            "2597/18950 (epoch 6), train_loss = 1.802, time/batch = 0.733\n",
            "2598/18950 (epoch 6), train_loss = 1.795, time/batch = 0.780\n",
            "2599/18950 (epoch 6), train_loss = 1.771, time/batch = 0.756\n",
            "2600/18950 (epoch 6), train_loss = 1.782, time/batch = 0.729\n",
            "2601/18950 (epoch 6), train_loss = 1.796, time/batch = 0.760\n",
            "2602/18950 (epoch 6), train_loss = 1.790, time/batch = 0.768\n",
            "2603/18950 (epoch 6), train_loss = 1.789, time/batch = 0.749\n",
            "2604/18950 (epoch 6), train_loss = 1.767, time/batch = 0.709\n",
            "2605/18950 (epoch 6), train_loss = 1.765, time/batch = 0.594\n",
            "2606/18950 (epoch 6), train_loss = 1.779, time/batch = 0.573\n",
            "2607/18950 (epoch 6), train_loss = 1.783, time/batch = 0.598\n",
            "2608/18950 (epoch 6), train_loss = 1.788, time/batch = 0.571\n",
            "2609/18950 (epoch 6), train_loss = 1.791, time/batch = 0.603\n",
            "2610/18950 (epoch 6), train_loss = 1.783, time/batch = 0.594\n",
            "2611/18950 (epoch 6), train_loss = 1.777, time/batch = 0.601\n",
            "2612/18950 (epoch 6), train_loss = 1.806, time/batch = 0.595\n",
            "2613/18950 (epoch 6), train_loss = 1.774, time/batch = 0.588\n",
            "2614/18950 (epoch 6), train_loss = 1.782, time/batch = 0.598\n",
            "2615/18950 (epoch 6), train_loss = 1.775, time/batch = 0.585\n",
            "2616/18950 (epoch 6), train_loss = 1.793, time/batch = 0.583\n",
            "2617/18950 (epoch 6), train_loss = 1.789, time/batch = 0.583\n",
            "2618/18950 (epoch 6), train_loss = 1.783, time/batch = 0.579\n",
            "2619/18950 (epoch 6), train_loss = 1.783, time/batch = 0.586\n",
            "2620/18950 (epoch 6), train_loss = 1.761, time/batch = 0.584\n",
            "2621/18950 (epoch 6), train_loss = 1.789, time/batch = 0.575\n",
            "2622/18950 (epoch 6), train_loss = 1.783, time/batch = 0.719\n",
            "2623/18950 (epoch 6), train_loss = 1.773, time/batch = 0.733\n",
            "2624/18950 (epoch 6), train_loss = 1.781, time/batch = 0.732\n",
            "2625/18950 (epoch 6), train_loss = 1.786, time/batch = 0.763\n",
            "2626/18950 (epoch 6), train_loss = 1.791, time/batch = 0.753\n",
            "2627/18950 (epoch 6), train_loss = 1.771, time/batch = 0.736\n",
            "2628/18950 (epoch 6), train_loss = 1.785, time/batch = 0.756\n",
            "2629/18950 (epoch 6), train_loss = 1.783, time/batch = 0.704\n",
            "2630/18950 (epoch 6), train_loss = 1.783, time/batch = 0.751\n",
            "2631/18950 (epoch 6), train_loss = 1.822, time/batch = 0.744\n",
            "2632/18950 (epoch 6), train_loss = 1.773, time/batch = 0.741\n",
            "2633/18950 (epoch 6), train_loss = 1.793, time/batch = 0.767\n",
            "2634/18950 (epoch 6), train_loss = 1.774, time/batch = 0.749\n",
            "2635/18950 (epoch 6), train_loss = 1.794, time/batch = 0.744\n",
            "2636/18950 (epoch 6), train_loss = 1.796, time/batch = 0.751\n",
            "2637/18950 (epoch 6), train_loss = 1.792, time/batch = 0.775\n",
            "2638/18950 (epoch 6), train_loss = 1.779, time/batch = 0.600\n",
            "2639/18950 (epoch 6), train_loss = 1.792, time/batch = 0.598\n",
            "2640/18950 (epoch 6), train_loss = 1.822, time/batch = 0.575\n",
            "2641/18950 (epoch 6), train_loss = 1.793, time/batch = 0.578\n",
            "2642/18950 (epoch 6), train_loss = 1.780, time/batch = 0.573\n",
            "2643/18950 (epoch 6), train_loss = 1.768, time/batch = 0.594\n",
            "2644/18950 (epoch 6), train_loss = 1.757, time/batch = 0.581\n",
            "2645/18950 (epoch 6), train_loss = 1.776, time/batch = 0.572\n",
            "2646/18950 (epoch 6), train_loss = 1.781, time/batch = 0.582\n",
            "2647/18950 (epoch 6), train_loss = 1.783, time/batch = 0.593\n",
            "2648/18950 (epoch 6), train_loss = 1.756, time/batch = 0.573\n",
            "2649/18950 (epoch 6), train_loss = 1.780, time/batch = 0.578\n",
            "2650/18950 (epoch 6), train_loss = 1.793, time/batch = 0.575\n",
            "2651/18950 (epoch 6), train_loss = 1.786, time/batch = 0.566\n",
            "2652/18950 (epoch 6), train_loss = 1.790, time/batch = 0.586\n",
            "2653/18950 (epoch 7), train_loss = 1.830, time/batch = 0.666\n",
            "2654/18950 (epoch 7), train_loss = 1.788, time/batch = 0.762\n",
            "2655/18950 (epoch 7), train_loss = 1.797, time/batch = 0.755\n",
            "2656/18950 (epoch 7), train_loss = 1.780, time/batch = 0.775\n",
            "2657/18950 (epoch 7), train_loss = 1.814, time/batch = 0.741\n",
            "2658/18950 (epoch 7), train_loss = 1.767, time/batch = 0.746\n",
            "2659/18950 (epoch 7), train_loss = 1.748, time/batch = 0.764\n",
            "2660/18950 (epoch 7), train_loss = 1.763, time/batch = 0.742\n",
            "2661/18950 (epoch 7), train_loss = 1.762, time/batch = 0.752\n",
            "2662/18950 (epoch 7), train_loss = 1.775, time/batch = 0.749\n",
            "2663/18950 (epoch 7), train_loss = 1.775, time/batch = 0.738\n",
            "2664/18950 (epoch 7), train_loss = 1.772, time/batch = 0.752\n",
            "2665/18950 (epoch 7), train_loss = 1.783, time/batch = 0.755\n",
            "2666/18950 (epoch 7), train_loss = 1.760, time/batch = 0.736\n",
            "2667/18950 (epoch 7), train_loss = 1.763, time/batch = 0.713\n",
            "2668/18950 (epoch 7), train_loss = 1.769, time/batch = 0.751\n",
            "2669/18950 (epoch 7), train_loss = 1.784, time/batch = 0.562\n",
            "2670/18950 (epoch 7), train_loss = 1.766, time/batch = 0.574\n",
            "2671/18950 (epoch 7), train_loss = 1.751, time/batch = 0.578\n",
            "2672/18950 (epoch 7), train_loss = 1.759, time/batch = 0.587\n",
            "2673/18950 (epoch 7), train_loss = 1.754, time/batch = 0.572\n",
            "2674/18950 (epoch 7), train_loss = 1.759, time/batch = 0.568\n",
            "2675/18950 (epoch 7), train_loss = 1.765, time/batch = 0.589\n",
            "2676/18950 (epoch 7), train_loss = 1.771, time/batch = 0.577\n",
            "2677/18950 (epoch 7), train_loss = 1.757, time/batch = 0.573\n",
            "2678/18950 (epoch 7), train_loss = 1.762, time/batch = 0.594\n",
            "2679/18950 (epoch 7), train_loss = 1.767, time/batch = 0.606\n",
            "2680/18950 (epoch 7), train_loss = 1.767, time/batch = 0.591\n",
            "2681/18950 (epoch 7), train_loss = 1.739, time/batch = 0.586\n",
            "2682/18950 (epoch 7), train_loss = 1.772, time/batch = 0.577\n",
            "2683/18950 (epoch 7), train_loss = 1.759, time/batch = 0.572\n",
            "2684/18950 (epoch 7), train_loss = 1.776, time/batch = 0.575\n",
            "2685/18950 (epoch 7), train_loss = 1.773, time/batch = 0.571\n",
            "2686/18950 (epoch 7), train_loss = 1.760, time/batch = 0.711\n",
            "2687/18950 (epoch 7), train_loss = 1.763, time/batch = 0.750\n",
            "2688/18950 (epoch 7), train_loss = 1.772, time/batch = 0.715\n",
            "2689/18950 (epoch 7), train_loss = 1.754, time/batch = 0.744\n",
            "2690/18950 (epoch 7), train_loss = 1.789, time/batch = 0.736\n",
            "2691/18950 (epoch 7), train_loss = 1.742, time/batch = 0.740\n",
            "2692/18950 (epoch 7), train_loss = 1.757, time/batch = 0.744\n",
            "2693/18950 (epoch 7), train_loss = 1.773, time/batch = 0.733\n",
            "2694/18950 (epoch 7), train_loss = 1.760, time/batch = 0.746\n",
            "2695/18950 (epoch 7), train_loss = 1.785, time/batch = 0.750\n",
            "2696/18950 (epoch 7), train_loss = 1.782, time/batch = 0.766\n",
            "2697/18950 (epoch 7), train_loss = 1.765, time/batch = 0.762\n",
            "2698/18950 (epoch 7), train_loss = 1.770, time/batch = 0.766\n",
            "2699/18950 (epoch 7), train_loss = 1.796, time/batch = 0.753\n",
            "2700/18950 (epoch 7), train_loss = 1.758, time/batch = 0.767\n",
            "2701/18950 (epoch 7), train_loss = 1.751, time/batch = 0.749\n",
            "2702/18950 (epoch 7), train_loss = 1.784, time/batch = 0.630\n",
            "2703/18950 (epoch 7), train_loss = 1.753, time/batch = 0.574\n",
            "2704/18950 (epoch 7), train_loss = 1.756, time/batch = 0.576\n",
            "2705/18950 (epoch 7), train_loss = 1.769, time/batch = 0.600\n",
            "2706/18950 (epoch 7), train_loss = 1.775, time/batch = 0.581\n",
            "2707/18950 (epoch 7), train_loss = 1.778, time/batch = 0.578\n",
            "2708/18950 (epoch 7), train_loss = 1.787, time/batch = 0.590\n",
            "2709/18950 (epoch 7), train_loss = 1.789, time/batch = 0.599\n",
            "2710/18950 (epoch 7), train_loss = 1.768, time/batch = 0.583\n",
            "2711/18950 (epoch 7), train_loss = 1.746, time/batch = 0.573\n",
            "2712/18950 (epoch 7), train_loss = 1.745, time/batch = 0.574\n",
            "2713/18950 (epoch 7), train_loss = 1.757, time/batch = 0.583\n",
            "2714/18950 (epoch 7), train_loss = 1.774, time/batch = 0.576\n",
            "2715/18950 (epoch 7), train_loss = 1.755, time/batch = 0.567\n",
            "2716/18950 (epoch 7), train_loss = 1.772, time/batch = 0.570\n",
            "2717/18950 (epoch 7), train_loss = 1.783, time/batch = 0.568\n",
            "2718/18950 (epoch 7), train_loss = 1.795, time/batch = 0.568\n",
            "2719/18950 (epoch 7), train_loss = 1.774, time/batch = 0.637\n",
            "2720/18950 (epoch 7), train_loss = 1.762, time/batch = 0.748\n",
            "2721/18950 (epoch 7), train_loss = 1.770, time/batch = 0.740\n",
            "2722/18950 (epoch 7), train_loss = 1.792, time/batch = 0.736\n",
            "2723/18950 (epoch 7), train_loss = 1.797, time/batch = 0.742\n",
            "2724/18950 (epoch 7), train_loss = 1.780, time/batch = 0.798\n",
            "2725/18950 (epoch 7), train_loss = 1.761, time/batch = 0.758\n",
            "2726/18950 (epoch 7), train_loss = 1.782, time/batch = 0.777\n",
            "2727/18950 (epoch 7), train_loss = 1.750, time/batch = 0.752\n",
            "2728/18950 (epoch 7), train_loss = 1.755, time/batch = 0.742\n",
            "2729/18950 (epoch 7), train_loss = 1.747, time/batch = 0.749\n",
            "2730/18950 (epoch 7), train_loss = 1.744, time/batch = 0.745\n",
            "2731/18950 (epoch 7), train_loss = 1.747, time/batch = 0.746\n",
            "2732/18950 (epoch 7), train_loss = 1.765, time/batch = 0.774\n",
            "2733/18950 (epoch 7), train_loss = 1.752, time/batch = 0.743\n",
            "2734/18950 (epoch 7), train_loss = 1.777, time/batch = 0.763\n",
            "2735/18950 (epoch 7), train_loss = 1.765, time/batch = 0.718\n",
            "2736/18950 (epoch 7), train_loss = 1.791, time/batch = 0.588\n",
            "2737/18950 (epoch 7), train_loss = 1.776, time/batch = 0.589\n",
            "2738/18950 (epoch 7), train_loss = 1.769, time/batch = 0.586\n",
            "2739/18950 (epoch 7), train_loss = 1.816, time/batch = 0.602\n",
            "2740/18950 (epoch 7), train_loss = 1.776, time/batch = 0.602\n",
            "2741/18950 (epoch 7), train_loss = 1.766, time/batch = 0.593\n",
            "2742/18950 (epoch 7), train_loss = 1.757, time/batch = 0.582\n",
            "2743/18950 (epoch 7), train_loss = 1.787, time/batch = 0.586\n",
            "2744/18950 (epoch 7), train_loss = 1.772, time/batch = 0.589\n",
            "2745/18950 (epoch 7), train_loss = 1.761, time/batch = 0.589\n",
            "2746/18950 (epoch 7), train_loss = 1.785, time/batch = 0.591\n",
            "2747/18950 (epoch 7), train_loss = 1.789, time/batch = 0.591\n",
            "2748/18950 (epoch 7), train_loss = 1.775, time/batch = 0.583\n",
            "2749/18950 (epoch 7), train_loss = 1.772, time/batch = 0.592\n",
            "2750/18950 (epoch 7), train_loss = 1.789, time/batch = 0.589\n",
            "2751/18950 (epoch 7), train_loss = 1.771, time/batch = 0.583\n",
            "2752/18950 (epoch 7), train_loss = 1.762, time/batch = 0.603\n",
            "2753/18950 (epoch 7), train_loss = 1.791, time/batch = 0.778\n",
            "2754/18950 (epoch 7), train_loss = 1.787, time/batch = 0.734\n",
            "2755/18950 (epoch 7), train_loss = 1.762, time/batch = 0.730\n",
            "2756/18950 (epoch 7), train_loss = 1.777, time/batch = 0.748\n",
            "2757/18950 (epoch 7), train_loss = 1.772, time/batch = 0.747\n",
            "2758/18950 (epoch 7), train_loss = 1.757, time/batch = 0.752\n",
            "2759/18950 (epoch 7), train_loss = 1.769, time/batch = 0.756\n",
            "2760/18950 (epoch 7), train_loss = 1.751, time/batch = 0.754\n",
            "2761/18950 (epoch 7), train_loss = 1.757, time/batch = 0.745\n",
            "2762/18950 (epoch 7), train_loss = 1.765, time/batch = 0.751\n",
            "2763/18950 (epoch 7), train_loss = 1.784, time/batch = 0.755\n",
            "2764/18950 (epoch 7), train_loss = 1.776, time/batch = 0.765\n",
            "2765/18950 (epoch 7), train_loss = 1.756, time/batch = 0.732\n",
            "2766/18950 (epoch 7), train_loss = 1.752, time/batch = 0.752\n",
            "2767/18950 (epoch 7), train_loss = 1.749, time/batch = 0.767\n",
            "2768/18950 (epoch 7), train_loss = 1.757, time/batch = 0.709\n",
            "2769/18950 (epoch 7), train_loss = 1.791, time/batch = 0.584\n",
            "2770/18950 (epoch 7), train_loss = 1.774, time/batch = 0.584\n",
            "2771/18950 (epoch 7), train_loss = 1.780, time/batch = 0.585\n",
            "2772/18950 (epoch 7), train_loss = 1.773, time/batch = 0.571\n",
            "2773/18950 (epoch 7), train_loss = 1.742, time/batch = 0.593\n",
            "2774/18950 (epoch 7), train_loss = 1.745, time/batch = 0.588\n",
            "2775/18950 (epoch 7), train_loss = 1.789, time/batch = 0.567\n",
            "2776/18950 (epoch 7), train_loss = 1.766, time/batch = 0.594\n",
            "2777/18950 (epoch 7), train_loss = 1.764, time/batch = 0.603\n",
            "2778/18950 (epoch 7), train_loss = 1.747, time/batch = 0.588\n",
            "2779/18950 (epoch 7), train_loss = 1.772, time/batch = 0.568\n",
            "2780/18950 (epoch 7), train_loss = 1.766, time/batch = 0.599\n",
            "2781/18950 (epoch 7), train_loss = 1.742, time/batch = 0.579\n",
            "2782/18950 (epoch 7), train_loss = 1.765, time/batch = 0.594\n",
            "2783/18950 (epoch 7), train_loss = 1.742, time/batch = 0.607\n",
            "2784/18950 (epoch 7), train_loss = 1.742, time/batch = 0.569\n",
            "2785/18950 (epoch 7), train_loss = 1.757, time/batch = 0.652\n",
            "2786/18950 (epoch 7), train_loss = 1.735, time/batch = 0.735\n",
            "2787/18950 (epoch 7), train_loss = 1.714, time/batch = 0.749\n",
            "2788/18950 (epoch 7), train_loss = 1.751, time/batch = 0.704\n",
            "2789/18950 (epoch 7), train_loss = 1.745, time/batch = 0.748\n",
            "2790/18950 (epoch 7), train_loss = 1.745, time/batch = 0.757\n",
            "2791/18950 (epoch 7), train_loss = 1.749, time/batch = 0.738\n",
            "2792/18950 (epoch 7), train_loss = 1.739, time/batch = 0.766\n",
            "2793/18950 (epoch 7), train_loss = 1.762, time/batch = 0.751\n",
            "2794/18950 (epoch 7), train_loss = 1.764, time/batch = 0.746\n",
            "2795/18950 (epoch 7), train_loss = 1.751, time/batch = 0.731\n",
            "2796/18950 (epoch 7), train_loss = 1.760, time/batch = 0.827\n",
            "2797/18950 (epoch 7), train_loss = 1.771, time/batch = 0.747\n",
            "2798/18950 (epoch 7), train_loss = 1.757, time/batch = 0.752\n",
            "2799/18950 (epoch 7), train_loss = 1.785, time/batch = 0.788\n",
            "2800/18950 (epoch 7), train_loss = 1.762, time/batch = 0.751\n",
            "2801/18950 (epoch 7), train_loss = 1.748, time/batch = 0.663\n",
            "2802/18950 (epoch 7), train_loss = 1.765, time/batch = 0.586\n",
            "2803/18950 (epoch 7), train_loss = 1.776, time/batch = 0.571\n",
            "2804/18950 (epoch 7), train_loss = 1.744, time/batch = 0.576\n",
            "2805/18950 (epoch 7), train_loss = 1.743, time/batch = 0.579\n",
            "2806/18950 (epoch 7), train_loss = 1.763, time/batch = 0.572\n",
            "2807/18950 (epoch 7), train_loss = 1.765, time/batch = 0.580\n",
            "2808/18950 (epoch 7), train_loss = 1.782, time/batch = 0.593\n",
            "2809/18950 (epoch 7), train_loss = 1.774, time/batch = 0.572\n",
            "2810/18950 (epoch 7), train_loss = 1.766, time/batch = 0.570\n",
            "2811/18950 (epoch 7), train_loss = 1.741, time/batch = 0.576\n",
            "2812/18950 (epoch 7), train_loss = 1.758, time/batch = 0.569\n",
            "2813/18950 (epoch 7), train_loss = 1.752, time/batch = 0.573\n",
            "2814/18950 (epoch 7), train_loss = 1.729, time/batch = 0.591\n",
            "2815/18950 (epoch 7), train_loss = 1.764, time/batch = 0.571\n",
            "2816/18950 (epoch 7), train_loss = 1.751, time/batch = 0.597\n",
            "2817/18950 (epoch 7), train_loss = 1.745, time/batch = 0.599\n",
            "2818/18950 (epoch 7), train_loss = 1.719, time/batch = 0.585\n",
            "2819/18950 (epoch 7), train_loss = 1.734, time/batch = 0.735\n",
            "2820/18950 (epoch 7), train_loss = 1.745, time/batch = 0.779\n",
            "2821/18950 (epoch 7), train_loss = 1.788, time/batch = 0.730\n",
            "2822/18950 (epoch 7), train_loss = 1.761, time/batch = 0.722\n",
            "2823/18950 (epoch 7), train_loss = 1.753, time/batch = 0.729\n",
            "2824/18950 (epoch 7), train_loss = 1.734, time/batch = 0.758\n",
            "2825/18950 (epoch 7), train_loss = 1.752, time/batch = 0.700\n",
            "2826/18950 (epoch 7), train_loss = 1.734, time/batch = 0.803\n",
            "2827/18950 (epoch 7), train_loss = 1.720, time/batch = 0.768\n",
            "2828/18950 (epoch 7), train_loss = 1.723, time/batch = 0.760\n",
            "2829/18950 (epoch 7), train_loss = 1.741, time/batch = 0.712\n",
            "2830/18950 (epoch 7), train_loss = 1.777, time/batch = 0.756\n",
            "2831/18950 (epoch 7), train_loss = 1.736, time/batch = 0.791\n",
            "2832/18950 (epoch 7), train_loss = 1.760, time/batch = 0.746\n",
            "2833/18950 (epoch 7), train_loss = 1.762, time/batch = 0.758\n",
            "2834/18950 (epoch 7), train_loss = 1.744, time/batch = 0.745\n",
            "2835/18950 (epoch 7), train_loss = 1.764, time/batch = 0.582\n",
            "2836/18950 (epoch 7), train_loss = 1.742, time/batch = 0.575\n",
            "2837/18950 (epoch 7), train_loss = 1.765, time/batch = 0.576\n",
            "2838/18950 (epoch 7), train_loss = 1.749, time/batch = 0.586\n",
            "2839/18950 (epoch 7), train_loss = 1.738, time/batch = 0.574\n",
            "2840/18950 (epoch 7), train_loss = 1.755, time/batch = 0.575\n",
            "2841/18950 (epoch 7), train_loss = 1.740, time/batch = 0.583\n",
            "2842/18950 (epoch 7), train_loss = 1.773, time/batch = 0.568\n",
            "2843/18950 (epoch 7), train_loss = 1.742, time/batch = 0.585\n",
            "2844/18950 (epoch 7), train_loss = 1.780, time/batch = 0.582\n",
            "2845/18950 (epoch 7), train_loss = 1.752, time/batch = 0.577\n",
            "2846/18950 (epoch 7), train_loss = 1.755, time/batch = 0.587\n",
            "2847/18950 (epoch 7), train_loss = 1.727, time/batch = 0.572\n",
            "2848/18950 (epoch 7), train_loss = 1.711, time/batch = 0.592\n",
            "2849/18950 (epoch 7), train_loss = 1.740, time/batch = 0.566\n",
            "2850/18950 (epoch 7), train_loss = 1.733, time/batch = 0.591\n",
            "2851/18950 (epoch 7), train_loss = 1.750, time/batch = 0.565\n",
            "2852/18950 (epoch 7), train_loss = 1.773, time/batch = 0.690\n",
            "2853/18950 (epoch 7), train_loss = 1.758, time/batch = 0.740\n",
            "2854/18950 (epoch 7), train_loss = 1.754, time/batch = 0.750\n",
            "2855/18950 (epoch 7), train_loss = 1.723, time/batch = 0.742\n",
            "2856/18950 (epoch 7), train_loss = 1.725, time/batch = 0.766\n",
            "2857/18950 (epoch 7), train_loss = 1.735, time/batch = 0.744\n",
            "2858/18950 (epoch 7), train_loss = 1.745, time/batch = 0.750\n",
            "2859/18950 (epoch 7), train_loss = 1.751, time/batch = 0.717\n",
            "2860/18950 (epoch 7), train_loss = 1.762, time/batch = 0.742\n",
            "2861/18950 (epoch 7), train_loss = 1.751, time/batch = 0.741\n",
            "2862/18950 (epoch 7), train_loss = 1.777, time/batch = 0.731\n",
            "2863/18950 (epoch 7), train_loss = 1.762, time/batch = 0.755\n",
            "2864/18950 (epoch 7), train_loss = 1.755, time/batch = 0.735\n",
            "2865/18950 (epoch 7), train_loss = 1.749, time/batch = 0.756\n",
            "2866/18950 (epoch 7), train_loss = 1.754, time/batch = 0.753\n",
            "2867/18950 (epoch 7), train_loss = 1.758, time/batch = 0.778\n",
            "2868/18950 (epoch 7), train_loss = 1.741, time/batch = 0.590\n",
            "2869/18950 (epoch 7), train_loss = 1.759, time/batch = 0.571\n",
            "2870/18950 (epoch 7), train_loss = 1.748, time/batch = 0.567\n",
            "2871/18950 (epoch 7), train_loss = 1.737, time/batch = 0.571\n",
            "2872/18950 (epoch 7), train_loss = 1.736, time/batch = 0.585\n",
            "2873/18950 (epoch 7), train_loss = 1.741, time/batch = 0.561\n",
            "2874/18950 (epoch 7), train_loss = 1.740, time/batch = 0.591\n",
            "2875/18950 (epoch 7), train_loss = 1.738, time/batch = 0.605\n",
            "2876/18950 (epoch 7), train_loss = 1.748, time/batch = 0.596\n",
            "2877/18950 (epoch 7), train_loss = 1.749, time/batch = 0.624\n",
            "2878/18950 (epoch 7), train_loss = 1.747, time/batch = 0.611\n",
            "2879/18950 (epoch 7), train_loss = 1.765, time/batch = 0.594\n",
            "2880/18950 (epoch 7), train_loss = 1.764, time/batch = 0.595\n",
            "2881/18950 (epoch 7), train_loss = 1.752, time/batch = 0.591\n",
            "2882/18950 (epoch 7), train_loss = 1.765, time/batch = 0.593\n",
            "2883/18950 (epoch 7), train_loss = 1.767, time/batch = 0.607\n",
            "2884/18950 (epoch 7), train_loss = 1.770, time/batch = 0.583\n",
            "2885/18950 (epoch 7), train_loss = 1.761, time/batch = 0.707\n",
            "2886/18950 (epoch 7), train_loss = 1.764, time/batch = 0.754\n",
            "2887/18950 (epoch 7), train_loss = 1.752, time/batch = 0.763\n",
            "2888/18950 (epoch 7), train_loss = 1.725, time/batch = 0.768\n",
            "2889/18950 (epoch 7), train_loss = 1.754, time/batch = 0.714\n",
            "2890/18950 (epoch 7), train_loss = 1.742, time/batch = 0.732\n",
            "2891/18950 (epoch 7), train_loss = 1.741, time/batch = 0.748\n",
            "2892/18950 (epoch 7), train_loss = 1.739, time/batch = 0.724\n",
            "2893/18950 (epoch 7), train_loss = 1.732, time/batch = 0.748\n",
            "2894/18950 (epoch 7), train_loss = 1.748, time/batch = 0.770\n",
            "2895/18950 (epoch 7), train_loss = 1.781, time/batch = 0.743\n",
            "2896/18950 (epoch 7), train_loss = 1.753, time/batch = 0.735\n",
            "2897/18950 (epoch 7), train_loss = 1.764, time/batch = 0.744\n",
            "2898/18950 (epoch 7), train_loss = 1.766, time/batch = 0.741\n",
            "2899/18950 (epoch 7), train_loss = 1.759, time/batch = 0.758\n",
            "2900/18950 (epoch 7), train_loss = 1.755, time/batch = 0.773\n",
            "2901/18950 (epoch 7), train_loss = 1.740, time/batch = 0.588\n",
            "2902/18950 (epoch 7), train_loss = 1.775, time/batch = 0.613\n",
            "2903/18950 (epoch 7), train_loss = 1.726, time/batch = 0.580\n",
            "2904/18950 (epoch 7), train_loss = 1.769, time/batch = 0.601\n",
            "2905/18950 (epoch 7), train_loss = 1.759, time/batch = 0.581\n",
            "2906/18950 (epoch 7), train_loss = 1.775, time/batch = 0.570\n",
            "2907/18950 (epoch 7), train_loss = 1.745, time/batch = 0.579\n",
            "2908/18950 (epoch 7), train_loss = 1.724, time/batch = 0.585\n",
            "2909/18950 (epoch 7), train_loss = 1.763, time/batch = 0.576\n",
            "2910/18950 (epoch 7), train_loss = 1.768, time/batch = 0.562\n",
            "2911/18950 (epoch 7), train_loss = 1.733, time/batch = 0.597\n",
            "2912/18950 (epoch 7), train_loss = 1.729, time/batch = 0.577\n",
            "2913/18950 (epoch 7), train_loss = 1.743, time/batch = 0.585\n",
            "2914/18950 (epoch 7), train_loss = 1.735, time/batch = 0.579\n",
            "2915/18950 (epoch 7), train_loss = 1.731, time/batch = 0.581\n",
            "2916/18950 (epoch 7), train_loss = 1.747, time/batch = 0.567\n",
            "2917/18950 (epoch 7), train_loss = 1.741, time/batch = 0.598\n",
            "2918/18950 (epoch 7), train_loss = 1.735, time/batch = 0.653\n",
            "2919/18950 (epoch 7), train_loss = 1.752, time/batch = 0.818\n",
            "2920/18950 (epoch 7), train_loss = 1.736, time/batch = 0.811\n",
            "2921/18950 (epoch 7), train_loss = 1.752, time/batch = 0.774\n",
            "2922/18950 (epoch 7), train_loss = 1.722, time/batch = 0.765\n",
            "2923/18950 (epoch 7), train_loss = 1.723, time/batch = 0.753\n",
            "2924/18950 (epoch 7), train_loss = 1.743, time/batch = 0.748\n",
            "2925/18950 (epoch 7), train_loss = 1.708, time/batch = 0.753\n",
            "2926/18950 (epoch 7), train_loss = 1.742, time/batch = 0.746\n",
            "2927/18950 (epoch 7), train_loss = 1.726, time/batch = 0.773\n",
            "2928/18950 (epoch 7), train_loss = 1.733, time/batch = 0.763\n",
            "2929/18950 (epoch 7), train_loss = 1.720, time/batch = 0.760\n",
            "2930/18950 (epoch 7), train_loss = 1.730, time/batch = 0.743\n",
            "2931/18950 (epoch 7), train_loss = 1.763, time/batch = 0.778\n",
            "2932/18950 (epoch 7), train_loss = 1.764, time/batch = 0.754\n",
            "2933/18950 (epoch 7), train_loss = 1.770, time/batch = 0.737\n",
            "2934/18950 (epoch 7), train_loss = 1.746, time/batch = 0.600\n",
            "2935/18950 (epoch 7), train_loss = 1.741, time/batch = 0.581\n",
            "2936/18950 (epoch 7), train_loss = 1.758, time/batch = 0.593\n",
            "2937/18950 (epoch 7), train_loss = 1.731, time/batch = 0.580\n",
            "2938/18950 (epoch 7), train_loss = 1.703, time/batch = 0.582\n",
            "2939/18950 (epoch 7), train_loss = 1.728, time/batch = 0.567\n",
            "2940/18950 (epoch 7), train_loss = 1.749, time/batch = 0.590\n",
            "2941/18950 (epoch 7), train_loss = 1.755, time/batch = 0.578\n",
            "2942/18950 (epoch 7), train_loss = 1.764, time/batch = 0.589\n",
            "2943/18950 (epoch 7), train_loss = 1.749, time/batch = 0.571\n",
            "2944/18950 (epoch 7), train_loss = 1.719, time/batch = 0.587\n",
            "2945/18950 (epoch 7), train_loss = 1.733, time/batch = 0.609\n",
            "2946/18950 (epoch 7), train_loss = 1.773, time/batch = 0.579\n",
            "2947/18950 (epoch 7), train_loss = 1.798, time/batch = 0.586\n",
            "2948/18950 (epoch 7), train_loss = 1.755, time/batch = 0.581\n",
            "2949/18950 (epoch 7), train_loss = 1.746, time/batch = 0.578\n",
            "2950/18950 (epoch 7), train_loss = 1.763, time/batch = 0.592\n",
            "2951/18950 (epoch 7), train_loss = 1.742, time/batch = 0.658\n",
            "2952/18950 (epoch 7), train_loss = 1.744, time/batch = 0.739\n",
            "2953/18950 (epoch 7), train_loss = 1.739, time/batch = 0.758\n",
            "2954/18950 (epoch 7), train_loss = 1.743, time/batch = 0.735\n",
            "2955/18950 (epoch 7), train_loss = 1.726, time/batch = 0.723\n",
            "2956/18950 (epoch 7), train_loss = 1.723, time/batch = 0.724\n",
            "2957/18950 (epoch 7), train_loss = 1.739, time/batch = 0.778\n",
            "2958/18950 (epoch 7), train_loss = 1.767, time/batch = 0.782\n",
            "2959/18950 (epoch 7), train_loss = 1.748, time/batch = 0.752\n",
            "2960/18950 (epoch 7), train_loss = 1.717, time/batch = 0.736\n",
            "2961/18950 (epoch 7), train_loss = 1.747, time/batch = 0.726\n",
            "2962/18950 (epoch 7), train_loss = 1.759, time/batch = 0.761\n",
            "2963/18950 (epoch 7), train_loss = 1.726, time/batch = 0.755\n",
            "2964/18950 (epoch 7), train_loss = 1.708, time/batch = 0.769\n",
            "2965/18950 (epoch 7), train_loss = 1.729, time/batch = 0.724\n",
            "2966/18950 (epoch 7), train_loss = 1.734, time/batch = 0.771\n",
            "2967/18950 (epoch 7), train_loss = 1.764, time/batch = 0.652\n",
            "2968/18950 (epoch 7), train_loss = 1.730, time/batch = 0.602\n",
            "2969/18950 (epoch 7), train_loss = 1.774, time/batch = 0.587\n",
            "2970/18950 (epoch 7), train_loss = 1.741, time/batch = 0.605\n",
            "2971/18950 (epoch 7), train_loss = 1.727, time/batch = 0.598\n",
            "2972/18950 (epoch 7), train_loss = 1.720, time/batch = 0.587\n",
            "2973/18950 (epoch 7), train_loss = 1.710, time/batch = 0.583\n",
            "2974/18950 (epoch 7), train_loss = 1.725, time/batch = 0.575\n",
            "2975/18950 (epoch 7), train_loss = 1.736, time/batch = 0.578\n",
            "2976/18950 (epoch 7), train_loss = 1.742, time/batch = 0.577\n",
            "2977/18950 (epoch 7), train_loss = 1.734, time/batch = 0.578\n",
            "2978/18950 (epoch 7), train_loss = 1.718, time/batch = 0.590\n",
            "2979/18950 (epoch 7), train_loss = 1.719, time/batch = 0.585\n",
            "2980/18950 (epoch 7), train_loss = 1.743, time/batch = 0.588\n",
            "2981/18950 (epoch 7), train_loss = 1.732, time/batch = 0.596\n",
            "2982/18950 (epoch 7), train_loss = 1.730, time/batch = 0.595\n",
            "2983/18950 (epoch 7), train_loss = 1.705, time/batch = 0.597\n",
            "2984/18950 (epoch 7), train_loss = 1.714, time/batch = 0.664\n",
            "2985/18950 (epoch 7), train_loss = 1.726, time/batch = 0.748\n",
            "2986/18950 (epoch 7), train_loss = 1.730, time/batch = 0.752\n",
            "2987/18950 (epoch 7), train_loss = 1.733, time/batch = 0.791\n",
            "2988/18950 (epoch 7), train_loss = 1.731, time/batch = 0.768\n",
            "2989/18950 (epoch 7), train_loss = 1.726, time/batch = 0.732\n",
            "2990/18950 (epoch 7), train_loss = 1.720, time/batch = 0.753\n",
            "2991/18950 (epoch 7), train_loss = 1.752, time/batch = 0.745\n",
            "2992/18950 (epoch 7), train_loss = 1.718, time/batch = 0.749\n",
            "2993/18950 (epoch 7), train_loss = 1.725, time/batch = 0.746\n",
            "2994/18950 (epoch 7), train_loss = 1.722, time/batch = 0.765\n",
            "2995/18950 (epoch 7), train_loss = 1.740, time/batch = 0.746\n",
            "2996/18950 (epoch 7), train_loss = 1.734, time/batch = 0.782\n",
            "2997/18950 (epoch 7), train_loss = 1.723, time/batch = 0.773\n",
            "2998/18950 (epoch 7), train_loss = 1.723, time/batch = 0.788\n",
            "2999/18950 (epoch 7), train_loss = 1.708, time/batch = 0.755\n",
            "3000/18950 (epoch 7), train_loss = 1.728, time/batch = 0.636\n",
            "model saved to /content/drive/Shareddrives/Materials UNI/UNIPI/ISPR/Midterm3 Assignment_4/save/save1/model.ckpt\n",
            "3001/18950 (epoch 7), train_loss = 1.723, time/batch = 0.668\n",
            "3002/18950 (epoch 7), train_loss = 1.719, time/batch = 0.585\n",
            "3003/18950 (epoch 7), train_loss = 1.728, time/batch = 0.608\n",
            "3004/18950 (epoch 7), train_loss = 1.728, time/batch = 0.598\n",
            "3005/18950 (epoch 7), train_loss = 1.744, time/batch = 0.598\n",
            "3006/18950 (epoch 7), train_loss = 1.715, time/batch = 0.600\n",
            "3007/18950 (epoch 7), train_loss = 1.733, time/batch = 0.638\n",
            "3008/18950 (epoch 7), train_loss = 1.734, time/batch = 0.583\n",
            "3009/18950 (epoch 7), train_loss = 1.728, time/batch = 0.638\n",
            "3010/18950 (epoch 7), train_loss = 1.767, time/batch = 0.587\n",
            "3011/18950 (epoch 7), train_loss = 1.720, time/batch = 0.603\n",
            "3012/18950 (epoch 7), train_loss = 1.730, time/batch = 0.591\n",
            "3013/18950 (epoch 7), train_loss = 1.717, time/batch = 0.584\n",
            "3014/18950 (epoch 7), train_loss = 1.736, time/batch = 0.829\n",
            "3015/18950 (epoch 7), train_loss = 1.739, time/batch = 0.791\n",
            "3016/18950 (epoch 7), train_loss = 1.739, time/batch = 0.786\n",
            "3017/18950 (epoch 7), train_loss = 1.727, time/batch = 0.750\n",
            "3018/18950 (epoch 7), train_loss = 1.743, time/batch = 0.736\n",
            "3019/18950 (epoch 7), train_loss = 1.769, time/batch = 0.759\n",
            "3020/18950 (epoch 7), train_loss = 1.739, time/batch = 0.755\n",
            "3021/18950 (epoch 7), train_loss = 1.729, time/batch = 0.755\n",
            "3022/18950 (epoch 7), train_loss = 1.714, time/batch = 0.763\n",
            "3023/18950 (epoch 7), train_loss = 1.706, time/batch = 0.743\n",
            "3024/18950 (epoch 7), train_loss = 1.719, time/batch = 0.763\n",
            "3025/18950 (epoch 7), train_loss = 1.726, time/batch = 0.754\n",
            "3026/18950 (epoch 7), train_loss = 1.732, time/batch = 0.751\n",
            "3027/18950 (epoch 7), train_loss = 1.697, time/batch = 0.740\n",
            "3028/18950 (epoch 7), train_loss = 1.725, time/batch = 0.698\n",
            "3029/18950 (epoch 7), train_loss = 1.736, time/batch = 0.750\n",
            "3030/18950 (epoch 7), train_loss = 1.732, time/batch = 0.582\n",
            "3031/18950 (epoch 7), train_loss = 1.735, time/batch = 0.561\n",
            "3032/18950 (epoch 8), train_loss = 1.780, time/batch = 0.573\n",
            "3033/18950 (epoch 8), train_loss = 1.731, time/batch = 0.579\n",
            "3034/18950 (epoch 8), train_loss = 1.739, time/batch = 0.617\n",
            "3035/18950 (epoch 8), train_loss = 1.729, time/batch = 0.576\n",
            "3036/18950 (epoch 8), train_loss = 1.761, time/batch = 0.585\n",
            "3037/18950 (epoch 8), train_loss = 1.711, time/batch = 0.573\n",
            "3038/18950 (epoch 8), train_loss = 1.706, time/batch = 0.565\n",
            "3039/18950 (epoch 8), train_loss = 1.708, time/batch = 0.561\n",
            "3040/18950 (epoch 8), train_loss = 1.707, time/batch = 0.577\n",
            "3041/18950 (epoch 8), train_loss = 1.718, time/batch = 0.564\n",
            "3042/18950 (epoch 8), train_loss = 1.721, time/batch = 0.578\n",
            "3043/18950 (epoch 8), train_loss = 1.714, time/batch = 0.624\n",
            "3044/18950 (epoch 8), train_loss = 1.727, time/batch = 0.594\n",
            "3045/18950 (epoch 8), train_loss = 1.703, time/batch = 0.750\n",
            "3046/18950 (epoch 8), train_loss = 1.707, time/batch = 0.742\n",
            "3047/18950 (epoch 8), train_loss = 1.711, time/batch = 0.723\n",
            "3048/18950 (epoch 8), train_loss = 1.726, time/batch = 0.743\n",
            "3049/18950 (epoch 8), train_loss = 1.719, time/batch = 0.753\n",
            "3050/18950 (epoch 8), train_loss = 1.696, time/batch = 0.752\n",
            "3051/18950 (epoch 8), train_loss = 1.698, time/batch = 0.752\n",
            "3052/18950 (epoch 8), train_loss = 1.708, time/batch = 0.762\n",
            "3053/18950 (epoch 8), train_loss = 1.711, time/batch = 0.762\n",
            "3054/18950 (epoch 8), train_loss = 1.718, time/batch = 0.760\n",
            "3055/18950 (epoch 8), train_loss = 1.723, time/batch = 0.760\n",
            "3056/18950 (epoch 8), train_loss = 1.711, time/batch = 0.745\n",
            "3057/18950 (epoch 8), train_loss = 1.706, time/batch = 0.755\n",
            "3058/18950 (epoch 8), train_loss = 1.715, time/batch = 0.743\n",
            "3059/18950 (epoch 8), train_loss = 1.720, time/batch = 0.748\n",
            "3060/18950 (epoch 8), train_loss = 1.681, time/batch = 0.750\n",
            "3061/18950 (epoch 8), train_loss = 1.724, time/batch = 0.578\n",
            "3062/18950 (epoch 8), train_loss = 1.721, time/batch = 0.571\n",
            "3063/18950 (epoch 8), train_loss = 1.725, time/batch = 0.585\n",
            "3064/18950 (epoch 8), train_loss = 1.717, time/batch = 0.622\n",
            "3065/18950 (epoch 8), train_loss = 1.704, time/batch = 0.575\n",
            "3066/18950 (epoch 8), train_loss = 1.705, time/batch = 0.580\n",
            "3067/18950 (epoch 8), train_loss = 1.725, time/batch = 0.597\n",
            "3068/18950 (epoch 8), train_loss = 1.701, time/batch = 0.585\n",
            "3069/18950 (epoch 8), train_loss = 1.737, time/batch = 0.600\n",
            "3070/18950 (epoch 8), train_loss = 1.698, time/batch = 0.600\n",
            "3071/18950 (epoch 8), train_loss = 1.699, time/batch = 0.589\n",
            "3072/18950 (epoch 8), train_loss = 1.719, time/batch = 0.581\n",
            "3073/18950 (epoch 8), train_loss = 1.700, time/batch = 0.589\n",
            "3074/18950 (epoch 8), train_loss = 1.734, time/batch = 0.577\n",
            "3075/18950 (epoch 8), train_loss = 1.730, time/batch = 0.577\n",
            "3076/18950 (epoch 8), train_loss = 1.714, time/batch = 0.584\n",
            "3077/18950 (epoch 8), train_loss = 1.716, time/batch = 0.577\n",
            "3078/18950 (epoch 8), train_loss = 1.742, time/batch = 0.719\n",
            "3079/18950 (epoch 8), train_loss = 1.711, time/batch = 0.751\n",
            "3080/18950 (epoch 8), train_loss = 1.702, time/batch = 0.741\n",
            "3081/18950 (epoch 8), train_loss = 1.736, time/batch = 0.749\n",
            "3082/18950 (epoch 8), train_loss = 1.698, time/batch = 0.762\n",
            "3083/18950 (epoch 8), train_loss = 1.699, time/batch = 0.715\n",
            "3084/18950 (epoch 8), train_loss = 1.712, time/batch = 0.745\n",
            "3085/18950 (epoch 8), train_loss = 1.726, time/batch = 0.743\n",
            "3086/18950 (epoch 8), train_loss = 1.728, time/batch = 0.771\n",
            "3087/18950 (epoch 8), train_loss = 1.737, time/batch = 0.781\n",
            "3088/18950 (epoch 8), train_loss = 1.732, time/batch = 0.752\n",
            "3089/18950 (epoch 8), train_loss = 1.724, time/batch = 0.785\n",
            "3090/18950 (epoch 8), train_loss = 1.698, time/batch = 0.751\n",
            "3091/18950 (epoch 8), train_loss = 1.697, time/batch = 0.768\n",
            "3092/18950 (epoch 8), train_loss = 1.704, time/batch = 0.749\n",
            "3093/18950 (epoch 8), train_loss = 1.722, time/batch = 0.769\n",
            "3094/18950 (epoch 8), train_loss = 1.704, time/batch = 0.578\n",
            "3095/18950 (epoch 8), train_loss = 1.716, time/batch = 0.591\n",
            "3096/18950 (epoch 8), train_loss = 1.734, time/batch = 0.591\n",
            "3097/18950 (epoch 8), train_loss = 1.741, time/batch = 0.569\n",
            "3098/18950 (epoch 8), train_loss = 1.725, time/batch = 0.588\n",
            "3099/18950 (epoch 8), train_loss = 1.715, time/batch = 0.569\n",
            "3100/18950 (epoch 8), train_loss = 1.718, time/batch = 0.580\n",
            "3101/18950 (epoch 8), train_loss = 1.734, time/batch = 0.569\n",
            "3102/18950 (epoch 8), train_loss = 1.746, time/batch = 0.572\n",
            "3103/18950 (epoch 8), train_loss = 1.729, time/batch = 0.564\n",
            "3104/18950 (epoch 8), train_loss = 1.706, time/batch = 0.577\n",
            "3105/18950 (epoch 8), train_loss = 1.734, time/batch = 0.577\n",
            "3106/18950 (epoch 8), train_loss = 1.704, time/batch = 0.598\n",
            "3107/18950 (epoch 8), train_loss = 1.704, time/batch = 0.568\n",
            "3108/18950 (epoch 8), train_loss = 1.694, time/batch = 0.570\n",
            "3109/18950 (epoch 8), train_loss = 1.698, time/batch = 0.575\n",
            "3110/18950 (epoch 8), train_loss = 1.697, time/batch = 0.559\n",
            "3111/18950 (epoch 8), train_loss = 1.713, time/batch = 0.694\n",
            "3112/18950 (epoch 8), train_loss = 1.705, time/batch = 0.728\n",
            "3113/18950 (epoch 8), train_loss = 1.733, time/batch = 0.732\n",
            "3114/18950 (epoch 8), train_loss = 1.719, time/batch = 0.765\n",
            "3115/18950 (epoch 8), train_loss = 1.743, time/batch = 0.750\n",
            "3116/18950 (epoch 8), train_loss = 1.719, time/batch = 0.754\n",
            "3117/18950 (epoch 8), train_loss = 1.713, time/batch = 0.773\n",
            "3118/18950 (epoch 8), train_loss = 1.761, time/batch = 0.767\n",
            "3119/18950 (epoch 8), train_loss = 1.722, time/batch = 0.756\n",
            "3120/18950 (epoch 8), train_loss = 1.714, time/batch = 0.761\n",
            "3121/18950 (epoch 8), train_loss = 1.706, time/batch = 0.742\n",
            "3122/18950 (epoch 8), train_loss = 1.740, time/batch = 0.739\n",
            "3123/18950 (epoch 8), train_loss = 1.725, time/batch = 0.751\n",
            "3124/18950 (epoch 8), train_loss = 1.711, time/batch = 0.759\n",
            "3125/18950 (epoch 8), train_loss = 1.736, time/batch = 0.740\n",
            "3126/18950 (epoch 8), train_loss = 1.733, time/batch = 0.781\n",
            "3127/18950 (epoch 8), train_loss = 1.733, time/batch = 0.579\n",
            "3128/18950 (epoch 8), train_loss = 1.719, time/batch = 0.566\n",
            "3129/18950 (epoch 8), train_loss = 1.737, time/batch = 0.583\n",
            "3130/18950 (epoch 8), train_loss = 1.718, time/batch = 0.570\n",
            "3131/18950 (epoch 8), train_loss = 1.708, time/batch = 0.569\n",
            "3132/18950 (epoch 8), train_loss = 1.742, time/batch = 0.589\n",
            "3133/18950 (epoch 8), train_loss = 1.727, time/batch = 0.593\n",
            "3134/18950 (epoch 8), train_loss = 1.718, time/batch = 0.594\n",
            "3135/18950 (epoch 8), train_loss = 1.728, time/batch = 0.570\n",
            "3136/18950 (epoch 8), train_loss = 1.727, time/batch = 0.578\n",
            "3137/18950 (epoch 8), train_loss = 1.709, time/batch = 0.565\n",
            "3138/18950 (epoch 8), train_loss = 1.708, time/batch = 0.583\n",
            "3139/18950 (epoch 8), train_loss = 1.702, time/batch = 0.569\n",
            "3140/18950 (epoch 8), train_loss = 1.702, time/batch = 0.576\n",
            "3141/18950 (epoch 8), train_loss = 1.715, time/batch = 0.570\n",
            "3142/18950 (epoch 8), train_loss = 1.734, time/batch = 0.587\n",
            "3143/18950 (epoch 8), train_loss = 1.723, time/batch = 0.595\n",
            "3144/18950 (epoch 8), train_loss = 1.707, time/batch = 0.688\n",
            "3145/18950 (epoch 8), train_loss = 1.702, time/batch = 0.739\n",
            "3146/18950 (epoch 8), train_loss = 1.698, time/batch = 0.760\n",
            "3147/18950 (epoch 8), train_loss = 1.706, time/batch = 0.745\n",
            "3148/18950 (epoch 8), train_loss = 1.738, time/batch = 0.744\n",
            "3149/18950 (epoch 8), train_loss = 1.730, time/batch = 0.745\n",
            "3150/18950 (epoch 8), train_loss = 1.731, time/batch = 0.753\n",
            "3151/18950 (epoch 8), train_loss = 1.725, time/batch = 0.740\n",
            "3152/18950 (epoch 8), train_loss = 1.689, time/batch = 0.730\n",
            "3153/18950 (epoch 8), train_loss = 1.695, time/batch = 0.767\n",
            "3154/18950 (epoch 8), train_loss = 1.743, time/batch = 0.782\n",
            "3155/18950 (epoch 8), train_loss = 1.715, time/batch = 0.780\n",
            "3156/18950 (epoch 8), train_loss = 1.720, time/batch = 0.768\n",
            "3157/18950 (epoch 8), train_loss = 1.695, time/batch = 0.737\n",
            "3158/18950 (epoch 8), train_loss = 1.718, time/batch = 0.747\n",
            "3159/18950 (epoch 8), train_loss = 1.720, time/batch = 0.752\n",
            "3160/18950 (epoch 8), train_loss = 1.692, time/batch = 0.626\n",
            "3161/18950 (epoch 8), train_loss = 1.710, time/batch = 0.597\n",
            "3162/18950 (epoch 8), train_loss = 1.692, time/batch = 0.585\n",
            "3163/18950 (epoch 8), train_loss = 1.691, time/batch = 0.603\n",
            "3164/18950 (epoch 8), train_loss = 1.708, time/batch = 0.575\n",
            "3165/18950 (epoch 8), train_loss = 1.689, time/batch = 0.582\n",
            "3166/18950 (epoch 8), train_loss = 1.665, time/batch = 0.586\n",
            "3167/18950 (epoch 8), train_loss = 1.702, time/batch = 0.583\n",
            "3168/18950 (epoch 8), train_loss = 1.697, time/batch = 0.578\n",
            "3169/18950 (epoch 8), train_loss = 1.693, time/batch = 0.582\n",
            "3170/18950 (epoch 8), train_loss = 1.689, time/batch = 0.577\n",
            "3171/18950 (epoch 8), train_loss = 1.691, time/batch = 0.586\n",
            "3172/18950 (epoch 8), train_loss = 1.712, time/batch = 0.585\n",
            "3173/18950 (epoch 8), train_loss = 1.713, time/batch = 0.582\n",
            "3174/18950 (epoch 8), train_loss = 1.704, time/batch = 0.581\n",
            "3175/18950 (epoch 8), train_loss = 1.713, time/batch = 0.559\n",
            "3176/18950 (epoch 8), train_loss = 1.716, time/batch = 0.581\n",
            "3177/18950 (epoch 8), train_loss = 1.715, time/batch = 0.652\n",
            "3178/18950 (epoch 8), train_loss = 1.731, time/batch = 0.746\n",
            "3179/18950 (epoch 8), train_loss = 1.710, time/batch = 0.733\n",
            "3180/18950 (epoch 8), train_loss = 1.694, time/batch = 0.766\n",
            "3181/18950 (epoch 8), train_loss = 1.722, time/batch = 0.753\n",
            "3182/18950 (epoch 8), train_loss = 1.726, time/batch = 0.721\n",
            "3183/18950 (epoch 8), train_loss = 1.694, time/batch = 0.780\n",
            "3184/18950 (epoch 8), train_loss = 1.699, time/batch = 0.752\n",
            "3185/18950 (epoch 8), train_loss = 1.719, time/batch = 0.756\n",
            "3186/18950 (epoch 8), train_loss = 1.718, time/batch = 0.729\n",
            "3187/18950 (epoch 8), train_loss = 1.734, time/batch = 0.784\n",
            "3188/18950 (epoch 8), train_loss = 1.724, time/batch = 0.749\n",
            "3189/18950 (epoch 8), train_loss = 1.719, time/batch = 0.746\n",
            "3190/18950 (epoch 8), train_loss = 1.697, time/batch = 0.728\n",
            "3191/18950 (epoch 8), train_loss = 1.710, time/batch = 0.743\n",
            "3192/18950 (epoch 8), train_loss = 1.711, time/batch = 0.757\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine tuning \n",
        "Fine tune the best model on the Lercio Headlines Dataset, in such a way that we can perform transfer learning keeping the sintax of Italian."
      ],
      "metadata": {
        "id": "FkFHMTjySGxz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown \"1D0b_4pS2TdYcTMpruTv1fH1sejWs5kYX\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_QGpp6cKmbAj",
        "outputId": "3324b44b-9fbc-45b9-f625-29df5c9c3ad5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1D0b_4pS2TdYcTMpruTv1fH1sejWs5kYX\n",
            "To: /content/char-rnn-tensorflow/FinalModel.zip\n",
            "100% 175M/175M [00:02<00:00, 71.5MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "!unzip FinalModel.zip"
      ],
      "metadata": {
        "id": "gSSS7EIS0wkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "with open(\"FinalModel/config.pkl\",\"rb\") as file_handle:\n",
        "    retrieved_data = pickle.load(file_handle)\n",
        "    print(retrieved_data)\n",
        "retrieved_data.save_dir = './FinalModel'\n",
        "print(retrieved_data)\n",
        "with open(\"FinalModel/config.pkl\",\"wb\") as file:\n",
        "  pickle.dump(retrieved_data, file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LtUqG_xj1yiQ",
        "outputId": "4c891f6b-656d-4d16-b85a-e2890d3cd6e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(data_dir='./data/lercioheadlines/', save_dir='/content/drive/Shareddrives/Materials UNI/UNIPI/ISPR/Midterm3 Assignment_4/save/save0', log_dir='logs', save_every=1000, init_from=None, model='nas', rnn_size=256, num_layers=2, seq_length=130, batch_size=200, num_epochs=50, grad_clip=5.0, learning_rate=0.019, decay_rate=0.92, output_keep_prob=0.9999999999999999, input_keep_prob=0.8, vocab_size=145)\n",
            "Namespace(data_dir='./data/lercioheadlines/', save_dir='./FinalModel', log_dir='logs', save_every=1000, init_from=None, model='nas', rnn_size=256, num_layers=2, seq_length=130, batch_size=200, num_epochs=50, grad_clip=5.0, learning_rate=0.019, decay_rate=0.92, output_keep_prob=0.9999999999999999, input_keep_prob=0.8, vocab_size=145)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"FinalModel/checkpoint\") as file:\n",
        "  retrieved = file.read()\n",
        "print(retrieved)\n",
        "retrieved = retrieved.replace('model_checkpoint_path: \"/content/drive/Shareddrives/Materials UNI/UNIPI/ISPR/Midterm3 Assignment_4/save/save0/model.ckpt-849', 'model_checkpoint_path: \"/FinalModel/model.ckpt-849\"')\n",
        "retrieved = retrieved.replace(\"/content/drive/Shareddrives/Materials UNI/UNIPI/ISPR/Midterm3 Assignment_4/save/save0/model.ckpt-0\", \"FinalModel/model.ckpt-0\")\n",
        "retrieved = retrieved.replace('all_model_checkpoint_paths: \"/content/drive/Shareddrives/Materials UNI/UNIPI/ISPR/Midterm3 Assignment_4/save/save0/model.ckpt-849', 'all_model_checkpoint_paths: \"/FinalModel/model.ckpt-849\"')\n",
        "print(retrieved)\n",
        "with open(\"FinalModel/checkpoint\", \"w\") as file:\n",
        "  retrieved = file.write(retrieved)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tXR4qWuF58eb",
        "outputId": "0b979ceb-7c4b-4e03-8571-68143e4a2f79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_checkpoint_path: \"/content/drive/Shareddrives/Materials UNI/UNIPI/ISPR/Midterm3 Assignment_4/save/save0/model.ckpt-849\"\n",
            "all_model_checkpoint_paths: \"/content/drive/Shareddrives/Materials UNI/UNIPI/ISPR/Midterm3 Assignment_4/save/save0/model.ckpt-0\"\n",
            "all_model_checkpoint_paths: \"/content/drive/Shareddrives/Materials UNI/UNIPI/ISPR/Midterm3 Assignment_4/save/save0/model.ckpt-849\"\n",
            "\n",
            "model_checkpoint_path: \"/FinalModel/model.ckpt-849\"\"\n",
            "all_model_checkpoint_paths: \"FinalModel/model.ckpt-0\"\n",
            "all_model_checkpoint_paths: \"/FinalModel/model.ckpt-849\"\"\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "APO_p_clv8Lj",
        "outputId": "ad3f7749-9c93-45de-8d2d-7dc2c53f67eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "reading text file\n",
            "WARNING:tensorflow:From /content/char-rnn-tensorflow/model.py:36: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From /content/char-rnn-tensorflow/model.py:39: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/char-rnn-tensorflow/model.py:46: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/char-rnn-tensorflow/model.py:47: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/char-rnn-tensorflow/model.py:57: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/envs/myenv/lib/python3.6/site-packages/tensorflow_core/contrib/rnn/python/ops/rnn_cell.py:1529: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.add_weight` method instead.\n",
            "WARNING:tensorflow:From /content/char-rnn-tensorflow/model.py:86: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/envs/myenv/lib/python3.6/site-packages/tensorflow_core/python/ops/clip_ops.py:301: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /content/char-rnn-tensorflow/model.py:92: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/char-rnn-tensorflow/model.py:98: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/char-rnn-tensorflow/model.py:100: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "WARNING:tensorflow:From train.py:99: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2023-05-12 12:58:38.959019: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2023-05-12 12:58:38.982204: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-05-12 12:58:38.982457: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2023-05-12 12:58:38.982772: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2023-05-12 12:58:38.984644: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "2023-05-12 12:58:38.986204: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
            "2023-05-12 12:58:38.986600: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
            "2023-05-12 12:58:38.988764: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2023-05-12 12:58:38.990294: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2023-05-12 12:58:38.994698: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2023-05-12 12:58:38.994834: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-05-12 12:58:38.995130: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-05-12 12:58:38.995301: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2023-05-12 12:58:38.995731: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\n",
            "2023-05-12 12:58:39.005371: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000150000 Hz\n",
            "2023-05-12 12:58:39.005577: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55e7af80c070 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2023-05-12 12:58:39.005598: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2023-05-12 12:58:39.108531: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-05-12 12:58:39.108848: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55e7af6ba0c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2023-05-12 12:58:39.108874: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2023-05-12 12:58:39.109123: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-05-12 12:58:39.109300: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2023-05-12 12:58:39.109358: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2023-05-12 12:58:39.109376: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "2023-05-12 12:58:39.109418: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
            "2023-05-12 12:58:39.109436: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
            "2023-05-12 12:58:39.109453: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2023-05-12 12:58:39.109470: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2023-05-12 12:58:39.109487: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2023-05-12 12:58:39.109551: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-05-12 12:58:39.109738: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-05-12 12:58:39.109883: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2023-05-12 12:58:39.109941: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2023-05-12 12:58:39.110323: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2023-05-12 12:58:39.110351: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2023-05-12 12:58:39.110364: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2023-05-12 12:58:39.110480: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-05-12 12:58:39.110706: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-05-12 12:58:39.110878: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2023-05-12 12:58:39.110917: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14248 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "WARNING:tensorflow:From train.py:101: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
            "\n",
            "WARNING:tensorflow:From train.py:102: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "WARNING:tensorflow:From train.py:106: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From train.py:107: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "WARNING:tensorflow:From train.py:107: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From train.py:112: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "2023-05-12 12:59:06.054355: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "0/850 (epoch 0), train_loss = 4.959, time/batch = 22.100\n",
            "model saved to /content/drive/Shareddrives/Materials UNI/UNIPI/ISPR/Midterm3 Assignment_4/save/save0/model.ckpt\n",
            "1/850 (epoch 0), train_loss = 4.861, time/batch = 0.426\n",
            "2/850 (epoch 0), train_loss = 3.488, time/batch = 0.308\n",
            "3/850 (epoch 0), train_loss = 3.457, time/batch = 0.274\n",
            "4/850 (epoch 0), train_loss = 3.308, time/batch = 0.217\n",
            "5/850 (epoch 0), train_loss = 3.261, time/batch = 0.217\n",
            "6/850 (epoch 0), train_loss = 3.241, time/batch = 0.218\n",
            "7/850 (epoch 0), train_loss = 3.218, time/batch = 0.219\n",
            "8/850 (epoch 0), train_loss = 3.162, time/batch = 0.237\n",
            "9/850 (epoch 0), train_loss = 3.141, time/batch = 0.226\n",
            "10/850 (epoch 0), train_loss = 3.128, time/batch = 0.244\n",
            "11/850 (epoch 0), train_loss = 3.069, time/batch = 0.244\n",
            "12/850 (epoch 0), train_loss = 3.026, time/batch = 0.244\n",
            "13/850 (epoch 0), train_loss = 2.998, time/batch = 0.231\n",
            "14/850 (epoch 0), train_loss = 2.993, time/batch = 0.225\n",
            "15/850 (epoch 0), train_loss = 2.954, time/batch = 0.232\n",
            "16/850 (epoch 0), train_loss = 2.922, time/batch = 0.243\n",
            "17/850 (epoch 1), train_loss = 2.871, time/batch = 0.244\n",
            "18/850 (epoch 1), train_loss = 2.821, time/batch = 0.235\n",
            "19/850 (epoch 1), train_loss = 2.804, time/batch = 0.238\n",
            "20/850 (epoch 1), train_loss = 2.789, time/batch = 0.242\n",
            "21/850 (epoch 1), train_loss = 2.731, time/batch = 0.247\n",
            "22/850 (epoch 1), train_loss = 2.738, time/batch = 0.244\n",
            "23/850 (epoch 1), train_loss = 2.715, time/batch = 0.247\n",
            "24/850 (epoch 1), train_loss = 2.683, time/batch = 0.257\n",
            "25/850 (epoch 1), train_loss = 2.670, time/batch = 0.243\n",
            "26/850 (epoch 1), train_loss = 2.661, time/batch = 0.261\n",
            "27/850 (epoch 1), train_loss = 2.632, time/batch = 0.249\n",
            "28/850 (epoch 1), train_loss = 2.609, time/batch = 0.245\n",
            "29/850 (epoch 1), train_loss = 2.586, time/batch = 0.249\n",
            "30/850 (epoch 1), train_loss = 2.578, time/batch = 0.250\n",
            "31/850 (epoch 1), train_loss = 2.580, time/batch = 0.245\n",
            "32/850 (epoch 1), train_loss = 2.573, time/batch = 0.322\n",
            "33/850 (epoch 1), train_loss = 2.559, time/batch = 0.286\n",
            "34/850 (epoch 2), train_loss = 2.531, time/batch = 0.252\n",
            "35/850 (epoch 2), train_loss = 2.493, time/batch = 0.247\n",
            "36/850 (epoch 2), train_loss = 2.499, time/batch = 0.261\n",
            "37/850 (epoch 2), train_loss = 2.498, time/batch = 0.377\n",
            "38/850 (epoch 2), train_loss = 2.471, time/batch = 0.306\n",
            "39/850 (epoch 2), train_loss = 2.476, time/batch = 0.341\n",
            "40/850 (epoch 2), train_loss = 2.474, time/batch = 0.360\n",
            "41/850 (epoch 2), train_loss = 2.450, time/batch = 0.342\n",
            "42/850 (epoch 2), train_loss = 2.440, time/batch = 0.351\n",
            "43/850 (epoch 2), train_loss = 2.441, time/batch = 0.366\n",
            "44/850 (epoch 2), train_loss = 2.416, time/batch = 0.337\n",
            "45/850 (epoch 2), train_loss = 2.410, time/batch = 0.355\n",
            "46/850 (epoch 2), train_loss = 2.396, time/batch = 0.352\n",
            "47/850 (epoch 2), train_loss = 2.384, time/batch = 0.361\n",
            "48/850 (epoch 2), train_loss = 2.388, time/batch = 0.355\n",
            "49/850 (epoch 2), train_loss = 2.398, time/batch = 0.340\n",
            "50/850 (epoch 2), train_loss = 2.382, time/batch = 0.352\n",
            "51/850 (epoch 3), train_loss = 2.369, time/batch = 0.355\n",
            "52/850 (epoch 3), train_loss = 2.330, time/batch = 0.351\n",
            "53/850 (epoch 3), train_loss = 2.342, time/batch = 0.365\n",
            "54/850 (epoch 3), train_loss = 2.341, time/batch = 0.361\n",
            "55/850 (epoch 3), train_loss = 2.321, time/batch = 0.357\n",
            "56/850 (epoch 3), train_loss = 2.333, time/batch = 0.363\n",
            "57/850 (epoch 3), train_loss = 2.324, time/batch = 0.351\n",
            "58/850 (epoch 3), train_loss = 2.310, time/batch = 0.381\n",
            "59/850 (epoch 3), train_loss = 2.302, time/batch = 0.354\n",
            "60/850 (epoch 3), train_loss = 2.304, time/batch = 0.386\n",
            "61/850 (epoch 3), train_loss = 2.285, time/batch = 0.370\n",
            "62/850 (epoch 3), train_loss = 2.289, time/batch = 0.363\n",
            "63/850 (epoch 3), train_loss = 2.278, time/batch = 0.357\n",
            "64/850 (epoch 3), train_loss = 2.264, time/batch = 0.354\n",
            "65/850 (epoch 3), train_loss = 2.267, time/batch = 0.365\n",
            "66/850 (epoch 3), train_loss = 2.283, time/batch = 0.367\n",
            "67/850 (epoch 3), train_loss = 2.266, time/batch = 0.366\n",
            "68/850 (epoch 4), train_loss = 2.257, time/batch = 0.253\n",
            "69/850 (epoch 4), train_loss = 2.219, time/batch = 0.257\n",
            "70/850 (epoch 4), train_loss = 2.233, time/batch = 0.260\n",
            "71/850 (epoch 4), train_loss = 2.233, time/batch = 0.255\n",
            "72/850 (epoch 4), train_loss = 2.217, time/batch = 0.255\n",
            "73/850 (epoch 4), train_loss = 2.229, time/batch = 0.256\n",
            "74/850 (epoch 4), train_loss = 2.224, time/batch = 0.258\n",
            "75/850 (epoch 4), train_loss = 2.213, time/batch = 0.253\n",
            "76/850 (epoch 4), train_loss = 2.205, time/batch = 0.265\n",
            "77/850 (epoch 4), train_loss = 2.209, time/batch = 0.263\n",
            "78/850 (epoch 4), train_loss = 2.197, time/batch = 0.270\n",
            "79/850 (epoch 4), train_loss = 2.201, time/batch = 0.254\n",
            "80/850 (epoch 4), train_loss = 2.188, time/batch = 0.253\n",
            "81/850 (epoch 4), train_loss = 2.169, time/batch = 0.273\n",
            "82/850 (epoch 4), train_loss = 2.177, time/batch = 0.261\n",
            "83/850 (epoch 4), train_loss = 2.190, time/batch = 0.256\n",
            "84/850 (epoch 4), train_loss = 2.184, time/batch = 0.269\n",
            "85/850 (epoch 5), train_loss = 2.175, time/batch = 0.264\n",
            "86/850 (epoch 5), train_loss = 2.137, time/batch = 0.257\n",
            "87/850 (epoch 5), train_loss = 2.153, time/batch = 0.265\n",
            "88/850 (epoch 5), train_loss = 2.150, time/batch = 0.260\n",
            "89/850 (epoch 5), train_loss = 2.137, time/batch = 0.260\n",
            "90/850 (epoch 5), train_loss = 2.158, time/batch = 0.258\n",
            "91/850 (epoch 5), train_loss = 2.147, time/batch = 0.262\n",
            "92/850 (epoch 5), train_loss = 2.140, time/batch = 0.258\n",
            "93/850 (epoch 5), train_loss = 2.131, time/batch = 0.268\n",
            "94/850 (epoch 5), train_loss = 2.136, time/batch = 0.261\n",
            "95/850 (epoch 5), train_loss = 2.127, time/batch = 0.273\n",
            "96/850 (epoch 5), train_loss = 2.136, time/batch = 0.256\n",
            "97/850 (epoch 5), train_loss = 2.122, time/batch = 0.263\n",
            "98/850 (epoch 5), train_loss = 2.104, time/batch = 0.264\n",
            "99/850 (epoch 5), train_loss = 2.112, time/batch = 0.286\n",
            "100/850 (epoch 5), train_loss = 2.127, time/batch = 0.344\n",
            "101/850 (epoch 5), train_loss = 2.120, time/batch = 0.350\n",
            "102/850 (epoch 6), train_loss = 2.111, time/batch = 0.356\n",
            "103/850 (epoch 6), train_loss = 2.078, time/batch = 0.349\n",
            "104/850 (epoch 6), train_loss = 2.092, time/batch = 0.367\n",
            "105/850 (epoch 6), train_loss = 2.089, time/batch = 0.374\n",
            "106/850 (epoch 6), train_loss = 2.080, time/batch = 0.378\n",
            "107/850 (epoch 6), train_loss = 2.099, time/batch = 0.380\n",
            "108/850 (epoch 6), train_loss = 2.089, time/batch = 0.361\n",
            "109/850 (epoch 6), train_loss = 2.085, time/batch = 0.381\n",
            "110/850 (epoch 6), train_loss = 2.077, time/batch = 0.357\n",
            "111/850 (epoch 6), train_loss = 2.077, time/batch = 0.342\n",
            "112/850 (epoch 6), train_loss = 2.073, time/batch = 0.357\n",
            "113/850 (epoch 6), train_loss = 2.079, time/batch = 0.398\n",
            "114/850 (epoch 6), train_loss = 2.066, time/batch = 0.398\n",
            "115/850 (epoch 6), train_loss = 2.051, time/batch = 0.362\n",
            "116/850 (epoch 6), train_loss = 2.058, time/batch = 0.369\n",
            "117/850 (epoch 6), train_loss = 2.074, time/batch = 0.372\n",
            "118/850 (epoch 6), train_loss = 2.068, time/batch = 0.355\n",
            "119/850 (epoch 7), train_loss = 2.061, time/batch = 0.383\n",
            "120/850 (epoch 7), train_loss = 2.035, time/batch = 0.394\n",
            "121/850 (epoch 7), train_loss = 2.040, time/batch = 0.379\n",
            "122/850 (epoch 7), train_loss = 2.037, time/batch = 0.338\n",
            "123/850 (epoch 7), train_loss = 2.032, time/batch = 0.373\n",
            "124/850 (epoch 7), train_loss = 2.051, time/batch = 0.362\n",
            "125/850 (epoch 7), train_loss = 2.042, time/batch = 0.348\n",
            "126/850 (epoch 7), train_loss = 2.040, time/batch = 0.365\n",
            "127/850 (epoch 7), train_loss = 2.030, time/batch = 0.269\n",
            "128/850 (epoch 7), train_loss = 2.038, time/batch = 0.276\n",
            "129/850 (epoch 7), train_loss = 2.025, time/batch = 0.264\n",
            "130/850 (epoch 7), train_loss = 2.037, time/batch = 0.262\n",
            "131/850 (epoch 7), train_loss = 2.028, time/batch = 0.265\n",
            "132/850 (epoch 7), train_loss = 2.007, time/batch = 0.275\n",
            "133/850 (epoch 7), train_loss = 2.016, time/batch = 0.266\n",
            "134/850 (epoch 7), train_loss = 2.031, time/batch = 0.263\n",
            "135/850 (epoch 7), train_loss = 2.030, time/batch = 0.262\n",
            "136/850 (epoch 8), train_loss = 2.017, time/batch = 0.274\n",
            "137/850 (epoch 8), train_loss = 1.993, time/batch = 0.270\n",
            "138/850 (epoch 8), train_loss = 1.998, time/batch = 0.263\n",
            "139/850 (epoch 8), train_loss = 1.997, time/batch = 0.273\n",
            "140/850 (epoch 8), train_loss = 1.987, time/batch = 0.262\n",
            "141/850 (epoch 8), train_loss = 2.008, time/batch = 0.261\n",
            "142/850 (epoch 8), train_loss = 1.995, time/batch = 0.265\n",
            "143/850 (epoch 8), train_loss = 2.000, time/batch = 0.277\n",
            "144/850 (epoch 8), train_loss = 1.987, time/batch = 0.268\n",
            "145/850 (epoch 8), train_loss = 1.996, time/batch = 0.265\n",
            "146/850 (epoch 8), train_loss = 1.988, time/batch = 0.269\n",
            "147/850 (epoch 8), train_loss = 1.998, time/batch = 0.269\n",
            "148/850 (epoch 8), train_loss = 1.988, time/batch = 0.265\n",
            "149/850 (epoch 8), train_loss = 1.966, time/batch = 0.265\n",
            "150/850 (epoch 8), train_loss = 1.980, time/batch = 0.266\n",
            "151/850 (epoch 8), train_loss = 1.990, time/batch = 0.279\n",
            "152/850 (epoch 8), train_loss = 1.991, time/batch = 0.271\n",
            "153/850 (epoch 9), train_loss = 1.977, time/batch = 0.278\n",
            "154/850 (epoch 9), train_loss = 1.955, time/batch = 0.281\n",
            "155/850 (epoch 9), train_loss = 1.959, time/batch = 0.277\n",
            "156/850 (epoch 9), train_loss = 1.958, time/batch = 0.310\n",
            "157/850 (epoch 9), train_loss = 1.956, time/batch = 0.366\n",
            "158/850 (epoch 9), train_loss = 1.970, time/batch = 0.376\n",
            "159/850 (epoch 9), train_loss = 1.962, time/batch = 0.383\n",
            "160/850 (epoch 9), train_loss = 1.963, time/batch = 0.369\n",
            "161/850 (epoch 9), train_loss = 1.955, time/batch = 0.374\n",
            "162/850 (epoch 9), train_loss = 1.956, time/batch = 0.369\n",
            "163/850 (epoch 9), train_loss = 1.947, time/batch = 0.361\n",
            "164/850 (epoch 9), train_loss = 1.966, time/batch = 0.364\n",
            "165/850 (epoch 9), train_loss = 1.952, time/batch = 0.323\n",
            "166/850 (epoch 9), train_loss = 1.932, time/batch = 0.339\n",
            "167/850 (epoch 9), train_loss = 1.949, time/batch = 0.368\n",
            "168/850 (epoch 9), train_loss = 1.955, time/batch = 0.363\n",
            "169/850 (epoch 9), train_loss = 1.955, time/batch = 0.369\n",
            "170/850 (epoch 10), train_loss = 1.950, time/batch = 0.359\n",
            "171/850 (epoch 10), train_loss = 1.923, time/batch = 0.323\n",
            "172/850 (epoch 10), train_loss = 1.931, time/batch = 0.364\n",
            "173/850 (epoch 10), train_loss = 1.926, time/batch = 0.367\n",
            "174/850 (epoch 10), train_loss = 1.920, time/batch = 0.371\n",
            "175/850 (epoch 10), train_loss = 1.941, time/batch = 0.373\n",
            "176/850 (epoch 10), train_loss = 1.931, time/batch = 0.371\n",
            "177/850 (epoch 10), train_loss = 1.934, time/batch = 0.363\n",
            "178/850 (epoch 10), train_loss = 1.923, time/batch = 0.364\n",
            "179/850 (epoch 10), train_loss = 1.929, time/batch = 0.360\n",
            "180/850 (epoch 10), train_loss = 1.919, time/batch = 0.377\n",
            "181/850 (epoch 10), train_loss = 1.938, time/batch = 0.356\n",
            "182/850 (epoch 10), train_loss = 1.922, time/batch = 0.357\n",
            "183/850 (epoch 10), train_loss = 1.901, time/batch = 0.369\n",
            "184/850 (epoch 10), train_loss = 1.920, time/batch = 0.344\n",
            "185/850 (epoch 10), train_loss = 1.922, time/batch = 0.365\n",
            "186/850 (epoch 10), train_loss = 1.926, time/batch = 0.368\n",
            "187/850 (epoch 11), train_loss = 1.919, time/batch = 0.274\n",
            "188/850 (epoch 11), train_loss = 1.895, time/batch = 0.265\n",
            "189/850 (epoch 11), train_loss = 1.898, time/batch = 0.266\n",
            "190/850 (epoch 11), train_loss = 1.894, time/batch = 0.278\n",
            "191/850 (epoch 11), train_loss = 1.894, time/batch = 0.264\n",
            "192/850 (epoch 11), train_loss = 1.909, time/batch = 0.268\n",
            "193/850 (epoch 11), train_loss = 1.898, time/batch = 0.269\n",
            "194/850 (epoch 11), train_loss = 1.905, time/batch = 0.270\n",
            "195/850 (epoch 11), train_loss = 1.896, time/batch = 0.275\n",
            "196/850 (epoch 11), train_loss = 1.901, time/batch = 0.275\n",
            "197/850 (epoch 11), train_loss = 1.892, time/batch = 0.267\n",
            "198/850 (epoch 11), train_loss = 1.906, time/batch = 0.273\n",
            "199/850 (epoch 11), train_loss = 1.899, time/batch = 0.272\n",
            "200/850 (epoch 11), train_loss = 1.876, time/batch = 0.270\n",
            "201/850 (epoch 11), train_loss = 1.891, time/batch = 0.267\n",
            "202/850 (epoch 11), train_loss = 1.898, time/batch = 0.277\n",
            "203/850 (epoch 11), train_loss = 1.896, time/batch = 0.278\n",
            "204/850 (epoch 12), train_loss = 1.886, time/batch = 0.268\n",
            "205/850 (epoch 12), train_loss = 1.867, time/batch = 0.268\n",
            "206/850 (epoch 12), train_loss = 1.874, time/batch = 0.271\n",
            "207/850 (epoch 12), train_loss = 1.867, time/batch = 0.282\n",
            "208/850 (epoch 12), train_loss = 1.866, time/batch = 0.271\n",
            "209/850 (epoch 12), train_loss = 1.883, time/batch = 0.270\n",
            "210/850 (epoch 12), train_loss = 1.872, time/batch = 0.281\n",
            "211/850 (epoch 12), train_loss = 1.879, time/batch = 0.264\n",
            "212/850 (epoch 12), train_loss = 1.869, time/batch = 0.268\n",
            "213/850 (epoch 12), train_loss = 1.876, time/batch = 0.267\n",
            "214/850 (epoch 12), train_loss = 1.868, time/batch = 0.278\n",
            "215/850 (epoch 12), train_loss = 1.882, time/batch = 0.279\n",
            "216/850 (epoch 12), train_loss = 1.876, time/batch = 0.273\n",
            "217/850 (epoch 12), train_loss = 1.853, time/batch = 0.308\n",
            "218/850 (epoch 12), train_loss = 1.870, time/batch = 0.363\n",
            "219/850 (epoch 12), train_loss = 1.870, time/batch = 0.390\n",
            "220/850 (epoch 12), train_loss = 1.875, time/batch = 0.367\n",
            "221/850 (epoch 13), train_loss = 1.870, time/batch = 0.361\n",
            "222/850 (epoch 13), train_loss = 1.845, time/batch = 0.366\n",
            "223/850 (epoch 13), train_loss = 1.852, time/batch = 0.378\n",
            "224/850 (epoch 13), train_loss = 1.840, time/batch = 0.351\n",
            "225/850 (epoch 13), train_loss = 1.845, time/batch = 0.355\n",
            "226/850 (epoch 13), train_loss = 1.857, time/batch = 0.374\n",
            "227/850 (epoch 13), train_loss = 1.855, time/batch = 0.372\n",
            "228/850 (epoch 13), train_loss = 1.858, time/batch = 0.378\n",
            "229/850 (epoch 13), train_loss = 1.846, time/batch = 0.371\n",
            "230/850 (epoch 13), train_loss = 1.856, time/batch = 0.361\n",
            "231/850 (epoch 13), train_loss = 1.846, time/batch = 0.365\n",
            "232/850 (epoch 13), train_loss = 1.861, time/batch = 0.366\n",
            "233/850 (epoch 13), train_loss = 1.856, time/batch = 0.366\n",
            "234/850 (epoch 13), train_loss = 1.827, time/batch = 0.378\n",
            "235/850 (epoch 13), train_loss = 1.850, time/batch = 0.351\n",
            "236/850 (epoch 13), train_loss = 1.853, time/batch = 0.369\n",
            "237/850 (epoch 13), train_loss = 1.852, time/batch = 0.367\n",
            "238/850 (epoch 14), train_loss = 1.848, time/batch = 0.373\n",
            "239/850 (epoch 14), train_loss = 1.827, time/batch = 0.371\n",
            "240/850 (epoch 14), train_loss = 1.833, time/batch = 0.369\n",
            "241/850 (epoch 14), train_loss = 1.821, time/batch = 0.369\n",
            "242/850 (epoch 14), train_loss = 1.826, time/batch = 0.365\n",
            "243/850 (epoch 14), train_loss = 1.838, time/batch = 0.354\n",
            "244/850 (epoch 14), train_loss = 1.828, time/batch = 0.271\n",
            "245/850 (epoch 14), train_loss = 1.837, time/batch = 0.268\n",
            "246/850 (epoch 14), train_loss = 1.828, time/batch = 0.270\n",
            "247/850 (epoch 14), train_loss = 1.838, time/batch = 0.279\n",
            "248/850 (epoch 14), train_loss = 1.829, time/batch = 0.268\n",
            "249/850 (epoch 14), train_loss = 1.841, time/batch = 0.272\n",
            "250/850 (epoch 14), train_loss = 1.837, time/batch = 0.278\n",
            "251/850 (epoch 14), train_loss = 1.810, time/batch = 0.268\n",
            "252/850 (epoch 14), train_loss = 1.834, time/batch = 0.278\n",
            "253/850 (epoch 14), train_loss = 1.835, time/batch = 0.269\n",
            "254/850 (epoch 14), train_loss = 1.835, time/batch = 0.292\n",
            "255/850 (epoch 15), train_loss = 1.828, time/batch = 0.273\n",
            "256/850 (epoch 15), train_loss = 1.806, time/batch = 0.271\n",
            "257/850 (epoch 15), train_loss = 1.814, time/batch = 0.273\n",
            "258/850 (epoch 15), train_loss = 1.804, time/batch = 0.275\n",
            "259/850 (epoch 15), train_loss = 1.803, time/batch = 0.286\n",
            "260/850 (epoch 15), train_loss = 1.819, time/batch = 0.268\n",
            "261/850 (epoch 15), train_loss = 1.811, time/batch = 0.273\n",
            "262/850 (epoch 15), train_loss = 1.822, time/batch = 0.274\n",
            "263/850 (epoch 15), train_loss = 1.809, time/batch = 0.277\n",
            "264/850 (epoch 15), train_loss = 1.817, time/batch = 0.270\n",
            "265/850 (epoch 15), train_loss = 1.806, time/batch = 0.283\n",
            "266/850 (epoch 15), train_loss = 1.825, time/batch = 0.266\n",
            "267/850 (epoch 15), train_loss = 1.814, time/batch = 0.294\n",
            "268/850 (epoch 15), train_loss = 1.789, time/batch = 0.276\n",
            "269/850 (epoch 15), train_loss = 1.812, time/batch = 0.281\n",
            "270/850 (epoch 15), train_loss = 1.817, time/batch = 0.275\n",
            "271/850 (epoch 15), train_loss = 1.819, time/batch = 0.274\n",
            "272/850 (epoch 16), train_loss = 1.808, time/batch = 0.284\n",
            "273/850 (epoch 16), train_loss = 1.791, time/batch = 0.371\n",
            "274/850 (epoch 16), train_loss = 1.795, time/batch = 0.370\n",
            "275/850 (epoch 16), train_loss = 1.788, time/batch = 0.372\n",
            "276/850 (epoch 16), train_loss = 1.785, time/batch = 0.379\n",
            "277/850 (epoch 16), train_loss = 1.800, time/batch = 0.368\n",
            "278/850 (epoch 16), train_loss = 1.797, time/batch = 0.370\n",
            "279/850 (epoch 16), train_loss = 1.804, time/batch = 0.363\n",
            "280/850 (epoch 16), train_loss = 1.792, time/batch = 0.384\n",
            "281/850 (epoch 16), train_loss = 1.798, time/batch = 0.363\n",
            "282/850 (epoch 16), train_loss = 1.790, time/batch = 0.371\n",
            "283/850 (epoch 16), train_loss = 1.805, time/batch = 0.372\n",
            "284/850 (epoch 16), train_loss = 1.798, time/batch = 0.364\n",
            "285/850 (epoch 16), train_loss = 1.776, time/batch = 0.389\n",
            "286/850 (epoch 16), train_loss = 1.799, time/batch = 0.369\n",
            "287/850 (epoch 16), train_loss = 1.801, time/batch = 0.366\n",
            "288/850 (epoch 16), train_loss = 1.796, time/batch = 0.370\n",
            "289/850 (epoch 17), train_loss = 1.792, time/batch = 0.443\n",
            "290/850 (epoch 17), train_loss = 1.775, time/batch = 0.393\n",
            "291/850 (epoch 17), train_loss = 1.781, time/batch = 0.382\n",
            "292/850 (epoch 17), train_loss = 1.773, time/batch = 0.384\n",
            "293/850 (epoch 17), train_loss = 1.771, time/batch = 0.394\n",
            "294/850 (epoch 17), train_loss = 1.784, time/batch = 0.364\n",
            "295/850 (epoch 17), train_loss = 1.780, time/batch = 0.373\n",
            "296/850 (epoch 17), train_loss = 1.786, time/batch = 0.358\n",
            "297/850 (epoch 17), train_loss = 1.776, time/batch = 0.371\n",
            "298/850 (epoch 17), train_loss = 1.782, time/batch = 0.378\n",
            "299/850 (epoch 17), train_loss = 1.771, time/batch = 0.380\n",
            "300/850 (epoch 17), train_loss = 1.791, time/batch = 0.369\n",
            "301/850 (epoch 17), train_loss = 1.784, time/batch = 0.363\n",
            "302/850 (epoch 17), train_loss = 1.758, time/batch = 0.364\n",
            "303/850 (epoch 17), train_loss = 1.780, time/batch = 0.277\n",
            "304/850 (epoch 17), train_loss = 1.779, time/batch = 0.274\n",
            "305/850 (epoch 17), train_loss = 1.786, time/batch = 0.283\n",
            "306/850 (epoch 18), train_loss = 1.776, time/batch = 0.273\n",
            "307/850 (epoch 18), train_loss = 1.762, time/batch = 0.272\n",
            "308/850 (epoch 18), train_loss = 1.763, time/batch = 0.268\n",
            "309/850 (epoch 18), train_loss = 1.757, time/batch = 0.271\n",
            "310/850 (epoch 18), train_loss = 1.756, time/batch = 0.272\n",
            "311/850 (epoch 18), train_loss = 1.767, time/batch = 0.278\n",
            "312/850 (epoch 18), train_loss = 1.760, time/batch = 0.280\n",
            "313/850 (epoch 18), train_loss = 1.772, time/batch = 0.276\n",
            "314/850 (epoch 18), train_loss = 1.765, time/batch = 0.270\n",
            "315/850 (epoch 18), train_loss = 1.767, time/batch = 0.276\n",
            "316/850 (epoch 18), train_loss = 1.765, time/batch = 0.290\n",
            "317/850 (epoch 18), train_loss = 1.783, time/batch = 0.276\n",
            "318/850 (epoch 18), train_loss = 1.770, time/batch = 0.277\n",
            "319/850 (epoch 18), train_loss = 1.746, time/batch = 0.274\n",
            "320/850 (epoch 18), train_loss = 1.769, time/batch = 0.278\n",
            "321/850 (epoch 18), train_loss = 1.769, time/batch = 0.272\n",
            "322/850 (epoch 18), train_loss = 1.773, time/batch = 0.274\n",
            "323/850 (epoch 19), train_loss = 1.767, time/batch = 0.277\n",
            "324/850 (epoch 19), train_loss = 1.750, time/batch = 0.273\n",
            "325/850 (epoch 19), train_loss = 1.750, time/batch = 0.275\n",
            "326/850 (epoch 19), train_loss = 1.745, time/batch = 0.275\n",
            "327/850 (epoch 19), train_loss = 1.741, time/batch = 0.286\n",
            "328/850 (epoch 19), train_loss = 1.757, time/batch = 0.271\n",
            "329/850 (epoch 19), train_loss = 1.749, time/batch = 0.272\n",
            "330/850 (epoch 19), train_loss = 1.763, time/batch = 0.275\n",
            "331/850 (epoch 19), train_loss = 1.746, time/batch = 0.303\n",
            "332/850 (epoch 19), train_loss = 1.758, time/batch = 0.356\n",
            "333/850 (epoch 19), train_loss = 1.748, time/batch = 0.386\n",
            "334/850 (epoch 19), train_loss = 1.767, time/batch = 0.367\n",
            "335/850 (epoch 19), train_loss = 1.758, time/batch = 0.372\n",
            "336/850 (epoch 19), train_loss = 1.732, time/batch = 0.387\n",
            "337/850 (epoch 19), train_loss = 1.754, time/batch = 0.369\n",
            "338/850 (epoch 19), train_loss = 1.756, time/batch = 0.360\n",
            "339/850 (epoch 19), train_loss = 1.758, time/batch = 0.372\n",
            "340/850 (epoch 20), train_loss = 1.750, time/batch = 0.366\n",
            "341/850 (epoch 20), train_loss = 1.735, time/batch = 0.380\n",
            "342/850 (epoch 20), train_loss = 1.740, time/batch = 0.371\n",
            "343/850 (epoch 20), train_loss = 1.730, time/batch = 0.377\n",
            "344/850 (epoch 20), train_loss = 1.729, time/batch = 0.377\n",
            "345/850 (epoch 20), train_loss = 1.744, time/batch = 0.378\n",
            "346/850 (epoch 20), train_loss = 1.736, time/batch = 0.377\n",
            "347/850 (epoch 20), train_loss = 1.750, time/batch = 0.376\n",
            "348/850 (epoch 20), train_loss = 1.739, time/batch = 0.357\n",
            "349/850 (epoch 20), train_loss = 1.745, time/batch = 0.371\n",
            "350/850 (epoch 20), train_loss = 1.742, time/batch = 0.377\n",
            "351/850 (epoch 20), train_loss = 1.756, time/batch = 0.375\n",
            "352/850 (epoch 20), train_loss = 1.747, time/batch = 0.384\n",
            "353/850 (epoch 20), train_loss = 1.717, time/batch = 0.383\n",
            "354/850 (epoch 20), train_loss = 1.745, time/batch = 0.380\n",
            "355/850 (epoch 20), train_loss = 1.742, time/batch = 0.381\n",
            "356/850 (epoch 20), train_loss = 1.752, time/batch = 0.354\n",
            "357/850 (epoch 21), train_loss = 1.742, time/batch = 0.422\n",
            "358/850 (epoch 21), train_loss = 1.721, time/batch = 0.278\n",
            "359/850 (epoch 21), train_loss = 1.728, time/batch = 0.279\n",
            "360/850 (epoch 21), train_loss = 1.722, time/batch = 0.280\n",
            "361/850 (epoch 21), train_loss = 1.718, time/batch = 0.275\n",
            "362/850 (epoch 21), train_loss = 1.732, time/batch = 0.297\n",
            "363/850 (epoch 21), train_loss = 1.725, time/batch = 0.276\n",
            "364/850 (epoch 21), train_loss = 1.740, time/batch = 0.285\n",
            "365/850 (epoch 21), train_loss = 1.726, time/batch = 0.274\n",
            "366/850 (epoch 21), train_loss = 1.738, time/batch = 0.290\n",
            "367/850 (epoch 21), train_loss = 1.728, time/batch = 0.277\n",
            "368/850 (epoch 21), train_loss = 1.741, time/batch = 0.272\n",
            "369/850 (epoch 21), train_loss = 1.734, time/batch = 0.277\n",
            "370/850 (epoch 21), train_loss = 1.711, time/batch = 0.275\n",
            "371/850 (epoch 21), train_loss = 1.733, time/batch = 0.276\n",
            "372/850 (epoch 21), train_loss = 1.731, time/batch = 0.272\n",
            "373/850 (epoch 21), train_loss = 1.740, time/batch = 0.287\n",
            "374/850 (epoch 22), train_loss = 1.727, time/batch = 0.272\n",
            "375/850 (epoch 22), train_loss = 1.710, time/batch = 0.275\n",
            "376/850 (epoch 22), train_loss = 1.717, time/batch = 0.279\n",
            "377/850 (epoch 22), train_loss = 1.711, time/batch = 0.274\n",
            "378/850 (epoch 22), train_loss = 1.709, time/batch = 0.280\n",
            "379/850 (epoch 22), train_loss = 1.720, time/batch = 0.273\n",
            "380/850 (epoch 22), train_loss = 1.717, time/batch = 0.284\n",
            "381/850 (epoch 22), train_loss = 1.731, time/batch = 0.276\n",
            "382/850 (epoch 22), train_loss = 1.718, time/batch = 0.278\n",
            "383/850 (epoch 22), train_loss = 1.721, time/batch = 0.275\n",
            "384/850 (epoch 22), train_loss = 1.718, time/batch = 0.275\n",
            "385/850 (epoch 22), train_loss = 1.728, time/batch = 0.272\n",
            "386/850 (epoch 22), train_loss = 1.725, time/batch = 0.273\n",
            "387/850 (epoch 22), train_loss = 1.703, time/batch = 0.282\n",
            "388/850 (epoch 22), train_loss = 1.727, time/batch = 0.272\n",
            "389/850 (epoch 22), train_loss = 1.721, time/batch = 0.276\n",
            "390/850 (epoch 22), train_loss = 1.731, time/batch = 0.358\n",
            "391/850 (epoch 23), train_loss = 1.720, time/batch = 0.356\n",
            "392/850 (epoch 23), train_loss = 1.703, time/batch = 0.364\n",
            "393/850 (epoch 23), train_loss = 1.706, time/batch = 0.368\n",
            "394/850 (epoch 23), train_loss = 1.703, time/batch = 0.389\n",
            "395/850 (epoch 23), train_loss = 1.699, time/batch = 0.372\n",
            "396/850 (epoch 23), train_loss = 1.714, time/batch = 0.376\n",
            "397/850 (epoch 23), train_loss = 1.709, time/batch = 0.360\n",
            "398/850 (epoch 23), train_loss = 1.720, time/batch = 0.379\n",
            "399/850 (epoch 23), train_loss = 1.707, time/batch = 0.382\n",
            "400/850 (epoch 23), train_loss = 1.713, time/batch = 0.362\n",
            "401/850 (epoch 23), train_loss = 1.711, time/batch = 0.357\n",
            "402/850 (epoch 23), train_loss = 1.723, time/batch = 0.393\n",
            "403/850 (epoch 23), train_loss = 1.715, time/batch = 0.369\n",
            "404/850 (epoch 23), train_loss = 1.692, time/batch = 0.376\n",
            "405/850 (epoch 23), train_loss = 1.718, time/batch = 0.367\n",
            "406/850 (epoch 23), train_loss = 1.719, time/batch = 0.364\n",
            "407/850 (epoch 23), train_loss = 1.715, time/batch = 0.368\n",
            "408/850 (epoch 24), train_loss = 1.711, time/batch = 0.443\n",
            "409/850 (epoch 24), train_loss = 1.695, time/batch = 0.366\n",
            "410/850 (epoch 24), train_loss = 1.703, time/batch = 0.378\n",
            "411/850 (epoch 24), train_loss = 1.695, time/batch = 0.376\n",
            "412/850 (epoch 24), train_loss = 1.692, time/batch = 0.379\n",
            "413/850 (epoch 24), train_loss = 1.699, time/batch = 0.358\n",
            "414/850 (epoch 24), train_loss = 1.697, time/batch = 0.344\n",
            "415/850 (epoch 24), train_loss = 1.708, time/batch = 0.377\n",
            "416/850 (epoch 24), train_loss = 1.702, time/batch = 0.276\n",
            "417/850 (epoch 24), train_loss = 1.707, time/batch = 0.281\n",
            "418/850 (epoch 24), train_loss = 1.702, time/batch = 0.275\n",
            "419/850 (epoch 24), train_loss = 1.713, time/batch = 0.285\n",
            "420/850 (epoch 24), train_loss = 1.710, time/batch = 0.275\n",
            "421/850 (epoch 24), train_loss = 1.686, time/batch = 0.271\n",
            "422/850 (epoch 24), train_loss = 1.706, time/batch = 0.281\n",
            "423/850 (epoch 24), train_loss = 1.710, time/batch = 0.290\n",
            "424/850 (epoch 24), train_loss = 1.708, time/batch = 0.275\n",
            "425/850 (epoch 25), train_loss = 1.707, time/batch = 0.271\n",
            "426/850 (epoch 25), train_loss = 1.690, time/batch = 0.289\n",
            "427/850 (epoch 25), train_loss = 1.686, time/batch = 0.276\n",
            "428/850 (epoch 25), train_loss = 1.685, time/batch = 0.275\n",
            "429/850 (epoch 25), train_loss = 1.683, time/batch = 0.275\n",
            "430/850 (epoch 25), train_loss = 1.690, time/batch = 0.276\n",
            "431/850 (epoch 25), train_loss = 1.690, time/batch = 0.273\n",
            "432/850 (epoch 25), train_loss = 1.703, time/batch = 0.273\n",
            "433/850 (epoch 25), train_loss = 1.693, time/batch = 0.272\n",
            "434/850 (epoch 25), train_loss = 1.697, time/batch = 0.281\n",
            "435/850 (epoch 25), train_loss = 1.693, time/batch = 0.270\n",
            "436/850 (epoch 25), train_loss = 1.706, time/batch = 0.272\n",
            "437/850 (epoch 25), train_loss = 1.703, time/batch = 0.273\n",
            "438/850 (epoch 25), train_loss = 1.675, time/batch = 0.275\n",
            "439/850 (epoch 25), train_loss = 1.703, time/batch = 0.270\n",
            "440/850 (epoch 25), train_loss = 1.696, time/batch = 0.272\n",
            "441/850 (epoch 25), train_loss = 1.702, time/batch = 0.279\n",
            "442/850 (epoch 26), train_loss = 1.700, time/batch = 0.268\n",
            "443/850 (epoch 26), train_loss = 1.683, time/batch = 0.295\n",
            "444/850 (epoch 26), train_loss = 1.681, time/batch = 0.342\n",
            "445/850 (epoch 26), train_loss = 1.675, time/batch = 0.364\n",
            "446/850 (epoch 26), train_loss = 1.679, time/batch = 0.371\n",
            "447/850 (epoch 26), train_loss = 1.682, time/batch = 0.369\n",
            "448/850 (epoch 26), train_loss = 1.681, time/batch = 0.358\n",
            "449/850 (epoch 26), train_loss = 1.695, time/batch = 0.370\n",
            "450/850 (epoch 26), train_loss = 1.684, time/batch = 0.397\n",
            "451/850 (epoch 26), train_loss = 1.688, time/batch = 0.353\n",
            "452/850 (epoch 26), train_loss = 1.682, time/batch = 0.389\n",
            "453/850 (epoch 26), train_loss = 1.699, time/batch = 0.307\n",
            "454/850 (epoch 26), train_loss = 1.695, time/batch = 0.373\n",
            "455/850 (epoch 26), train_loss = 1.667, time/batch = 0.374\n",
            "456/850 (epoch 26), train_loss = 1.693, time/batch = 0.369\n",
            "457/850 (epoch 26), train_loss = 1.692, time/batch = 0.384\n",
            "458/850 (epoch 26), train_loss = 1.695, time/batch = 0.377\n",
            "459/850 (epoch 27), train_loss = 1.692, time/batch = 0.462\n",
            "460/850 (epoch 27), train_loss = 1.674, time/batch = 0.378\n",
            "461/850 (epoch 27), train_loss = 1.676, time/batch = 0.369\n",
            "462/850 (epoch 27), train_loss = 1.669, time/batch = 0.375\n",
            "463/850 (epoch 27), train_loss = 1.669, time/batch = 0.360\n",
            "464/850 (epoch 27), train_loss = 1.679, time/batch = 0.387\n",
            "465/850 (epoch 27), train_loss = 1.673, time/batch = 0.360\n",
            "466/850 (epoch 27), train_loss = 1.687, time/batch = 0.394\n",
            "467/850 (epoch 27), train_loss = 1.681, time/batch = 0.372\n",
            "468/850 (epoch 27), train_loss = 1.685, time/batch = 0.370\n",
            "469/850 (epoch 27), train_loss = 1.680, time/batch = 0.363\n",
            "470/850 (epoch 27), train_loss = 1.690, time/batch = 0.360\n",
            "471/850 (epoch 27), train_loss = 1.687, time/batch = 0.367\n",
            "472/850 (epoch 27), train_loss = 1.657, time/batch = 0.392\n",
            "473/850 (epoch 27), train_loss = 1.690, time/batch = 0.363\n",
            "474/850 (epoch 27), train_loss = 1.685, time/batch = 0.383\n",
            "475/850 (epoch 27), train_loss = 1.686, time/batch = 0.282\n",
            "476/850 (epoch 28), train_loss = 1.680, time/batch = 0.279\n",
            "477/850 (epoch 28), train_loss = 1.666, time/batch = 0.271\n",
            "478/850 (epoch 28), train_loss = 1.671, time/batch = 0.288\n",
            "479/850 (epoch 28), train_loss = 1.665, time/batch = 0.272\n",
            "480/850 (epoch 28), train_loss = 1.664, time/batch = 0.277\n",
            "481/850 (epoch 28), train_loss = 1.671, time/batch = 0.299\n",
            "482/850 (epoch 28), train_loss = 1.666, time/batch = 0.290\n",
            "483/850 (epoch 28), train_loss = 1.682, time/batch = 0.276\n",
            "484/850 (epoch 28), train_loss = 1.673, time/batch = 0.281\n",
            "485/850 (epoch 28), train_loss = 1.676, time/batch = 0.273\n",
            "486/850 (epoch 28), train_loss = 1.674, time/batch = 0.269\n",
            "487/850 (epoch 28), train_loss = 1.690, time/batch = 0.288\n",
            "488/850 (epoch 28), train_loss = 1.679, time/batch = 0.271\n",
            "489/850 (epoch 28), train_loss = 1.653, time/batch = 0.268\n",
            "490/850 (epoch 28), train_loss = 1.681, time/batch = 0.265\n",
            "491/850 (epoch 28), train_loss = 1.676, time/batch = 0.285\n",
            "492/850 (epoch 28), train_loss = 1.682, time/batch = 0.276\n",
            "493/850 (epoch 29), train_loss = 1.676, time/batch = 0.269\n",
            "494/850 (epoch 29), train_loss = 1.660, time/batch = 0.276\n",
            "495/850 (epoch 29), train_loss = 1.666, time/batch = 0.266\n",
            "496/850 (epoch 29), train_loss = 1.656, time/batch = 0.273\n",
            "497/850 (epoch 29), train_loss = 1.654, time/batch = 0.270\n",
            "498/850 (epoch 29), train_loss = 1.665, time/batch = 0.274\n",
            "499/850 (epoch 29), train_loss = 1.665, time/batch = 0.275\n",
            "500/850 (epoch 29), train_loss = 1.675, time/batch = 0.274\n",
            "501/850 (epoch 29), train_loss = 1.668, time/batch = 0.281\n",
            "502/850 (epoch 29), train_loss = 1.667, time/batch = 0.282\n",
            "503/850 (epoch 29), train_loss = 1.665, time/batch = 0.332\n",
            "504/850 (epoch 29), train_loss = 1.674, time/batch = 0.371\n",
            "505/850 (epoch 29), train_loss = 1.677, time/batch = 0.379\n",
            "506/850 (epoch 29), train_loss = 1.648, time/batch = 0.368\n",
            "507/850 (epoch 29), train_loss = 1.676, time/batch = 0.359\n",
            "508/850 (epoch 29), train_loss = 1.672, time/batch = 0.366\n",
            "509/850 (epoch 29), train_loss = 1.676, time/batch = 0.364\n",
            "510/850 (epoch 30), train_loss = 1.673, time/batch = 0.456\n",
            "511/850 (epoch 30), train_loss = 1.651, time/batch = 0.377\n",
            "512/850 (epoch 30), train_loss = 1.658, time/batch = 0.382\n",
            "513/850 (epoch 30), train_loss = 1.650, time/batch = 0.388\n",
            "514/850 (epoch 30), train_loss = 1.650, time/batch = 0.399\n",
            "515/850 (epoch 30), train_loss = 1.658, time/batch = 0.372\n",
            "516/850 (epoch 30), train_loss = 1.661, time/batch = 0.376\n",
            "517/850 (epoch 30), train_loss = 1.672, time/batch = 0.386\n",
            "518/850 (epoch 30), train_loss = 1.662, time/batch = 0.381\n",
            "519/850 (epoch 30), train_loss = 1.662, time/batch = 0.373\n",
            "520/850 (epoch 30), train_loss = 1.664, time/batch = 0.367\n",
            "521/850 (epoch 30), train_loss = 1.679, time/batch = 0.376\n",
            "522/850 (epoch 30), train_loss = 1.669, time/batch = 0.373\n",
            "523/850 (epoch 30), train_loss = 1.646, time/batch = 0.368\n",
            "524/850 (epoch 30), train_loss = 1.669, time/batch = 0.379\n",
            "525/850 (epoch 30), train_loss = 1.663, time/batch = 0.334\n",
            "526/850 (epoch 30), train_loss = 1.672, time/batch = 0.341\n",
            "527/850 (epoch 31), train_loss = 1.670, time/batch = 0.307\n",
            "528/850 (epoch 31), train_loss = 1.653, time/batch = 0.275\n",
            "529/850 (epoch 31), train_loss = 1.650, time/batch = 0.276\n",
            "530/850 (epoch 31), train_loss = 1.652, time/batch = 0.275\n",
            "531/850 (epoch 31), train_loss = 1.647, time/batch = 0.284\n",
            "532/850 (epoch 31), train_loss = 1.652, time/batch = 0.273\n",
            "533/850 (epoch 31), train_loss = 1.655, time/batch = 0.286\n",
            "534/850 (epoch 31), train_loss = 1.664, time/batch = 0.277\n",
            "535/850 (epoch 31), train_loss = 1.653, time/batch = 0.280\n",
            "536/850 (epoch 31), train_loss = 1.660, time/batch = 0.272\n",
            "537/850 (epoch 31), train_loss = 1.657, time/batch = 0.278\n",
            "538/850 (epoch 31), train_loss = 1.671, time/batch = 0.274\n",
            "539/850 (epoch 31), train_loss = 1.660, time/batch = 0.270\n",
            "540/850 (epoch 31), train_loss = 1.640, time/batch = 0.277\n",
            "541/850 (epoch 31), train_loss = 1.662, time/batch = 0.272\n",
            "542/850 (epoch 31), train_loss = 1.666, time/batch = 0.276\n",
            "543/850 (epoch 31), train_loss = 1.667, time/batch = 0.273\n",
            "544/850 (epoch 32), train_loss = 1.664, time/batch = 0.284\n",
            "545/850 (epoch 32), train_loss = 1.645, time/batch = 0.275\n",
            "546/850 (epoch 32), train_loss = 1.650, time/batch = 0.269\n",
            "547/850 (epoch 32), train_loss = 1.644, time/batch = 0.272\n",
            "548/850 (epoch 32), train_loss = 1.642, time/batch = 0.270\n",
            "549/850 (epoch 32), train_loss = 1.648, time/batch = 0.272\n",
            "550/850 (epoch 32), train_loss = 1.650, time/batch = 0.275\n",
            "551/850 (epoch 32), train_loss = 1.662, time/batch = 0.276\n",
            "552/850 (epoch 32), train_loss = 1.650, time/batch = 0.276\n",
            "553/850 (epoch 32), train_loss = 1.658, time/batch = 0.272\n",
            "554/850 (epoch 32), train_loss = 1.650, time/batch = 0.274\n",
            "555/850 (epoch 32), train_loss = 1.668, time/batch = 0.283\n",
            "556/850 (epoch 32), train_loss = 1.660, time/batch = 0.274\n",
            "557/850 (epoch 32), train_loss = 1.636, time/batch = 0.272\n",
            "558/850 (epoch 32), train_loss = 1.660, time/batch = 0.359\n",
            "559/850 (epoch 32), train_loss = 1.657, time/batch = 0.375\n",
            "560/850 (epoch 32), train_loss = 1.661, time/batch = 0.370\n",
            "561/850 (epoch 33), train_loss = 1.655, time/batch = 0.391\n",
            "562/850 (epoch 33), train_loss = 1.643, time/batch = 0.364\n",
            "563/850 (epoch 33), train_loss = 1.645, time/batch = 0.371\n",
            "564/850 (epoch 33), train_loss = 1.642, time/batch = 0.374\n",
            "565/850 (epoch 33), train_loss = 1.638, time/batch = 0.382\n",
            "566/850 (epoch 33), train_loss = 1.639, time/batch = 0.376\n",
            "567/850 (epoch 33), train_loss = 1.647, time/batch = 0.355\n",
            "568/850 (epoch 33), train_loss = 1.655, time/batch = 0.376\n",
            "569/850 (epoch 33), train_loss = 1.647, time/batch = 0.372\n",
            "570/850 (epoch 33), train_loss = 1.650, time/batch = 0.365\n",
            "571/850 (epoch 33), train_loss = 1.653, time/batch = 0.365\n",
            "572/850 (epoch 33), train_loss = 1.662, time/batch = 0.370\n",
            "573/850 (epoch 33), train_loss = 1.659, time/batch = 0.358\n",
            "574/850 (epoch 33), train_loss = 1.628, time/batch = 0.380\n",
            "575/850 (epoch 33), train_loss = 1.660, time/batch = 0.370\n",
            "576/850 (epoch 33), train_loss = 1.657, time/batch = 0.366\n",
            "577/850 (epoch 33), train_loss = 1.657, time/batch = 0.377\n",
            "578/850 (epoch 34), train_loss = 1.655, time/batch = 0.439\n",
            "579/850 (epoch 34), train_loss = 1.637, time/batch = 0.312\n",
            "580/850 (epoch 34), train_loss = 1.641, time/batch = 0.355\n",
            "581/850 (epoch 34), train_loss = 1.635, time/batch = 0.270\n",
            "582/850 (epoch 34), train_loss = 1.633, time/batch = 0.269\n",
            "583/850 (epoch 34), train_loss = 1.643, time/batch = 0.274\n",
            "584/850 (epoch 34), train_loss = 1.645, time/batch = 0.285\n",
            "585/850 (epoch 34), train_loss = 1.654, time/batch = 0.268\n",
            "586/850 (epoch 34), train_loss = 1.643, time/batch = 0.271\n",
            "587/850 (epoch 34), train_loss = 1.648, time/batch = 0.279\n",
            "588/850 (epoch 34), train_loss = 1.645, time/batch = 0.276\n",
            "589/850 (epoch 34), train_loss = 1.656, time/batch = 0.277\n",
            "590/850 (epoch 34), train_loss = 1.651, time/batch = 0.272\n",
            "591/850 (epoch 34), train_loss = 1.627, time/batch = 0.288\n",
            "592/850 (epoch 34), train_loss = 1.658, time/batch = 0.277\n",
            "593/850 (epoch 34), train_loss = 1.650, time/batch = 0.288\n",
            "594/850 (epoch 34), train_loss = 1.656, time/batch = 0.277\n",
            "595/850 (epoch 35), train_loss = 1.649, time/batch = 0.271\n",
            "596/850 (epoch 35), train_loss = 1.633, time/batch = 0.271\n",
            "597/850 (epoch 35), train_loss = 1.637, time/batch = 0.277\n",
            "598/850 (epoch 35), train_loss = 1.628, time/batch = 0.280\n",
            "599/850 (epoch 35), train_loss = 1.630, time/batch = 0.278\n",
            "600/850 (epoch 35), train_loss = 1.637, time/batch = 0.269\n",
            "601/850 (epoch 35), train_loss = 1.638, time/batch = 0.277\n",
            "602/850 (epoch 35), train_loss = 1.648, time/batch = 0.296\n",
            "603/850 (epoch 35), train_loss = 1.641, time/batch = 0.287\n",
            "604/850 (epoch 35), train_loss = 1.638, time/batch = 0.272\n",
            "605/850 (epoch 35), train_loss = 1.641, time/batch = 0.271\n",
            "606/850 (epoch 35), train_loss = 1.658, time/batch = 0.274\n",
            "607/850 (epoch 35), train_loss = 1.647, time/batch = 0.271\n",
            "608/850 (epoch 35), train_loss = 1.624, time/batch = 0.274\n",
            "609/850 (epoch 35), train_loss = 1.651, time/batch = 0.275\n",
            "610/850 (epoch 35), train_loss = 1.645, time/batch = 0.278\n",
            "611/850 (epoch 35), train_loss = 1.652, time/batch = 0.272\n",
            "612/850 (epoch 36), train_loss = 1.648, time/batch = 0.355\n",
            "613/850 (epoch 36), train_loss = 1.633, time/batch = 0.365\n",
            "614/850 (epoch 36), train_loss = 1.632, time/batch = 0.349\n",
            "615/850 (epoch 36), train_loss = 1.627, time/batch = 0.368\n",
            "616/850 (epoch 36), train_loss = 1.633, time/batch = 0.373\n",
            "617/850 (epoch 36), train_loss = 1.635, time/batch = 0.381\n",
            "618/850 (epoch 36), train_loss = 1.636, time/batch = 0.373\n",
            "619/850 (epoch 36), train_loss = 1.648, time/batch = 0.400\n",
            "620/850 (epoch 36), train_loss = 1.638, time/batch = 0.388\n",
            "621/850 (epoch 36), train_loss = 1.641, time/batch = 0.356\n",
            "622/850 (epoch 36), train_loss = 1.638, time/batch = 0.384\n",
            "623/850 (epoch 36), train_loss = 1.651, time/batch = 0.372\n",
            "624/850 (epoch 36), train_loss = 1.645, time/batch = 0.372\n",
            "625/850 (epoch 36), train_loss = 1.622, time/batch = 0.377\n",
            "626/850 (epoch 36), train_loss = 1.643, time/batch = 0.370\n",
            "627/850 (epoch 36), train_loss = 1.646, time/batch = 0.373\n",
            "628/850 (epoch 36), train_loss = 1.648, time/batch = 0.368\n",
            "629/850 (epoch 37), train_loss = 1.643, time/batch = 0.418\n",
            "630/850 (epoch 37), train_loss = 1.627, time/batch = 0.375\n",
            "631/850 (epoch 37), train_loss = 1.631, time/batch = 0.366\n",
            "632/850 (epoch 37), train_loss = 1.624, time/batch = 0.368\n",
            "633/850 (epoch 37), train_loss = 1.626, time/batch = 0.362\n",
            "634/850 (epoch 37), train_loss = 1.630, time/batch = 0.368\n",
            "635/850 (epoch 37), train_loss = 1.631, time/batch = 0.338\n",
            "636/850 (epoch 37), train_loss = 1.644, time/batch = 0.280\n",
            "637/850 (epoch 37), train_loss = 1.631, time/batch = 0.273\n",
            "638/850 (epoch 37), train_loss = 1.634, time/batch = 0.288\n",
            "639/850 (epoch 37), train_loss = 1.634, time/batch = 0.277\n",
            "640/850 (epoch 37), train_loss = 1.652, time/batch = 0.292\n",
            "641/850 (epoch 37), train_loss = 1.641, time/batch = 0.276\n",
            "642/850 (epoch 37), train_loss = 1.620, time/batch = 0.273\n",
            "643/850 (epoch 37), train_loss = 1.644, time/batch = 0.275\n",
            "644/850 (epoch 37), train_loss = 1.641, time/batch = 0.272\n",
            "645/850 (epoch 37), train_loss = 1.647, time/batch = 0.277\n",
            "646/850 (epoch 38), train_loss = 1.642, time/batch = 0.273\n",
            "647/850 (epoch 38), train_loss = 1.626, time/batch = 0.269\n",
            "648/850 (epoch 38), train_loss = 1.627, time/batch = 0.271\n",
            "649/850 (epoch 38), train_loss = 1.624, time/batch = 0.270\n",
            "650/850 (epoch 38), train_loss = 1.625, time/batch = 0.275\n",
            "651/850 (epoch 38), train_loss = 1.627, time/batch = 0.277\n",
            "652/850 (epoch 38), train_loss = 1.633, time/batch = 0.289\n",
            "653/850 (epoch 38), train_loss = 1.640, time/batch = 0.271\n",
            "654/850 (epoch 38), train_loss = 1.631, time/batch = 0.277\n",
            "655/850 (epoch 38), train_loss = 1.634, time/batch = 0.275\n",
            "656/850 (epoch 38), train_loss = 1.633, time/batch = 0.286\n",
            "657/850 (epoch 38), train_loss = 1.643, time/batch = 0.295\n",
            "658/850 (epoch 38), train_loss = 1.642, time/batch = 0.282\n",
            "659/850 (epoch 38), train_loss = 1.617, time/batch = 0.275\n",
            "660/850 (epoch 38), train_loss = 1.643, time/batch = 0.280\n",
            "661/850 (epoch 38), train_loss = 1.636, time/batch = 0.273\n",
            "662/850 (epoch 38), train_loss = 1.645, time/batch = 0.279\n",
            "663/850 (epoch 39), train_loss = 1.638, time/batch = 0.359\n",
            "664/850 (epoch 39), train_loss = 1.622, time/batch = 0.359\n",
            "665/850 (epoch 39), train_loss = 1.624, time/batch = 0.369\n",
            "666/850 (epoch 39), train_loss = 1.621, time/batch = 0.364\n",
            "667/850 (epoch 39), train_loss = 1.620, time/batch = 0.346\n",
            "668/850 (epoch 39), train_loss = 1.624, time/batch = 0.364\n",
            "669/850 (epoch 39), train_loss = 1.627, time/batch = 0.384\n",
            "670/850 (epoch 39), train_loss = 1.637, time/batch = 0.364\n",
            "671/850 (epoch 39), train_loss = 1.629, time/batch = 0.370\n",
            "672/850 (epoch 39), train_loss = 1.633, time/batch = 0.373\n",
            "673/850 (epoch 39), train_loss = 1.635, time/batch = 0.384\n",
            "674/850 (epoch 39), train_loss = 1.644, time/batch = 0.350\n",
            "675/850 (epoch 39), train_loss = 1.640, time/batch = 0.366\n",
            "676/850 (epoch 39), train_loss = 1.613, time/batch = 0.374\n",
            "677/850 (epoch 39), train_loss = 1.638, time/batch = 0.383\n",
            "678/850 (epoch 39), train_loss = 1.635, time/batch = 0.359\n",
            "679/850 (epoch 39), train_loss = 1.640, time/batch = 0.379\n",
            "680/850 (epoch 40), train_loss = 1.641, time/batch = 0.444\n",
            "681/850 (epoch 40), train_loss = 1.623, time/batch = 0.381\n",
            "682/850 (epoch 40), train_loss = 1.624, time/batch = 0.376\n",
            "683/850 (epoch 40), train_loss = 1.618, time/batch = 0.365\n",
            "684/850 (epoch 40), train_loss = 1.616, time/batch = 0.360\n",
            "685/850 (epoch 40), train_loss = 1.625, time/batch = 0.375\n",
            "686/850 (epoch 40), train_loss = 1.624, time/batch = 0.376\n",
            "687/850 (epoch 40), train_loss = 1.636, time/batch = 0.366\n",
            "688/850 (epoch 40), train_loss = 1.631, time/batch = 0.372\n",
            "689/850 (epoch 40), train_loss = 1.636, time/batch = 0.373\n",
            "690/850 (epoch 40), train_loss = 1.628, time/batch = 0.363\n",
            "691/850 (epoch 40), train_loss = 1.642, time/batch = 0.283\n",
            "692/850 (epoch 40), train_loss = 1.634, time/batch = 0.270\n",
            "693/850 (epoch 40), train_loss = 1.610, time/batch = 0.279\n",
            "694/850 (epoch 40), train_loss = 1.638, time/batch = 0.274\n",
            "695/850 (epoch 40), train_loss = 1.634, time/batch = 0.288\n",
            "696/850 (epoch 40), train_loss = 1.636, time/batch = 0.274\n",
            "697/850 (epoch 41), train_loss = 1.638, time/batch = 0.274\n",
            "698/850 (epoch 41), train_loss = 1.620, time/batch = 0.283\n",
            "699/850 (epoch 41), train_loss = 1.621, time/batch = 0.272\n",
            "700/850 (epoch 41), train_loss = 1.616, time/batch = 0.270\n",
            "701/850 (epoch 41), train_loss = 1.611, time/batch = 0.283\n",
            "702/850 (epoch 41), train_loss = 1.618, time/batch = 0.280\n",
            "703/850 (epoch 41), train_loss = 1.625, time/batch = 0.282\n",
            "704/850 (epoch 41), train_loss = 1.634, time/batch = 0.273\n",
            "705/850 (epoch 41), train_loss = 1.624, time/batch = 0.277\n",
            "706/850 (epoch 41), train_loss = 1.624, time/batch = 0.273\n",
            "707/850 (epoch 41), train_loss = 1.630, time/batch = 0.272\n",
            "708/850 (epoch 41), train_loss = 1.640, time/batch = 0.276\n",
            "709/850 (epoch 41), train_loss = 1.631, time/batch = 0.277\n",
            "710/850 (epoch 41), train_loss = 1.613, time/batch = 0.273\n",
            "711/850 (epoch 41), train_loss = 1.638, time/batch = 0.273\n",
            "712/850 (epoch 41), train_loss = 1.635, time/batch = 0.275\n",
            "713/850 (epoch 41), train_loss = 1.635, time/batch = 0.277\n",
            "714/850 (epoch 42), train_loss = 1.636, time/batch = 0.277\n",
            "715/850 (epoch 42), train_loss = 1.612, time/batch = 0.275\n",
            "716/850 (epoch 42), train_loss = 1.621, time/batch = 0.281\n",
            "717/850 (epoch 42), train_loss = 1.611, time/batch = 0.282\n",
            "718/850 (epoch 42), train_loss = 1.615, time/batch = 0.324\n",
            "719/850 (epoch 42), train_loss = 1.619, time/batch = 0.400\n",
            "720/850 (epoch 42), train_loss = 1.618, time/batch = 0.375\n",
            "721/850 (epoch 42), train_loss = 1.635, time/batch = 0.351\n",
            "722/850 (epoch 42), train_loss = 1.621, time/batch = 0.323\n",
            "723/850 (epoch 42), train_loss = 1.629, time/batch = 0.379\n",
            "724/850 (epoch 42), train_loss = 1.626, time/batch = 0.377\n",
            "725/850 (epoch 42), train_loss = 1.640, time/batch = 0.367\n",
            "726/850 (epoch 42), train_loss = 1.629, time/batch = 0.382\n",
            "727/850 (epoch 42), train_loss = 1.605, time/batch = 0.370\n",
            "728/850 (epoch 42), train_loss = 1.634, time/batch = 0.374\n",
            "729/850 (epoch 42), train_loss = 1.630, time/batch = 0.369\n",
            "730/850 (epoch 42), train_loss = 1.632, time/batch = 0.370\n",
            "731/850 (epoch 43), train_loss = 1.632, time/batch = 0.450\n",
            "732/850 (epoch 43), train_loss = 1.614, time/batch = 0.373\n",
            "733/850 (epoch 43), train_loss = 1.616, time/batch = 0.370\n",
            "734/850 (epoch 43), train_loss = 1.610, time/batch = 0.378\n",
            "735/850 (epoch 43), train_loss = 1.610, time/batch = 0.372\n",
            "736/850 (epoch 43), train_loss = 1.615, time/batch = 0.368\n",
            "737/850 (epoch 43), train_loss = 1.616, time/batch = 0.383\n",
            "738/850 (epoch 43), train_loss = 1.630, time/batch = 0.369\n",
            "739/850 (epoch 43), train_loss = 1.626, time/batch = 0.373\n",
            "740/850 (epoch 43), train_loss = 1.624, time/batch = 0.371\n",
            "741/850 (epoch 43), train_loss = 1.629, time/batch = 0.384\n",
            "742/850 (epoch 43), train_loss = 1.637, time/batch = 0.383\n",
            "743/850 (epoch 43), train_loss = 1.632, time/batch = 0.380\n",
            "744/850 (epoch 43), train_loss = 1.604, time/batch = 0.372\n",
            "745/850 (epoch 43), train_loss = 1.632, time/batch = 0.364\n",
            "746/850 (epoch 43), train_loss = 1.628, time/batch = 0.276\n",
            "747/850 (epoch 43), train_loss = 1.629, time/batch = 0.272\n",
            "748/850 (epoch 44), train_loss = 1.631, time/batch = 0.285\n",
            "749/850 (epoch 44), train_loss = 1.610, time/batch = 0.276\n",
            "750/850 (epoch 44), train_loss = 1.614, time/batch = 0.277\n",
            "751/850 (epoch 44), train_loss = 1.610, time/batch = 0.272\n",
            "752/850 (epoch 44), train_loss = 1.609, time/batch = 0.282\n",
            "753/850 (epoch 44), train_loss = 1.615, time/batch = 0.272\n",
            "754/850 (epoch 44), train_loss = 1.618, time/batch = 0.273\n",
            "755/850 (epoch 44), train_loss = 1.630, time/batch = 0.276\n",
            "756/850 (epoch 44), train_loss = 1.618, time/batch = 0.278\n",
            "757/850 (epoch 44), train_loss = 1.621, time/batch = 0.274\n",
            "758/850 (epoch 44), train_loss = 1.620, time/batch = 0.274\n",
            "759/850 (epoch 44), train_loss = 1.633, time/batch = 0.278\n",
            "760/850 (epoch 44), train_loss = 1.626, time/batch = 0.275\n",
            "761/850 (epoch 44), train_loss = 1.603, time/batch = 0.280\n",
            "762/850 (epoch 44), train_loss = 1.632, time/batch = 0.274\n",
            "763/850 (epoch 44), train_loss = 1.626, time/batch = 0.279\n",
            "764/850 (epoch 44), train_loss = 1.630, time/batch = 0.275\n",
            "765/850 (epoch 45), train_loss = 1.628, time/batch = 0.276\n",
            "766/850 (epoch 45), train_loss = 1.610, time/batch = 0.291\n",
            "767/850 (epoch 45), train_loss = 1.611, time/batch = 0.269\n",
            "768/850 (epoch 45), train_loss = 1.604, time/batch = 0.274\n",
            "769/850 (epoch 45), train_loss = 1.612, time/batch = 0.274\n",
            "770/850 (epoch 45), train_loss = 1.614, time/batch = 0.274\n",
            "771/850 (epoch 45), train_loss = 1.617, time/batch = 0.279\n",
            "772/850 (epoch 45), train_loss = 1.626, time/batch = 0.278\n",
            "773/850 (epoch 45), train_loss = 1.618, time/batch = 0.326\n",
            "774/850 (epoch 45), train_loss = 1.625, time/batch = 0.371\n",
            "775/850 (epoch 45), train_loss = 1.624, time/batch = 0.373\n",
            "776/850 (epoch 45), train_loss = 1.637, time/batch = 0.410\n",
            "777/850 (epoch 45), train_loss = 1.627, time/batch = 0.399\n",
            "778/850 (epoch 45), train_loss = 1.604, time/batch = 0.390\n",
            "779/850 (epoch 45), train_loss = 1.631, time/batch = 0.374\n",
            "780/850 (epoch 45), train_loss = 1.624, time/batch = 0.371\n",
            "781/850 (epoch 45), train_loss = 1.624, time/batch = 0.399\n",
            "782/850 (epoch 46), train_loss = 1.627, time/batch = 0.441\n",
            "783/850 (epoch 46), train_loss = 1.610, time/batch = 0.391\n",
            "784/850 (epoch 46), train_loss = 1.609, time/batch = 0.373\n",
            "785/850 (epoch 46), train_loss = 1.609, time/batch = 0.365\n",
            "786/850 (epoch 46), train_loss = 1.608, time/batch = 0.376\n",
            "787/850 (epoch 46), train_loss = 1.614, time/batch = 0.373\n",
            "788/850 (epoch 46), train_loss = 1.614, time/batch = 0.380\n",
            "789/850 (epoch 46), train_loss = 1.628, time/batch = 0.372\n",
            "790/850 (epoch 46), train_loss = 1.616, time/batch = 0.373\n",
            "791/850 (epoch 46), train_loss = 1.624, time/batch = 0.369\n",
            "792/850 (epoch 46), train_loss = 1.615, time/batch = 0.379\n",
            "793/850 (epoch 46), train_loss = 1.633, time/batch = 0.380\n",
            "794/850 (epoch 46), train_loss = 1.626, time/batch = 0.374\n",
            "795/850 (epoch 46), train_loss = 1.605, time/batch = 0.386\n",
            "796/850 (epoch 46), train_loss = 1.627, time/batch = 0.378\n",
            "797/850 (epoch 46), train_loss = 1.622, time/batch = 0.370\n",
            "798/850 (epoch 46), train_loss = 1.634, time/batch = 0.347\n",
            "799/850 (epoch 47), train_loss = 1.626, time/batch = 0.273\n",
            "800/850 (epoch 47), train_loss = 1.612, time/batch = 0.269\n",
            "801/850 (epoch 47), train_loss = 1.611, time/batch = 0.294\n",
            "802/850 (epoch 47), train_loss = 1.604, time/batch = 0.280\n",
            "803/850 (epoch 47), train_loss = 1.606, time/batch = 0.276\n",
            "804/850 (epoch 47), train_loss = 1.612, time/batch = 0.274\n",
            "805/850 (epoch 47), train_loss = 1.612, time/batch = 0.273\n",
            "806/850 (epoch 47), train_loss = 1.629, time/batch = 0.286\n",
            "807/850 (epoch 47), train_loss = 1.617, time/batch = 0.273\n",
            "808/850 (epoch 47), train_loss = 1.619, time/batch = 0.282\n",
            "809/850 (epoch 47), train_loss = 1.620, time/batch = 0.304\n",
            "810/850 (epoch 47), train_loss = 1.631, time/batch = 0.282\n",
            "811/850 (epoch 47), train_loss = 1.624, time/batch = 0.274\n",
            "812/850 (epoch 47), train_loss = 1.604, time/batch = 0.290\n",
            "813/850 (epoch 47), train_loss = 1.628, time/batch = 0.277\n",
            "814/850 (epoch 47), train_loss = 1.622, time/batch = 0.280\n",
            "815/850 (epoch 47), train_loss = 1.626, time/batch = 0.282\n",
            "816/850 (epoch 48), train_loss = 1.630, time/batch = 0.276\n",
            "817/850 (epoch 48), train_loss = 1.608, time/batch = 0.277\n",
            "818/850 (epoch 48), train_loss = 1.613, time/batch = 0.284\n",
            "819/850 (epoch 48), train_loss = 1.607, time/batch = 0.277\n",
            "820/850 (epoch 48), train_loss = 1.605, time/batch = 0.277\n",
            "821/850 (epoch 48), train_loss = 1.611, time/batch = 0.277\n",
            "822/850 (epoch 48), train_loss = 1.615, time/batch = 0.288\n",
            "823/850 (epoch 48), train_loss = 1.628, time/batch = 0.281\n",
            "824/850 (epoch 48), train_loss = 1.617, time/batch = 0.279\n",
            "825/850 (epoch 48), train_loss = 1.617, time/batch = 0.275\n",
            "826/850 (epoch 48), train_loss = 1.622, time/batch = 0.277\n",
            "827/850 (epoch 48), train_loss = 1.628, time/batch = 0.364\n",
            "828/850 (epoch 48), train_loss = 1.623, time/batch = 0.371\n",
            "829/850 (epoch 48), train_loss = 1.603, time/batch = 0.372\n",
            "830/850 (epoch 48), train_loss = 1.627, time/batch = 0.365\n",
            "831/850 (epoch 48), train_loss = 1.627, time/batch = 0.372\n",
            "832/850 (epoch 48), train_loss = 1.624, time/batch = 0.366\n",
            "833/850 (epoch 49), train_loss = 1.626, time/batch = 0.436\n",
            "834/850 (epoch 49), train_loss = 1.607, time/batch = 0.370\n",
            "835/850 (epoch 49), train_loss = 1.604, time/batch = 0.367\n",
            "836/850 (epoch 49), train_loss = 1.602, time/batch = 0.373\n",
            "837/850 (epoch 49), train_loss = 1.602, time/batch = 0.375\n",
            "838/850 (epoch 49), train_loss = 1.612, time/batch = 0.388\n",
            "839/850 (epoch 49), train_loss = 1.612, time/batch = 0.373\n",
            "840/850 (epoch 49), train_loss = 1.623, time/batch = 0.385\n",
            "841/850 (epoch 49), train_loss = 1.614, time/batch = 0.371\n",
            "842/850 (epoch 49), train_loss = 1.619, time/batch = 0.375\n",
            "843/850 (epoch 49), train_loss = 1.618, time/batch = 0.378\n",
            "844/850 (epoch 49), train_loss = 1.631, time/batch = 0.383\n",
            "845/850 (epoch 49), train_loss = 1.623, time/batch = 0.379\n",
            "846/850 (epoch 49), train_loss = 1.600, time/batch = 0.355\n",
            "847/850 (epoch 49), train_loss = 1.625, time/batch = 0.379\n",
            "848/850 (epoch 49), train_loss = 1.623, time/batch = 0.387\n",
            "849/850 (epoch 49), train_loss = 1.624, time/batch = 0.360\n",
            "model saved to /content/drive/Shareddrives/Materials UNI/UNIPI/ISPR/Midterm3 Assignment_4/save/save0/model.ckpt\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "eval \"$(conda shell.bash hook)\"\n",
        "conda activate myenv\n",
        "python train.py --data_dir=./data/lercioheadlines/ --save_dir=/FinalModel/ --model=nas --num_epochs=50 --rnn_size=256 --seq_length=130 --batch_size=200 --num_layers=2 --output_keep_prob=0.9999999999999999 --input_keep_prob=0.8 --grad_clip=5 --learning_rate=0.019 --decay_rate=0.92"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEGm-P5ZHrzf"
      },
      "source": [
        "# Generation of Headlines"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we are modifying the `sample.py` in order to avoid to print all the TensorFlow warnings."
      ],
      "metadata": {
        "id": "_d43Hvg_kd9S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('sample.py', 'r') as f:\n",
        "    script = f.readlines()\n",
        "script.insert(10, \"import tensorflow as tf\\n\"\n",
        "                  \"tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\\n\" )\n",
        "with open('sample2.py', 'a') as f:\n",
        "  for line in script:\n",
        "    f.write(line)"
      ],
      "metadata": {
        "id": "hgG6ykb9Jy3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### sample = 0\n",
        "The flag `sample=0` permits to run the prediction on max value each step of the prediction, this means that the model will pick at each time-step $t$ the value \n",
        "$$\\max_{v\\in V} P(V_t=v|V_{t-1}=v_{t-1})$$"
      ],
      "metadata": {
        "id": "b_wfepYaDc5s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAfc23i6w9BB",
        "outputId": "888c7a15-65d7-4375-cce5-d22f4574ab18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " più contro i propri di cantiere\n",
            "Scoperto il primo casa di cartellino di cantiere di casa di cantiere\n",
            "Scoperto il primo casa di casa di casa di casa di casa di cantiere\n",
            "Scoperto il primo casa di casa di casa di casa di casa di casa di casa di casa di casa di casa di casa di casa di casa di casa di casa di casa di casa di casa di casa di casa di casa di casa di casa di casa di casa di casa di casa di casa di casa di casa di casa di casa di casa di casa di casa di casa di casa di casa di casa di ca\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "%%shell\n",
        "eval \"$(conda shell.bash hook)\"\n",
        "conda activate myenv\n",
        "\n",
        "export TF_CPP_MIN_LOG_LEVEL=\"3\" && python sample2.py --sample=0 --save_dir=/FinalModel/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see the prediction keep enrolling a loop of characters, so the sampling process is not satisfying."
      ],
      "metadata": {
        "id": "3F5nlX7eDb7d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### sample = 1\n",
        "The flag `sample=1` permits to run the prediction on the weighted average on the precedent value so distribution will be described by a Markov Chain conditioned only by the n-precedent characters, where n is the seq_length of the RNN memory: \n",
        "$$\\prod_{i=0}^{seq\\_length} P(v_{t-i}|v_{t-i-1})$$"
      ],
      "metadata": {
        "id": "xZBbmSNcDrR_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b60399a-6656-4dd0-9a3a-05fdb5a3f053",
        "id": "Ts5cO9PTeAGE"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " sfigliata di Hilano in Wenneri\n",
            "Clancio e latto in adiuto con la cavallo: “Se”, ento di cone di guardato e Berlusconi: “Basta 500mila buss muore con un tusto più di Alfan\n",
            "Blello diventa “Arcere compie contro i prossima passa”\n",
            "SESMIO, IL RABOR A MANTO – ANDERETTE, LA PARDONA E PRUDO FAPIOCHITE-IL MORA\n",
            "Prepog sulla Fi la buca del moro e lancia2\n",
            "Merce in rotto: la vorigistra sbagliato corsa: “Con i Cabrio Francesco Basinis funi in auto d’auto e ingeglia al calcer.\n",
            "Padre viale, trattici inseg salta l\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "%%shell\n",
        "eval \"$(conda shell.bash hook)\"\n",
        "conda activate myenv\n",
        "\n",
        "export TF_CPP_MIN_LOG_LEVEL=\"3\" && python sample2.py --sample=1 --save_dir=/FinalModel/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see with the weighted average there are no more loops, but the probability of encountering some sequence of characters is lower therefore the model will have more difficulties in predict the next one based on the lower neuron activations."
      ],
      "metadata": {
        "id": "FN5JH-dvEZtl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### sample = 2\n",
        "\n",
        "The flag `sample=2` permits to run the prediction on the weighted average of the precedent value when it is a space, in such a way that new space will be sampled on the max values of the first characters per each new token, so distribution will be described by a Markov Chain conditioned only by the n-precedent characters and on the max value when the precedent char is a space so the model will predict with: \\begin{cases}\n",
        "    \\max_{v\\in V} P(V_t=v|V_{t-1}=v_{t-1}),& \\text{if } v_{t-1} = 0\\\\\n",
        "    \\prod_{i=0}^{seq\\_length} P(v_t-i|v_{t-i-1}),              & \\text{otherwise}\n",
        "\\end{cases}"
      ],
      "metadata": {
        "id": "k7pGXzbpEUFm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e906c5a4-17ee-4f49-fa94-845a29ab8e66",
        "id": "tAqJATxPeCap"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " si scopre sulla manica di quello con un testa a segreto di partita di solo la prima non la manico di partita di cantiere\n",
            "Scoperto anche a casa sulla carta di Papa per aver discorso Marino\n",
            "Scoperto ricorda sulla scopre Maria si proprio esperienza per la vita di Maria Jong per la prima bambino per diventa a #stato di Papa e ’0 con la prima discorsi ma diventa la manico di partita\n",
            "Scoperto diventa un vince il primo vince il nuovo legge in 7 negati finisce a martito di Renzi vince a Trump\n",
            "Marino di \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "%%shell\n",
        "eval \"$(conda shell.bash hook)\"\n",
        "conda activate myenv\n",
        "\n",
        "export TF_CPP_MIN_LOG_LEVEL=\"3\" && python sample2.py --sample=2 --save_dir=/FinalModel/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This surely is the best way to sample since the model will create words that are highly common through the corpus so it's mostly confident in the prediction of new characters, but there is a bit of loops in the creation of the first sequences."
      ],
      "metadata": {
        "id": "TzB0eOK2Et0-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we try to use the flag `prime_text` that will aim to start the model to predict the next sequence of characters basing on the entered string.\n"
      ],
      "metadata": {
        "id": "0yMpEx43BSwt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "eval \"$(conda shell.bash hook)\"\n",
        "conda activate myenv\n",
        "\n",
        "export TF_CPP_MIN_LOG_LEVEL=\"3\" && python sample2.py --save_dir=/FinalModel/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6ogIbhZBl0O",
        "outputId": "2f8f4861-90c0-472b-ebaf-34d02185d758"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Anno 2050 primate si evolvere il primo vince a ritrova per il nuovo Facebook si accordo di distrare di Allarme e Berlusconi tra avere ricorda un casa e quando ma zio contro il regalino di essere alla scopre con allarme a fare sulla fare alla discorsi per la prima testa per ricorda e inventa la vita la nonna di Jolie 5 contro i senza per aver bambini in jusa per ore sulla fare ma non avere essere alla discorsi è un giorno di Casaleggio “Salvini a un altro giorni di vende in discontare un giorno a scopre la manico a casa di\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can observe the model kept some sort of grammatic in the first part of the generated sequence and even a little of semantic if we want, since the primate could use for the first time Facebook to win some sort of competition.\n",
        "\n",
        "As we have stated the sintax can be some sort of understood by the model, after heavy usage of training the memory of the cells on \"big\" corpus and then the fine-tuning permits to shift the probability of the characters prediction to the Lercio kind of style...\n",
        "We think there is plenty of better models to deal with this kind of task, but even a little more complex one like the Hierarchical-RNN could smash this network having a larger receptive field that could permit to keep the semantic information more easily."
      ],
      "metadata": {
        "id": "IQ3RkG6KB0Mp"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
